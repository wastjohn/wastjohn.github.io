[
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Here is some text about my research.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nSwimSense: Open-Source Data Science for Swimmers\n\n\n\nPython\n\n\nArduino\n\n\nSwimming\n\n\n\n\n\n\n\n\n\n\nOct 13, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTURBO: Total-coverage Ultra-fast Response to Binary Mergers Observatory\n\n\n\nGravitational Waves\n\n\nOptical Astronomy\n\n\nPython\n\n\nSQL\n\n\nHTML/CSS\n\n\nIn progress\n\n\n\n\n\n\n\n\n\n\nOct 13, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n100 Proof: HI Observations of 100 Nearby Dwarf Galaxies with the 100 m Green Bank Telescope\n\n\n\nRadio Astronomy\n\n\nGBT\n\n\nInternational Collegues\n\n\nPaper\n\n\n\n\n\n\n\n\n\n\nJun 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPersistent Images of Gravitionally Lensed Galaxies\n\n\n\nTopology\n\n\nPython\n\n\nGraviational Lensing\n\n\nEuclid\n\n\n\n\n\n\n\n\n\n\nMay 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotes on Compact Obscured Nuclei. I. The Evolutionary Context\n\n\n\nPython\n\n\nR\n\n\nPaper\n\n\n\n\n\n\n\n\n\n\nApr 9, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMACRO Science: King-Fitted Structural Parameters of 20 Globular Clusters Observed with the RLMT\n\n\n\nAstronomy\n\n\nPython\n\n\nMACRO Consortium\n\n\n\n\n\n\n\n\n\n\nJun 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFundamental Astrophysics at Low Metallicity: Leo P\n\n\n\nRadio Astronomy\n\n\nVLA\n\n\nPython\n\n\nCASA\n\n\nIn progress\n\n\n\n\n\n\n\n\n\n\nJun 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npyScope: Optical Telescope Automation with Python\n\n\n\nOptical Astronomy\n\n\nPython\n\n\nIn progress\n\n\n\n\n\n\n\n\n\n\nJun 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of the Optical Spectra of H2O Megamaser Emissions\n\n\n\nOptical Astronomy\n\n\nGBT\n\n\nPython\n\n\nSQL\n\n\nPast Project\n\n\n\n\n\n\n\n\n\n\nJun 1, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "research/Topology/index.html",
    "href": "research/Topology/index.html",
    "title": "Persistent Images of Gravitionally Lensed Galaxies",
    "section": "",
    "text": "During the Spring semester of my junior year at Macalester, I took a topology math class taught by Professor Lori Ziegelmeier. At the end of the course, we conducted and presented research on a topic of our choosing, showcasing our new skills in topological data analysis or TDA. Here is a link to my final paper where I explored persisent images of graviationally lensed galaxies observed by the Euclid space telescope.\nThis project was extremely fun and exciting for me, particularly because TDA hasn’t really been explored in the field of astronomy except at the large-scale. I’m hoping to continue this project and explore TDA applications to astronomy and cosmology in the future."
  },
  {
    "objectID": "research/pyScope/index.html",
    "href": "research/pyScope/index.html",
    "title": "pyScope: Optical Telescope Automation with Python",
    "section": "",
    "text": "Small to midsize optical telescopes are a common resource utilized by colleges and universities for various astrophysical research and classroom applications. These instruments can be dedicated to specific curricular goals and research that would be impractical on larger, competitively allocated telescopes. By providing unrestricted access to these often-existing educational resources, these instruments fill a critical gap in performing challenging experiments requiring significant time-intensive, skilled, exploratory, or developmental observations. We are developing a new, open-source Python package for robotic observatory control and operation called Pyscope. Driven by Astropy, we have developed this package with undergraduate student support for experienced and amateur astronomers that provides a unified interface for scripting astronomical hardware. This package originated from the University of Iowa’s Iowa Optical Telescope Automation (IOTA) code, which undergraduate students have iteratively developed for the last eight years, building off nearly three decades of institutional history in telescope automation. Pyscope natively supports standard astronomical hardware, including the ASCOM Platform, PlaneWave software, and MaxImDL. We also have included a simple pipeline for easy custom hardware integration into Pyscope. Observatories using Pyscope can take advantage of the Telrun module, allowing fully robotic scheduling and operation through a standard and consistent API with upcoming support for triggered interruptions for transient sources. With several essential data calibration, reduction, and analysis tools for CCD and CMOS detectors–designed for data collected using Telrun methods–users may quickly begin advanced analyses. An observatory may have scheduling, data reduction, and storage processes on a local server, with operations occurring on a remote machine. The ongoing development of Pyscope has primarily been supported by the Macalester-Augustana-Coe Robotic Observatory (MACRO) Consortium, which operates the Robert L. Mutel Telescope located at Winer Observatory in Sonoita, AZ.\nHere is the link to the AAS244 IPoster presentation."
  },
  {
    "objectID": "research/KingFits/index.html",
    "href": "research/KingFits/index.html",
    "title": "MACRO Science: King-Fitted Structural Parameters of 20 Globular Clusters Observed with the RLMT",
    "section": "",
    "text": "Using data collected from the Robert L. Mutel Telescope, we performed g’, i’, and r’ photometry to determine the structural parameters of ~20 globular clusters, assuming King models. Our model parameters for core radius rc, tidal radius rt, and model constant k are presented and compared to the literature. Additionally, some of the targets have also been observed by Macalester’s 16-inch telescope, equipped with an SBIG CCD camera, located in St. Paul, MN. A comparison of the quality of data obtained at Macalester and those taken with the RLMT CMOS, located in Sonoita AZ, is presented.\nHere is the link to the AAS244 IPoster presentation."
  },
  {
    "objectID": "research/GBT-ConfirmedLocalVolumeDwarfGalaxies/index.html",
    "href": "research/GBT-ConfirmedLocalVolumeDwarfGalaxies/index.html",
    "title": "100 Proof: HI Observations of 100 Nearby Dwarf Galaxies with the 100 m Green Bank Telescope",
    "section": "",
    "text": "We describe the results of observations with the 100 m Robert C. Byrd Green Bank Telescope in the H I line of 105 nearby dwarf galaxies, 60 of which were discovered recently in the DESI Legacy Imaging Surveys. Of 105 objects observed, we detected 77 galaxies with the following median parameters: an H I flux of 0.69 Jy km s‑1, a heliocentric velocity of 732 km s‑1, and a W50 line width of 32 km s‑1. Seventy are isolated late-type objects and 35 are new probable satellites of nearby spiral galaxies (NGC 628, NGC 2787, NGC 3556, NGC 4490, NGC 4594, and NGC 5055). The detected galaxies are predominantly gas-rich systems with a median gas-to-stellar-mass ratio of 1.87. In general, they follow the classic Tully–Fisher relation obtained for large disk-dominated spiral galaxies if their M21 magnitudes are used instead of B magnitudes.\nHere is the link to the paper we published in the Astronomical Journal."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Will St. John",
    "section": "",
    "text": "He/Him/His\nUndergraduate Physics & Astronomy Student  2022-2026 | Macalester College\n\n\nMassanutten Regional Governor’s School  2020-2022 | Mt. Jackson, VA\nEast Rockingham High School  2018-2022 | Elkton, VA"
  },
  {
    "objectID": "index.html#will-st.-john",
    "href": "index.html#will-st.-john",
    "title": "Will St. John",
    "section": "",
    "text": "He/Him/His\nUndergraduate Physics & Astronomy Student  2022-2026 | Macalester College\n\n\nMassanutten Regional Governor’s School  2020-2022 | Mt. Jackson, VA\nEast Rockingham High School  2018-2022 | Elkton, VA"
  },
  {
    "objectID": "courses/topology/paper.html",
    "href": "courses/topology/paper.html",
    "title": "Persistent Images of Gravitionally Lensed Galaxies",
    "section": "",
    "text": "Dark matter in forground galaxies can distort the light from distant background galaxies. Depending on the distribution of dark matter and the location of the forground matter relative to the background galaxy, the resulting distortion of the distant background galaxy can vary in appearance. Persistent homology is commonly used to identify structure at large scales, but is yet to be used at smaller scales in the field of astronomy. We explore the possibility of using persistence images to identify common structures in images of gravitational lensed galaxies through a hierarchical clustering machine learning algorithm. We find a discrepancy in clustering results between persistence images and image cutouts where the former identified more similar lensed galaxies sooner. Additionally the persistence clustering process takes place at less than 20% of the cutout clustering process. These results could be influenced by the Curse of Dimensionality given the size of the cutouts. We propose a continued investigation into our results that will be enacted in the 2025-2026 school year at Macalester College as a Physics and Astronomy Honors Thesis."
  },
  {
    "objectID": "courses/topology/paper.html#astronomy",
    "href": "courses/topology/paper.html#astronomy",
    "title": "Persistent Images of Gravitionally Lensed Galaxies",
    "section": "Astronomy",
    "text": "Astronomy\nThrough observations of the Coma Cluster, Zwicky (1933) noted that measured velocity dispersions were an order of magnitude higher than what would be possible with the observed mass of the system. In the years that followed, the idea of galaxies being embedded in cold dark matter halos became intrinsically tied to our understanding of the cosmological structure of the Universe, and is well supported by observations and simulations (Frenk and White 2012).\nGravitionally lensed galaxies are intrinsically connected to dark matter, thus exploring commonalities amongst these lensed objects could reveal insight into the distribution of dark matter. Current methods for identifing classifying celestial objects is based on visual appearences, which depend on the orientation of the galaxies in question (e.g., Bertin and Arnouts 1996). Persistent homology offers a potential approach to identify structure regardless of orientation, especially with images (Ghrist 2008)."
  },
  {
    "objectID": "courses/topology/paper.html#persistent-homology",
    "href": "courses/topology/paper.html#persistent-homology",
    "title": "Persistent Images of Gravitionally Lensed Galaxies",
    "section": "Persistent Homology",
    "text": "Persistent Homology\nPersistent homology is a branch of topology that identifies persistent features of simplicial structures. By constructing simplicial structures across a filtration, we can compute the homology at each stage in the filtration, allowing us to construct birth-death pairs of features in data. From the birth-death pairs, or persistence diagram, we can compute the persistence image, which is a vectorized form of the information captured in the birth-death pairs, allowing for implementation in machine learning algorithms (Adams et al. 2015).\nIn astronomy, persistent homology is primarily used at the large-scale (e.g., Chen et al. 2015; Sousbie, Pichon, and Kawahara 2011). More specifically, persistent images have not yet been applied to smaller scale astronomical observations. If persistent homology is capable of idenfying structures in image data regardless of orientation, it seems apparent that we should be applying this to the field of astronomy, especially where classification of objects is based primarily on visual orientation."
  },
  {
    "objectID": "courses/topology/paper.html#machine-learning",
    "href": "courses/topology/paper.html#machine-learning",
    "title": "Persistent Images of Gravitionally Lensed Galaxies",
    "section": "Machine Learning",
    "text": "Machine Learning\nWhen trying to identify potential similar structure in data through unsupervised machine learning, there are many algorithms that can be implemented. One of the most simple unsupervised machine learning clustering algorithms is the hierarchical clustering algorithm. The idea behind this algorithm is to identify similar datapoints based on their distances to each other. All variables are standardized to allow for comparison between variables of different units, and the closest two points are fused together. Once a cluster is created, we use single linkage method to create the clusters at higher distances. This linkage method will only join two clusters together at a distance \\(d\\) if all of the point in both clusters are at least a distance \\(d\\) away from each other."
  },
  {
    "objectID": "courses/quantum/PS2.html",
    "href": "courses/quantum/PS2.html",
    "title": "PHYS 481 - Quantum Mechanics",
    "section": "",
    "text": "Problem 1.7\nCalculate \\(d\\left&lt;p\\right&gt;/dt\\). Answer:\n\\[\\frac{d\\left&lt;p\\right&gt;}{dt} = \\left&lt;-\\frac{\\partial V}{\\partial x}\\right&gt;\\]\nThis is an instance of Ehrenfest’s theorem, which asserts that expectation values obey classical laws.\n\nResponse: Before proving this, we should derive a few quantities.\n\\[\\begin{align}\\frac{\\partial \\Psi}{\\partial t} &= \\frac{i\\hbar}{2m}\\frac{\\partial^2\\Psi}{\\partial x^2} - \\frac{i}{\\hbar}V\\Psi \\\\ \\frac{\\partial \\Psi^*}{\\partial t} &= -\\frac{i\\hbar}{2m}\\frac{\\partial^2\\Psi}{\\partial x^2} + \\frac{i}{\\hbar}V\\Psi\\end{align}\\]\n\\[\\begin{align} \\frac{d\\left&lt;p\\right&gt;}{dt} &= \\frac{d}{dt}\\left(-i\\hbar\\int_{-\\infty}^{\\infty}\\Psi^*\\frac{\\partial\\Psi}{\\partial x} dx\\right) \\\\ &= -i\\hbar\\int_{-\\infty}^{\\infty}\\frac{d}{dt}\\left(\\Psi^*\\frac{\\partial\\Psi}{\\partial x}\\right) dx \\\\ &= -i\\hbar \\int_{-\\infty}^{\\infty}\\left(\\Psi^*\\frac{d}{dt}\\frac{\\partial\\Psi}{\\partial x} + \\frac{\\partial\\Psi}{\\partial x}\\frac{\\partial\\Psi^*}{dt}\\right) \\\\ &= -i\\hbar\\int_{-\\infty}^{\\infty}\\left(\\Psi^*\\frac{\\partial}{\\partial x}\\left(\\frac{i\\hbar}{2m}\\frac{\\partial^2\\Psi}{\\partial x^2} - \\frac{i}{\\hbar}V\\Psi\\right) + \\frac{\\partial \\Psi}{\\partial x}\\left(-\\frac{i\\hbar}{2m}\\frac{\\partial^2\\Psi}{\\partial x^2} + \\frac{i}{\\hbar}V\\Psi\\right)\\right) \\\\ &= -i\\hbar\\int_{-\\infty}^{\\infty}\\left(\\Psi^*\\frac{i\\hbar}{2m}\\frac{\\partial^3\\Psi}{\\partial x^3} - \\frac{i}{\\hbar}V\\Psi^*\\frac{\\partial\\Psi}{\\partial x} - \\frac{i}{\\hbar}\\frac{\\partial V}{\\partial x}\\Psi^*\\Psi - \\frac{i\\hbar}{2m}\\frac{\\partial^2\\Psi^*}{\\partial x^2}\\frac{\\partial \\Psi}{\\partial x} + \\frac{i}{\\hbar}V\\Psi^*\\frac{\\partial\\Psi}{\\partial x}\\right) \\\\ &= -i\\hbar\\left(\\frac{-i\\hbar}{2m}\\int_{-\\infty}^{\\infty}\\frac{\\partial^2\\Psi^*}{\\partial x^2}\\frac{\\partial\\Psi}{\\partial x}dx + \\frac{i\\hbar}{2m}\\int_{-\\infty}^{\\infty}\\Psi^*\\frac{\\partial^3\\Psi}{\\partial x^3}dx - \\frac{i}{\\hbar}\\int_{-\\infty}^{\\infty}\\frac{\\partial V}{\\partial x}\\Psi^*\\Psi dx\\right) \\\\ &= -i\\hbar \\left(\\frac{-i\\hbar}{2m}\\left(\\frac{\\partial \\Psi}{\\partial x}\\frac{\\partial \\Psi^*}{\\partial x}\\right)\\Big|_{-\\infty}^{\\infty} + \\frac{i\\hbar}{2m}\\int_{-\\infty}^{\\infty}\\frac{\\partial \\Psi^*}{\\partial x}\\frac{\\partial^2\\Psi}{\\partial x^2}dx + \\frac{i\\hbar}{2m}\\int_{-\\infty}^{\\infty}\\Psi^*\\frac{\\partial^3\\Psi}{\\partial x^3}dx - \\frac{i}{\\hbar}\\int_{-\\infty}^{\\infty}\\frac{\\partial V}{\\partial x}\\Psi^*\\Psi dx \\right) \\\\ &= -i\\hbar\\left(\\frac{i\\hbar}{2m}\\int_{-\\infty}^{\\infty}\\frac{\\partial \\Psi^*}{\\partial x}\\frac{\\partial^2\\Psi}{\\partial x^2}dx + \\int_{-\\infty}^{\\infty}\\left(\\frac{i\\hbar}{2m}\\Psi^*\\frac{\\partial^3\\Psi}{\\partial x^3} - \\frac{i}{\\hbar}\\frac{\\partial V}{\\partial x}\\Psi^*\\Psi\\right)dx\\right) \\\\ &= -i\\hbar\\left(\\frac{i\\hbar}{2m}\\Psi^*\\frac{\\partial^2\\Psi}{\\partial x^2}\\Big|_{-\\infty}^{\\infty} - \\frac{i\\hbar}{2m}\\int_{-\\infty}^{\\infty}\\Psi^*\\frac{\\partial^3\\Psi}{\\partial x^3}dx + \\frac{i\\hbar}{2m}\\int_{-\\infty}^{\\infty}\\Psi^*\\frac{\\partial^3\\Psi}{\\partial x^3}dx - \\frac{i}{\\hbar}\\int_{-\\infty}^{\\infty}\\frac{\\partial V}{\\partial x}\\Psi^*\\Psi dx\\right) \\\\ &= -\\int_{-\\infty}^{\\infty}\\Psi^*\\frac{\\partial V}{\\partial x}\\Psi dx \\\\ &= \\left&lt;-\\frac{\\partial V}{\\partial x}\\right&gt;\\end{align}\\]\n\n\n\nProblem 1.9\nA particle of mass \\(m\\) has the wave function\n\\[\\Psi(x,t) = Ae^{-a[(mx^2/\\hbar) + it]},\\]\nwhere \\(A\\) and \\(a\\) are positive real constants.\n\nFind \\(A\\).\nFor what potential energy function, \\(V(x)\\), is this a solution to the Schrodinger equation?\nCalculate the expectation values of \\(x\\), \\(x^2\\), \\(p\\), and \\(p^2\\).\nFind \\(\\sigma_x\\) and \\(\\sigma_p\\). Is their product consistent with the uncertainty principle?\n\n\nResponse: Normalizing \\(\\Psi\\) to find \\(A\\) is done below.\n\\[\\begin{align} 1 &= \\int_{-\\infty}^{\\infty}\\Psi^*\\Psi dx \\\\ &= A^2 \\int_{-\\infty}^{\\infty}e^{-2amx^2/\\hbar} \\\\ &= A^2\\left(\\frac{\\pi\\hbar}{2ma}\\right)^{1/2} \\\\ A &= \\left(\\frac{2ma}{\\pi\\hbar}\\right)^{1/4}\\end{align}\\]\nTo find what potential energy function \\(V(x)\\) is a solution to the the Schrodinger equation, we need to find \\(\\frac{\\partial \\Psi}{\\partial t}\\) and \\(\\frac{\\partial^2\\Psi}{\\partial x^2}\\), then plug the results in to the Schrodinger equation, and solve for \\(V(x)\\), all of which are done below.\n\\[\\begin{align} \\frac{\\partial \\Psi}{\\partial t} &= -ai\\left(\\frac{2ma}{\\pi\\hbar}\\right)^{1/4}e^{-a[(mx^2/\\hbar) + it]} \\\\ \\frac{\\partial^2\\Psi}{\\partial x^2} &= \\left(\\frac{2ma}{\\pi\\hbar}\\right)^{1/4}\\left(\\frac{2max}{\\hbar}\\right)^2e^{-a[(mx^2/\\hbar) + it]} + \\left(\\frac{2ma}{\\pi\\hbar}\\right)^{1/4}\\left(\\frac{-2ma}{\\hbar}\\right)e^{-a[(mx^2/\\hbar) + it]} \\\\ V(x) \\Psi &= i\\hbar\\frac{\\partial \\Psi}{\\partial t} + \\frac{\\hbar^2}{2m}\\frac{\\partial^2\\Psi}{\\partial x^2} \\\\ &= i\\hbar\\left(-ai\\left(\\frac{2ma}{\\pi\\hbar}\\right)^{1/4}e^{-a[(mx^2/\\hbar) + it]}\\right) + \\frac{\\hbar^2}{2m}\\left(\\left(\\frac{2ma}{\\pi\\hbar}\\right)^{1/4}\\left(\\frac{2max}{\\hbar}\\right)^2e^{-a[(mx^2/\\hbar) + it]} + \\left(\\frac{2ma}{\\pi\\hbar}\\right)^{1/4}\\left(\\frac{-2ma}{\\hbar}\\right)e^{-a[(mx^2/\\hbar) + it]}\\right) \\\\ V(x) &= a\\hbar\\left(\\frac{2ma}{\\pi\\hbar}\\right)^{1/4}+ \\frac{\\hbar^2}{2m}\\left(\\frac{2ma}{\\pi\\hbar}\\right)^{1/4}\\left(\\frac{2max}{\\hbar}\\right)^2 - \\frac{\\hbar^2}{2m}\\left(\\frac{2ma}{\\hbar}\\right)\\left(\\frac{2ma}{\\hbar}\\right) \\\\ V(x) &= 2ma^2x^2\\end{align}\\]\nCalculating the expecation values of \\(x\\), \\(x^2\\), \\(p\\), and \\(p^2\\) is done below.\n\\[\\begin{align} \\left&lt;x\\right&gt; &= \\int_{-\\infty}^{\\infty}\\Psi^*x\\Psi dx \\\\ &= A^2\\int_{-\\infty}^{\\infty}xe^{-2a(mx^2/\\hbar)} \\\\ &= 0 \\text{ because odd integrand over symmetric bounds} \\\\ \\\\ \\left&lt;x^2\\right&gt; &= \\Psi^*x^2\\Psi dx \\\\ &= \\left(\\frac{2am}{\\pi\\hbar}\\right)^{1/2}\\int_{-\\infty}^{\\infty}x^2e^{-2amx^2}dx \\\\ &= \\left(\\frac{2am}{\\pi\\hbar}\\right)^{1/2}\\left(\\frac{\\hbar}{4am}\\right)\\left(\\frac{\\pi\\hbar}{2am}\\right)^{1/2} \\\\ &= \\frac{\\hbar}{4am} \\\\ \\\\ \\left&lt;p\\right&gt; &= m\\frac{d\\left&lt;x\\right&gt;}{dt} \\\\ &= 0 \\text{ by Ehrenfest's Theorem} \\\\ \\\\ \\left&lt;p^2\\right&gt; &= \\int_{-\\infty}^{\\infty}\\Psi^*\\hat{p}^2\\Psi dx \\\\ &= -\\hbar^2\\int_{-\\infty}^{\\infty}\\Psi^*\\frac{\\partial^2 \\Psi}{\\partial x^2}dx \\\\ &= \\hbar^2\\left(\\frac{2am}{\\pi\\hbar}\\right)^{1/2}\\left(\\frac{2am}{\\hbar}\\right)\\int_{-\\infty}^{\\infty}\\left(1 - \\frac{2am}{\\hbar}x^2\\right)e^{-2amx^2/\\hbar}dx\\\\ &= \\hbar am\\end{align}\\]\nIt follows that \\(\\sigma_x\\) and \\(\\sigma_p\\) are\n\\[\\begin{align}\\sigma_x &= \\sqrt{\\left&lt;x^2\\right&gt; - \\left&lt;x\\right&gt;^2} = \\left(\\frac{\\hbar}{4am}\\right)^{1/2} \\\\ \\sigma_p &= \\sqrt{\\left&lt;p^2\\right&gt; - \\left&lt;p\\right&gt;^2} = \\left(\\hbar a m\\right)^{1/2} \\end{align}\\]\nAnd their product is consistent with the uncertainty principle.\n\\[\\sigma_x \\sigma_p = \\left(\\frac{\\hbar}{4am}\\right)^{1/2} \\left(\\hbar a m\\right)^{1/2} = \\frac{\\hbar}{2} \\geq \\frac{\\hbar}{2}\\]\n\n\n\nProblem 1.14\nLet \\(P_{ab}(t)\\) be the probability of finding the particle in the range \\((a &lt; x &lt; b)\\), at time \\(t\\).\n\nShow that \\[\\frac{dP_{ab}}{dt} = J(a,t) - J(b, t),\\] where \\[J(x,t) \\equiv \\frac{i\\hbar}{2m}\\left(\\Psi\\frac{\\partial \\Psi^*}{\\partial x} - \\Psi^*\\frac{\\partial\\Psi}{\\partial x}\\right).\\] What are the units of \\(J(x,t)\\)? Comment: \\(J\\) is called the probability current because it tells you the rate at which probability is “flowing” past the point \\(x\\). If \\(P_{ab}(t)\\) is increasing, then more probability is flowing into the region at one end than flows out at the other.\nFind the probability current for the wave function in Problem 1.9 (This is not a very pithy example, I’m afraid; we’ll encounter more substantial ones in due course.)\n\n\nResponse: For part a), we can derive the result the following way.\n\\[\\begin{align} \\frac{d}{dt}P_{ab} &= \\frac{d}{dt}\\int_{a}^{b}|\\Psi|^2 dx \\\\ &= \\int_a^b \\frac{d}{dt}|\\Psi|^2dx \\\\ &= \\int_a^b \\frac{\\partial}{\\partial x}\\left(\\frac{i\\hbar}{2m}\\left(\\Psi^*\\frac{\\partial \\Psi}{\\partial x} - \\frac{\\partial \\Psi^*}{\\partial x}\\Psi\\right)\\right)dx \\\\ &= \\int_a^b\\frac{\\partial}{\\partial x}-J(x,t) dx \\\\ &= -J(x,t)\\Big|_a^b \\\\ &= J(a,t) - J(b,t)\\end{align}\\]\nNote that the units of \\(J\\) are \\(\\frac{1}{s}\\).\nFor part b), if we call the real part of \\(\\Psi\\) \\(f(x)\\) (to minimize the writing), we get the following probability current.\n\\[\\begin{align}\\Psi &= f(x)e^{-iat} \\\\ \\Psi^* &= f(x)e^{iat} \\\\ \\frac{\\partial\\Psi}{\\partial x} &= f'(x)e^{-iat} \\\\ \\frac{\\partial\\Psi^*}{\\partial x} &= f'(x)e^{iat} \\\\ \\\\ J(x,t) &= \\frac{i\\hbar}{2m}\\left(\\Psi\\frac{\\partial \\Psi^*}{\\partial x} - \\Psi^*\\frac{\\partial\\Psi}{\\partial x}\\right) \\\\ &= \\frac{i\\hbar}{2m}\\left(f(x)e^{-iat}f'(x)e^{iat} - f(x)e^{iat}f'(x)e^{-iat}\\right) \\\\ &= 0\\end{align}\\]\n\n\n\nProblem 1.16\nA particle is represented (at time \\(t=0\\)) by the wavefunction \\[\\Psi(x,0) = \\cases{A(a^2 - x^2), \\hspace{1cm} -a\\leq x\\leq +a \\\\ 0, \\hspace{3.3cm} \\text{otherwise.}}\\]\n\nDetermine the normalization constant \\(A\\).\nWhat is the expectation value of \\(x\\)?\nWhat is the expectation value of \\(p\\) (Note that you cannot get it from \\(\\left&lt;p\\right&gt; = md\\left&lt;x\\right&gt;/dt\\). Why not?)\nFind the expectation value of \\(x^2\\).\nFind the expectation value of \\(p^2\\).\nFind the uncertainty in \\(x\\) (\\(\\sigma_x\\)).\nFind the uncertainty in \\(p\\) (\\(\\sigma_p\\)).\nCheck that your results are consistent with the uncertainty principle.\n\n\nResponse: We can normalize \\(\\Psi\\) to find \\(A\\) as follows:\n\\[\\begin{align} 1 &= A^2\\int_{-a}^{a}(a^2-x^2)^2dx \\\\ &= A^2\\left(a^4\\int_{-a}^{a}dx -2a^2\\int_{-a}^{a}x^2dx + \\int_{-a}^{a}x^4dx\\right) \\\\ &= A^2\\left(a^4x - 2a^2\\frac{1}{3}x^3 + \\frac{1}{3}x^5\\right)\\Big|_{-a}^{a} \\\\ &= A^2\\left(2a^5-\\frac{4}{3}a^5 + \\frac{2}{3}a^5\\right) \\\\ &= A^2\\frac{16}{15}a^5 \\\\ A&= \\frac{1}{4}\\sqrt{\\frac{15}{a^5}}\\end{align}\\]\nFrom there we can find \\(\\left&lt;x\\right&gt;\\),\n\\[\\begin{align}\\left&lt;x\\right&gt; &= \\int_{-a}^{a}\\Psi^*x\\Psi dx \\\\ &= A^2\\int_{-a}^{a}x(a^2-x^2)^2dx \\\\ &= 0 \\text{ because odd integrand over symmetric bounds}\\end{align}\\]\n\\(\\left&lt;p\\right&gt;\\) (which we can’t get from taking the time derivative because we are only given \\(\\Psi(t=0)\\), not a distribution of \\(\\Psi\\) across a range of \\(t\\)),\n\\[\\begin{align}\\left&lt;p\\right&gt; &= \\int_{-a}^{a}\\Psi^*p\\Psi dx \\\\ &= -i\\hbar A^2\\int_{-a}^{a}\\underbrace{(a^2-x^2)}_{even}\\underbrace{\\frac{\\partial}{\\partial x}(a^2 - x^2)}_{odd}dx \\\\ &= 0 \\text{ because odd integrand over symmetric bounds}\\end{align}\\]\n\\(\\left&lt;x^2\\right&gt;\\),\n\\[\\begin{align}\\left&lt;x^2\\right&gt; &= \\int_{-a}^{a}\\Psi^*x^2\\Psi dx \\\\ &= A^2\\left[a^4\\int_{-a}^{a}x^2 dx - 2a^2\\int_{-a}^{a}x^4 dx + \\int_{-a}^{a} x^6 dx\\right] \\\\ &= 2A^2\\left[a^4\\frac{1}{3}x^3 - \\frac{2}{5}a^2x^5 + \\frac{1}{7}a^7\\right]_{0}^{a} \\\\ &= \\frac{15}{8}a^2\\left[\\frac{1}{3} - \\frac{2}{5} + \\frac{1}{7}\\right] \\\\ &= \\frac{a^2}{7}\\end{align}\\]\nand \\(\\left&lt;p^2\\right&gt;\\).\n\\[\\begin{align}\\left&lt;p^2\\right&gt; &= \\int_{-a}^{a}\\Psi^*p^2\\Psi dx \\\\ &= -\\hbar^2 A^2\\int_{-a}^{a}(a^2 - x^2)\\frac{\\partial^2}{\\partial x^2}(a^2- x^2)dx \\\\ &= 2\\hbar^2 A^2\\int_{-a}^{a}(a^2-x^2)dx \\\\ &= \\frac{15\\hbar^2}{4a^5}\\int_{0}^{a}(a^2-x^2)dx \\\\ &= \\frac{15\\hbar^2}{4a^5} \\left(a^2 x - \\frac{1}{3}x^3\\right)_{0}^{a} \\\\ &= \\frac{5\\hbar^2}{2a^2}\\end{align}\\]\nWith those values, we can find \\(\\sigma_x\\) and \\(\\sigma_p\\), and compare them with the uncertainty principle.\n\\[\\begin{align}\\sigma_x &= \\sqrt{\\left&lt;x^2\\right&gt; - \\left&lt;x\\right&gt;^2} = \\frac{a}{\\sqrt{7}} \\\\ \\sigma_p &= \\sqrt{\\left&lt;p^2\\right&gt; - \\left&lt;p\\right&gt;^2} = \\sqrt{\\frac{5}{2}}\\frac{\\hbar}{a} \\\\ \\sigma_x\\sigma_p &= \\frac{a}{\\sqrt{7}}\\sqrt{\\frac{5}{2}}\\frac{\\hbar}{a} = \\sqrt{\\frac{5}{14}}\\hbar \\geq \\frac{\\hbar}{2} \\end{align}\\]\n\n\n\nProblem 1.17\nSuppose you wanted to describe an unstable particle, that spontaneously disitegrates with a “lifetime” \\(\\tau\\). In that case the total probability of finding the particle somewhere should not be constant, but should decrease at (say) an exponential rate:\n\\[P(t) \\equiv \\int_{-\\infty}^{\\infty}|\\Psi(x,t)|^2dx = e^{-t/\\tau}.\\]\nA crude way of achieving this result is as follows. In equation 1.24 we tacitly assumed that \\(V\\) (the potential energy) is real. That is certaintly reasonable, but it leads to the “conservation of probability” enshrined in Equations 1.27. What if we assign \\(V\\) to an imaginary part:\n\\[V = V_0 - i\\Gamma,\\]\nwhere \\(V_0\\) is the true potential energy and \\(\\Gamma\\) is a positive real constant?\n\nShow that (in place of Equation 1.27) we now get \\[\\frac{dP}{dt} = -\\frac{2\\Gamma}{\\hbar}P.\\]\nSolve for \\(P(t)\\), and find the lifetime of the particle in terms of \\(\\Gamma\\).\n\n\nResponse: If we check out the “old” version of Equation 1.24\n\\[\\frac{\\partial\\Psi^*}{\\partial t} = \\frac{-i\\hbar}{2m}\\frac{\\partial^2\\Psi^*}{\\partial x^2} + \\frac{i}{\\hbar}V\\Psi^*\\]\nwe can see that letting \\(V\\) be a complex function, then Equations 1.24 becomes the following\n\\[\\frac{\\partial\\Psi^*}{\\partial t} = \\frac{-i\\hbar}{2m}\\frac{\\partial^2\\Psi^*}{\\partial x^2} + \\frac{i}{\\hbar}V^*\\Psi^*\\]\nwhich causes 1.25, in which we apply the time derivative to \\(|\\Psi|^2\\), to become the following\n\\[\\begin{align} \\frac{\\partial}{\\partial t}|\\Psi|^2 &= \\frac{i\\hbar}{2m}\\left(\\Psi^*\\frac{\\partial^2\\Psi}{\\partial x^2} - \\frac{\\partial^2\\Psi^*}{\\partial x^2}\\Psi\\right) \\\\ &= \\frac{\\partial}{\\partial x}\\left[\\frac{i\\hbar}{2m}\\left(\\Psi^*\\frac{\\partial\\Psi}{\\partial x} - \\frac{\\partial\\Psi^*}{\\partial x}\\Psi\\right)\\right] + \\int\\frac{i}{\\hbar} |\\Psi|^2 (V^* - V) dx \\\\ &= \\frac{i}{\\hbar}(V_0 + i\\Gamma - V_0 + i\\Gamma)\\underbrace{\\int |\\Psi|^2 dx}_{P(t)} \\\\ \\frac{dP}{dt} &= -\\frac{2\\Gamma}{\\hbar}P\\end{align}\\]\nSolving for \\(P(t)\\), we get the following relationship:\n\\[\\begin{align} \\int\\frac{1}{P}dP &= \\frac{-2\\Gamma}{\\hbar}\\int dt \\\\ \\ln P &= \\frac{-2\\Gamma t}{\\hbar} + C \\\\ P(t) &= P(0)e^{-2\\Gamma t/\\hbar} \\longrightarrow\\tau \\equiv\\frac{\\hbar}{2\\Gamma}\\end{align}\\]\n\n\n\nProblem 1.18\nVery roughly speaking, quantum mechanics is relevant when the de Broglie wavelength of the particle in question (\\(h/p\\)) is greater than the characteristic size of the system (\\(d\\)). In thermal equilibrium at (Kelvin) temperature \\(T\\), the average kinetic energy of a particle is\n\\[\\frac{p^2}{2m} = \\frac{3}{2}k_B T\\]\n(where \\(k_B\\) is Boltzmann’s constant), so the typical de Broglie wavelength is\n\\[\\lambda = \\frac{h}{\\sqrt{3mk_B T}}.\\]\nThe purpose of this problem is to determine which systems will have to be treated quantum mechanically, and which can safely be described classically.\n\nSolids. The lattice spacing in a typical solid is around \\(d=0.3\\) nm. Find the temperature below which the unbound electrons in a solid are quantum mechanical. Below what temperature are the nuclei in a solid quantum mechanical? (Use silicon as an example.) $$$$ Moral: the free electrons in a solid are always quantum mechanical; the nuclei are generally not quantum mechanical. The same goes for liquids (for which the interatomic spacing is roughly the same), with the exception of helium below 4 K.\nGases. For what temperatures are the atoms in an ideal gas at pressure \\(P\\) quantum mechanical? Hint: Use the ideal gas law (\\(PV = Nk_B T\\)) to deduce the interatomic spacing. $$$$ Answer: \\(T&lt;(1/k_B)(h^2/3m)^{3/5}P^{2/5}\\). Obviously (for the gas to show quantum behavior) we want \\(m\\) to be as small as possible, and \\(P\\) as large as possible. Put in the numbers for helium at atmospheric pressure. Is hydrogen in outer space (where the interatomic spacing is about 1 cm and the temperature is about 3 K) quantum mechanical? (Assume it’s monatomic hydrogen, not H\\(_2\\).)\n\n\nResponse: If we rearrange the first equation given for \\(p\\), we can solve for the characteristic size of the system when things must be treated quantum mechanically, specifically\n\\[d &lt; \\frac{h}{\\sqrt{3mk_B T}}\\]\nwhich corresponds to a temperature of\n\\[T &lt; \\frac{h^2}{3mk_Bd^2}\\]\nbelow which systems must be treated quantum mechanically.\nPlugging in the mass of the electron (\\(m_e = 9.11\\cdot10^{-31}\\) kg) and the mass of a silicon nucleus (\\(m = 28.085m_p\\), \\(m_p = 1.67\\cdot10^{-27}\\) kg) at \\(d = 0.3\\) nm, we temperatures of \\(T&lt;1.3\\cdot10^5\\) K and \\(T &lt; 2.45\\) K, respectively, which jives with the given moral that free electrons are always quantum mechanical and nuclei are generally not.\nLooking into gasses, we can rewrite \\(d\\) in terms of the ideal gas law if we let \\(N=1\\) and \\(V = d^3\\).\n\\[PV = Nk_BT \\longrightarrow d = \\left(\\frac{k_BT}{P}\\right)^{1/3}\\]\nPlugging this result into our equation for \\(T\\), we get the following\n\\[\\begin{align} T &&lt; \\frac{h^2}{3mk_Bd^2} = \\frac{h^2}{3mk_B}\\left(\\frac{P}{k_B T}\\right)^{2/5} \\\\ T^{5/3} &&lt; \\left(\\frac{h^2P^{2/5}}{3mk_B^{5/3}}\\right)^{5/3} \\\\ T &&lt; \\left(\\frac{1}{k_B}\\right)\\left(\\frac{h^2}{2m}\\right)^{3/5}P^{2/5}\\end{align}\\]\nPlugging in numbers for helium (\\(m = 4m_p = 6.8\\cdot10^{-27}\\) kg) at atmospheric pressure (\\(P=10^5\\) N/m\\(^2\\)), using the equation we derived above, and hydrogen (\\(m=1.67\\cdot10^{-27}\\) kg) in outerspace with \\(d=0.01\\) m, using \\(T &lt; \\frac{h^2}{3mk_Bd^2}\\), we get \\(T&lt;2.8\\) K and \\(T&lt;6.35\\cdot10^{-14}\\) K for helium and hydrogen, respectively. These results imply that helium on earth are only quantum mechanical at very low temperatures, while hydrogen in space can be treated classically."
  },
  {
    "objectID": "courses/honors/index.html",
    "href": "courses/honors/index.html",
    "title": "Honors",
    "section": "",
    "text": "Honor’s Project\n\nEDA"
  },
  {
    "objectID": "courses/compGeom/index.html",
    "href": "courses/compGeom/index.html",
    "title": "Computational Geometry",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.path import Path\n\ndef get_polygon_area(x, y):\n    s = 0\n    for i in range(len(x)):\n        s += x[i] * y[i-1] - x[i-1] * y[i]\n    return 0.5 * np.abs(s)\n\n\ndef picks_thm(i, b):\n    return i + b / 2 - 1\n\n\n# square\nx = [0, 1, 1, 0]\ny = [0, 0, 1, 1]\ni = 0\nb = 4\nassert get_polygon_area(x, y) == picks_thm(i, b)\n\n# triangle\nx = [0, 1, 1]\ny = [0, 0, 1]\ni = 0\nb = 3\nassert get_polygon_area(x, y) == picks_thm(i, b)\n\n# 5 sided shape\nx = [-2, 0, 2, 1, -1]\ny = [1, 3, 1, 0, 0]\ni = 4\nb = 8\nassert get_polygon_area(x, y) == picks_thm(i, b)\n\n# something weirder\npoints = [\n    (1, 7),\n    (4, 7),\n    (4, 5),\n    (5, 9),\n    (5, 5),\n    (9, 5),\n    (5, 4),\n    (4, 1),\n    (4, 4),\n    (1, 4),\n        ]\nx = [xi[0] for xi in points]\ny = [yi[1] for yi in points]\ni = 4\nb = 25\nplt.figure(figsize=(8,8))\nplt.plot(x, y)\nplt.scatter(x, y)\nplt.xticks(np.arange(0, 10, 1).tolist())\nplt.yticks(np.arange(0, 10, 1).tolist())\nplt.grid(True)\nassert get_polygon_area(x, y) == picks_thm(i, b)"
  },
  {
    "objectID": "courses/astrophysics/PS7.html",
    "href": "courses/astrophysics/PS7.html",
    "title": "Astrophysics PS7",
    "section": "",
    "text": "Problem 1\nConsider a spectroscopic binary system with a period of 2.17 days. The orbits have zero eccentricity. The maximum Doppler shifts of the fainter and brighter stars are 112.6 km/s and 71.4 km/s, respectively.\n\nHow can an astronomer deduce the eccentricity of the orbits in a spectroscopic binary system? What observations, measurements, modeling, etc. are needed?\nAssuming a statistical value for , determine the total mass of the spectroscopic binary system (in solar masses). Explain your choice of statistical value.\nWhat is the mass of the fainter star?\nWhat is the mass of the brighter star?\n\n\nAnswer:\n\n\nimport astropy.units as u\nimport astropy.constants as c\nimport numpy as np\n\nP = 2.17 / 365 * u.yr\nV1 = 112.6 * u.km / u.s\nV2 = 71.4 * u.km / u.s\nsin3i = 2/3\n\ntotal_mass = P / (2 * np.pi * c.G) * (V1 + V2) ** 3 / sin3i\nprint(f\"total mass: {total_mass.to(u.M_sun):.2f}\")\n\nratio21 = V2 / V1\nm2 = total_mass / (ratio21 + 1)\nm1 = ratio21 * m2\nprint(f\"mass of fainter star: {m1.to(u.M_sun):.2f}\")\nprint(f\"mass of brighter star: {m2.to(u.M_sun):.2f}\")\n\ntotal mass: 2.10 solMass\nmass of fainter star: 0.82 solMass\nmass of brighter star: 1.29 solMass\n\n\n\n\nProblem 2\nAssume that two stars, “1” and “2”, are in circular obits about a common center of mass and are seperated by a distance “a”. Assume an orbital inclination of i and stellar radii of “R1” and “R2” for the two stars.\n\nFind an expression for i that just barely produces an eclipse. Hint: consider the Figure 7.8 from the textbook\n\nThe Kepler space telescope revolutionzed our understanding of extrasolar planets. Hundreds of scholarly manuscripts have been published about these results. One intriguing results is the Kepler-1661 system presented by Socia et al. (2020). Use the information in the paper and apply the equation you deduce for i to calculate:\n\nthe minimum inclination angle for an eclipse to occur at the minimum separation of the primary and secondary stars,\nand the minimum inclination angle for an eclipse to occur at the maximum separation of the primary and secondary stars.\nDo your constraints on i agree with the results presented by Socia et al. (2020)?\n\n\nAnswer:\n\n\\[i &gt; \\cos^{-1}\\left(\\frac{R_1 + R_2}{a}\\right)\\]\n\nR1 = 0.76 * u.R_sun\nR2 = 0.276 * u.R_sun\nP = 28.2 / 365 * u.yr\nm1 = 0.84 * u.M_sun\nm2 = 0.262 * u.M_sun\ne = 0.112\na = (P ** 2 * c.G * (m1 + m2) / (4 * np.pi ** 2)) ** (1/3)\ni = np.arccos((R1 + R2) / a)\ni.to(u.deg)\n\n\\(88.526554\\mathrm{{}^{\\circ}}\\)\n\n\n\n\nProblem 3\n\nConsider an eclipsing, spectroscopic binary system. The orbital period is 5.51 years and the eccentricity is zero. The maximum radial velocity of Star A is 4.7 km/s and the maximum radial velocity of Star B is 17.4 km/s. The time period between first contact (or beginning of eclipse) and minimum light is 0.48 days. The length of the primary minimum is 0.84 days. The apparent bolometeric magnitudes of maximum, primary minimum, and secondary minimum are 6.41, 9.12, and 6.66, respectively. Find the following:\n\n\nThe ratio of stellar masses\nThe sum of stellar masses (in solar masses)\nThe individual masses of Star A and Star B\nThe radii of Star A and Star B\nThe ratio of the effective temperature of the two stars.\n\n\nAnswer:\n\n\ne = 0\nP = 5.51 * u.yr\nv_A = 4.7 * u.km / u.s\nv_B = 17.4 * u.km / u.s\nv = v_A + v_B\ndt_fc_to_ml = 0.48 / 365 * u.yr\nprimary_minimum_length = 0.84 / 365 * u.yr\nB_0 = 6.41\nB_p = 9.12\nB_s = 6.66\n\nmAmB_ratio = v_B / v_A\n\ntotal_mass = P / (2 * np.pi * c.G) * (v_A + v_B) ** 3 / sin3i\nprint(f\"total mass: {total_mass.to(u.M_sun):.2f}\")\n\n\nm_B = total_mass / (mAmB_ratio + 1)\nm_A = mAmB_ratio * m_B\nprint(f\"Mass of Star A: {m_A.to(u.M_sun):.2f}\")\nprint(f\"Mass of Star B: {m_B.to(u.M_sun):.2f}\")\n\nR_s = v * dt_fc_to_ml / 2\nR_l = v * primary_minimum_length / 2 + R_s\n\nprint(f\"Radius of Star A: {R_l.to(u.R_sun):.2f}\")\nprint(f\"Radius of Star B: {R_s.to(u.R_sun):.2f}\")\n\n\ntemp_ratio = ((B_0 - B_p) / (B_0 - B_s)) ** (1/4)\nprint(f\"Temperature ratio (B to A): {temp_ratio:.2f}\")\n\ntotal mass: 3.38 solMass\nMass of Star A: 2.66 solMass\nMass of Star B: 0.72 solMass\nRadius of Star A: 1.81 solRad\nRadius of Star B: 0.66 solRad\nTemperature ratio (B to A): 1.81"
  },
  {
    "objectID": "courses/astrophysics/PS5.html",
    "href": "courses/astrophysics/PS5.html",
    "title": "Astrophysics PS5",
    "section": "",
    "text": "Problem 1\nConsider the following equations:\n\nv/c &lt; 1 for angles satisfying \\(\\frac{\\frac{v_{app}^2}{c^2} - 1}{\\frac{v_{app}^2}{c^2} + 1} &lt; \\cos\\phi &lt; 1\\)\nThat the smallest possible value for v/c for the source is \\(\\frac{v_{min}}{c} = \\sqrt{\\frac{\\frac{v_{app}^2}{c^2}}{1 + \\frac{v_{app}^2}{c^2}}}\\)\nThe smallest possible v/c will occur at an angle \\(\\phi_{min}\\) such that \\(\\cot\\phi_{min} = \\frac{v_{app}}{c}\\)\nThe Lorentz factor corresponding to the minimum v/c is \\(\\gamma_{min} = \\frac{1}{\\sqrt{1 - \\frac{v_{min}^2}{c^2}}} = \\sqrt{1 + v_{app}^2/c^2} = \\frac{1}{\\sin\\phi_{min}}\\)\n\n\nThe inner 6” of the jet from M87 is observed to have an apparent velocity of 4.5c https://ui.adsabs.harvard.edu/abs/2013ApJ…774L..21M/abstract. Using your newly derived equations, estimate the minimum velocity, minimum approaching angle, and Lorentz factor of the jet.\nPlot \\(\\frac{\\beta_{app}}{c}\\) vs \\(\\phi\\) for a variety of Lorentz factors at the source. Summarize your results to describe how superluminal motion depends on the properties of the source and relative position of the observer.\n\n\\[\\frac{v_{app}}{c} = \\frac{\\frac{v}{c}\\sin\\phi}{1 - \\frac{v}{c}\\cos\\phi}\\]\n\nAnswer: Using the equations provided, we can create a simple Python script to calculate the minimum velocity, minimum approaching angle, and Lorentz factor of the jet.\n\n\nimport astropy.units as u\nimport astropy.constants as c\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nv_app = 4.5 * c.c\nv_min = np.sqrt((v_app / c.c) ** 2 / (1 + (v_app/c.c) ** 2))\ncot_theta_min = v_app / c.c\ntheta_min = np.arctan(1 / cot_theta_min)\ngamma_min = 1 / np.sin(theta_min)\n\nprint(f\"v_min / c: {v_min:.2f}\")\nprint(f\"theta_min: {theta_min.to(u.deg):.2f}\")\nprint(f\"Lorentz factor: {gamma_min:.2f}\")\n\nv_min / c: 0.98\ntheta_min: 12.53 deg\nLorentz factor: 4.61\n\n\n\nBelow is a plot of \\(\\frac{\\beta_{app}}{c}\\) vs \\(\\phi\\) for a variety of Lorentz factors.\n\n\nl = np.linspace(1, 10, 101)\nt_min = np.arcsin(1/l)\nbeta = 1/np.tan(t_min)\n\nplt.scatter(y = beta, x = t_min)\nplt.xlabel(\"Minimum angle [rad]\")\nplt.ylabel(\"V_app / c\")\nplt.show()\n\n\n\n\n\n\n\n\n\nFrom this plot, as the angle approaches zero, indicating a jet directed along the line of site, \\(\\beta\\) approaches infinity. Conversely, as the angle increases, indicating a velocity that is entirely in the perpendicular direction, \\(\\beta\\) approaches zero. Thus, the magnitude of the superluminal motion depends primarily on the inclination angle at which an object is observed, assuming, of course, the object has produced sufficiently rapid motion already.\n\n\n\nProblem 2\nAn astronaut travels on a spaceship to a distant stellar system with a constant velocity at 85% the speed of light.\n\nA radio signal is sent from Earth to the spaceship every 4 months (as measured by a clock on Earth). What is the interval between reception of two signals on the ship?\nA return signal is sent from the ship to the Earth every 4 months (as measured by a clock on the ship). What is the interval between reception of two signals on Earth?\nIf the wavelength of the electromagnetic waves sent from Earth is 17 cm, what wavelength must the receivers on the spaceship must be tuned to receive the waves?\n\n\nAnswer: Problems a) and b) are essentially asking the same question: if an object is moving at relativistic speeds to a stationary observer, how will the interval in reception be affected in the stationary frame of reference. In both cases, we can use the time dilation formula.\nFrom a stationary frame of reference, moving clocks run slow. Thus, in both a) and b), we would expect the interval between receivers to be longer than 4 months. This is a similar argument to the Muon example in the textbook: in the muon’s stationary perspective, it has a short life, but to an external, stationary observer, the muon is moving at relativistic speeds and so the time it is alive is dilated, making the observer measure a longer time than the muon. In our case, the spaceship/earth are moving away from their respective stationary reference frames at relativistic speeds, but each party has agreed to transmit at a specific rate of 1 message every four months. Unfortunately, each receiver experiences the delay in messages as taking longer than 4 months due to the relativistic speeds each transmitter is moving at.\nAdditionally, the spaceship must tune their frequencies to a different wavelenth due to the relativistic motion of the transmitters. The relationship between observed and rest frequency is the following \\[\\nu_{obs} = \\nu_{rest} \\sqrt{\\frac{1-v/c}{1+v/c}}\\]\nNote that the conversion between wavelenth and frequency can be found using the simple relationship \\[c = \\lambda \\nu.\\]\nBelow is code that calculates the results described above.\n\n\n# time dilation\nv = 0.85 * c.c\nt_p = 4\ngamma = 1/np.sqrt(1 - (v/c.c) ** 2)\nt = 4 * gamma\n\n# redshift\nl_rest = 17 * u.cm\nf_rest = c.c / l_rest\nf_obs = f_rest * np.sqrt((1 - v/c.c) / (1 + v/c.c))\nl_obs = c.c / f_obs\n\nprint(f\"Interval between reception of two signals on the ship: {t:.2f} months\")\nprint(f\"Interval between reception of two signals on Earth: {t:.2f} months\")\nprint(f\"Wavelength ship receivers tuned to: {l_obs:.2f}\")\n\nInterval between reception of two signals on the ship: 7.59 months\nInterval between reception of two signals on Earth: 7.59 months\nWavelength ship receivers tuned to: 59.70 cm\n\n\n\n\nProblem 3\nBarnard’s star, named after American astronomer Edward E. Barnard (1857-1923), is an orange star in the constellation Ophiuchus. It has the largest proper motion (\\(\\mu=10.3577\\)” yr\\(^{-1}\\)) and the fourth-largest parallax (p = 0.54901”). In the spectrum of Barnard’s star, the H\\(\\alpha\\) absorption line is observed to have a wavelength of 656.034 nm when measured from the ground.\n\nDetermine the radial velocity of Barnard’s star.\nDetermine the transverse velocity of Barnard’s star.\nCalculate the speed of Barnard’s star through space.\nConsider that the Sun orbits the Milky Way at ~220 km/s, is Barnard’s star traveling particularly fast?\n\n\nAnswer: If we calculate the redshift \\[z = \\frac{\\lambda_{obs} - \\lambda_{rest}}{\\lambda_{rest}}\\] of the H\\(\\alpha\\) absorption line, we can use the following relationship to determine the radial velocity. \\[\\frac{v_r}{c} = \\frac{(z+1)^2 - 1}{(z+1)^2 + 1}\\]\nFor the transverse velocity, we need to multiply the proper motion and the distance to the target (which can be found via parallax), by a conversion factor of 4.74 to convert to units of km/s. \\[v_t = 4.74\\mu r\\] Adding the radial and transverse velocities in quadrature results in the total velocity of the star. \\[v = \\sqrt{v_r^2 + v_t^2}\\] Below is a Python script to perfom the above calculations.\n\n\n# a\nl_obs = 656.034 * u.nm\np = 0.54901 * u.arcsec\nmu = 10.3577 * u.arcsec / u.yr\n\nl_rest = 656.281 * u.nm\nz = (l_obs - l_rest) / l_rest\nv_r = (((z + 1) ** 2 - 1) / ((z + 1) ** 2 + 1)) * c.c\n\n# b\nr = p.to(u.pc, equivalencies=u.parallax())\nv_t = 4.74 * u.yr / u.arcsec / u.pc * u.km / u.s * mu * r \n\n#c\nv = np.sqrt(v_r ** 2 + v_t ** 2)\n\nprint(f\"radial velocity: {v_r.to(u.km / u.s):.2f}\")\nprint(f\"transverse velocity: {v_t:.2f}\")\nprint(f\"velocity: {v.to(u.km / u.s):.2f}\")\n\nradial velocity: -112.85 km / s\ntransverse velocity: 89.43 km / s\nvelocity: 143.99 km / s\n\n\n\nNote that the speed of Bernard’s star is less than that of the Sun with respect to the Milky Way, so the final speed is not particularly fast.\n\n\n\nProblem 4\nA white dwarf is a very dense star, with its ions and electrons packed extremely close together. Each electron may be considered to be located within a region of size \\(\\Delta x\\sim1.5\\times10^{-12}\\) m.\n\nUse Heisenberg’s uncertainty principle to estimate the minimum speed of the electron.\n\nDo you think that the effects of relativity will be important for these stars?\n\nAgain, using the ADS and/or ArXiV search engine tools, identify a recent (within the past few years) peer-reviewed scholarly manuscript that deals with white dwarf stars. Provide a brief summary of the main results of the paper, including a discussion of how the material connects to the concepts that we discussed in Chapter 5 of the textbook.\n\n\nAnswer: Rearranging the Heisenberg uncertainty principle to solve for the velocity \\[\\Delta v = \\frac{\\hbar}{2 m_e \\Delta x}\\] allows us to find the minimum velocity an electron must travel at given we know it’s location with a given uncertainty.\nA simple Python script below performs the above calculations.\n\n\ndx = 1.5E-12 * u.m\n\ndv = c.hbar / (2 * dx * c.m_e) \ndv.to(u.m / u.s)\n\n\nprint(f\"minimum speed of electron: {dv.to(u.m / u.s):.2e}\")\nprint(f\"v / c = {dv.to(u.m / u.s) / c.c:.2f}\")\n\nminimum speed of electron: 3.86e+07 m / s\nv / c = 0.13\n\n\n\nNote that the speed is just over 10% the speed of light, which is much greater than normal, everyday speeds we are used to. Given this, I would say relativistic effects would be just starting to have a noticable effect.\nThe Optically Thick Rotating Magnetic Wind from a Massive White Dwarf Merger Product. II. Axisymmetric Magnetohydrodynamic Simulations:\nSometimes white dwarfs collide, and when they do, they could: 1) trigger a supernova, 2) create a neutron star, or 3) merge into a single white dwarf. The latter was the topic of this paper, in particular, this team modeled the stellar wind that is created by the fast spinning and strong magnetic fields and carries mass away from the star. This team found that the wind is fastest and most luminous near the star’s equator and blows with an inconsistancy: some of the gas gets trapped in the magnetic field and subsequently ejected with the rearranging of the magnetic field lines. This is related to Chapter 5 in our textbook primarily with the fact that white dwarfs are found via spectroscopic (and photometric) methods, however, observationally, winds of a neutron star are measured via spectroscopy."
  },
  {
    "objectID": "courses/astrophysics/PS3.html",
    "href": "courses/astrophysics/PS3.html",
    "title": "Astrophysics PS3",
    "section": "",
    "text": "Problem 1\nConsider a model of a subgiant star with a surface temperature 27,000 K and radius 7.4 times as large as the Sun. The parallax measured for the stars is 0.008 arcseconds. Calculate\n\nThe luminosity in solar luminosity\nAbsolute bolometric magnitude\nApparent bolometric magnitude\nDistance modulus\nRadiant flux at the surface of the star\nRadiant flux at the surface of Jupiter\nPeak wavelength of the star’s spectrum\n\n\nAnswer:\n\n\nimport astropy.units as u\nimport astropy.constants as c\nimport math\nimport numpy as np\nT_e = 27000 * u.K\nparallax = 0.008 * u.arcsec\nR = 7.4 * c.R_sun\nm_sun = -27 \nM_sun = 4.74\n\ndistance = parallax.to(u.pc, equivalencies=u.parallax())\n\nsurface_flux = c.sigma_sb * T_e ** 4\nL = 4 * math.pi * R ** 2 * surface_flux\nflux = L / (4 * math.pi * distance ** 2)\nflux_jupiter = L / (4 * math.pi * (5.2 * u.au) ** 2)\nprint(M_sun,c.L_sun.value)\nM = M_sun - 2.5 * math.log10(L / c.L_sun)\nm = M + 5 * math.log10(distance / (10 * u.pc))\nl_max = 0.0029 * u.m * u.K / T_e\n\nprint(f\"Luminosity (L): {L.to(u.L_sun):.2e}\")\nprint(f\"Absolute bolometric magitude (M): {M:.2f}\")\nprint(f\"Relative bolometric magnitude (m): {m:.2f}\")\nprint(f\"Distance modulus (m-M): {m-M:.2f}\")\nprint(f\"Radiant flux at surface of star: {surface_flux:.2e}\")\nprint(f\"Radiant flux at surface of Jupiter: {flux_jupiter.to(u.W / u.m ** 2):.2e}\")\nprint(f\"Peak wavelength (l_max): {l_max.to(u.nm)}\")\n\n4.74 3.828e+26\nLuminosity (L): 2.62e+04 solLum\nAbsolute bolometric magitude (M): -6.31\nRelative bolometric magnitude (m): -0.82\nDistance modulus (m-M): 5.48\nRadiant flux at surface of star: 3.01e+10 W / m2\nRadiant flux at surface of Jupiter: 1.32e+06 W / m2\nPeak wavelength (l_max): 107.40740740740739 nm\n\n\n\n\nProblem 2\nA \\(1.2\\times10^4\\) kg spacecraft is launched from Earth and is to be accelerated radially away from the Sun using a circular solar sail. The initial acceleration of the spacecraft is to be 1g. Assuming a flat sail, determine the radius of the sail if it is\n\nBlack, so it absorbs the Sun’s light.\nShiny, so it reflects the Sun’s light.\nHow does this proposed spacecraft compare to what NASA is considering for it’s next generation of proposed solar sail enabled spacecraft: https://www.nasa.gov/general/nasa-next-generation-solar-sail-boom-technology-ready-for-launch/ Hint: The spacecraft, like Earth, is orbiting the Sun. Should you include the Sun’s gravity in your calculation?\n\n\nAnswer:\n\n\nd = 1 * u.au\nm = 1.2E4 * u.kg\na = 9.8  * u.m / u.s ** 2\ntheta = 0 * u.deg\nflux = c.L_sun / (4 * math.pi * d ** 2)\nr_abs = ((m * a * c.c) / (flux * math.pi * math.cos(0))) ** 0.5\nr_ref = ((m * a * c.c) / (2 * flux * math.pi * math.cos(0)**2)) ** 0.5\nprint(f\"{r_abs.to(u.km):.2f}\")\nprint(f\"{r_ref.to(u.km):.2f}\")\n\n90.80 km\n64.20 km\n\n\n\n\nProblem 3\nWe have discussed the blackbody spectral energy distributions of individual stars. Many studies have examined the relationships between the total stellar mass, luminosity, and observed colors of composite stellar populations (e.g., the millions or billions of stars found in typical galaxies). One such study is Hermann et al. (2016), which studies the properties of low-mass irregular galaxies. This approach is applied in many subsequent works (e.g., Gault et al. 2021). Consider a galaxy with the following measure global properties, measured in the SDSS g and r filters. Information on the SDSS filters can be found here; note that g and g’ filters are interchangeable.\nApparent magnitude in the r band: mr=18.02\nDistance: D = 76 Mpc\nMeasured color: (g - r) = 0.30\nUsing this information, and the M/L relation for the SDSS r-band relation from Herrmann et al. (2016), calculate the following characteristics. You may assume that the absolute magnitude of the Sun in the SDSS r-filter is r = +4.65.\n\nAbsolute magnitude in the r band\nM/L ratio (using Solar units)\nlog10(L/Lsun)\nlog10(M/Msun)\n\n\nAnswer:\n\n\nm_r = 18.02\nD = 76 * u.pc * 1E6\na_lambda = -0.313\nb_lambda = 0.894\ncolor = 0.30\nM_r = m_r - 5 * np.log10(D.value) + 5\nML = 10 ** (a_lambda + b_lambda * color)\nLlog = (M_r - M_sun) / -2.5\nMlog = math.log10(10 ** Llog)\nprint(f\"{10 ** Llog:.2e}\")\nprint(f\"Absolute magnitude in r band: {M_r}\")\nprint(f\"M/L ratio: {ML * u.M_sun / u.L_sun :.2f}\")\nprint(f\"log10(L/Lsun): {Llog}\")\nprint(f\"log10(M/Msun): {Mlog}\")\n\n2.82e+08\nAbsolute magnitude in r band: -16.384067961403954\nM/L ratio: 0.90 solMass / solLum\nlog10(L/Lsun): 8.449627184561582\nlog10(M/Msun): 8.449627184561582"
  },
  {
    "objectID": "courses/astrophysics/PS17.html",
    "href": "courses/astrophysics/PS17.html",
    "title": "Astrophysics PS17",
    "section": "",
    "text": "Problem 1\n\nimport astropy.units as u\nimport astropy.constants as const\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef c(rho1, rho2, r1, r2):\n    return (rho2 * r2) / (rho1 * r1)\n\ndef a(r1, r2, c):\n    return (r1 - r2 * c ** 0.5) / (c ** 0.5 - 1)\n\ndef rho0(rho, r1, a):\n    return rho * (r1 / a) * (1 + r1 / a) ** 2\n\ndef rhoNSF(rho0, r, a):\n    return rho0 / ((r / a) * (1 + r / a) ** 2)\n\n\nr_h1 = 8.04 * u.kpc\nv_h1 = 243.23 * u.km / u.s\nr_prcg = 14.95 * u.kpc\nv_prcg = 259.26 * u.km / u.s\nr_hkg = 16.55 * u.kpc\nv_hkg = 261.17 * u.km / u.s\n\n\n# a) calculate virial mass\nm_h1 = r_h1 * v_h1 ** 2 / (2 * const.G)\nm_prcg = r_prcg * v_prcg ** 2 / (2 * const.G)\nm_hkg = r_hkg * v_hkg ** 2 / (2 * const.G)\nprint(f\"{m_h1.to(u.M_sun):.2e}\")\nprint(f\"{m_prcg.to(u.M_sun):.2e}\")\nprint(f\"{m_hkg.to(u.M_sun):.2e}\")\n\n\n# b)\nrho_h1 = m_h1 / (4 * np.pi * r_h1 ** 3 / 3)\nrho_prcg = m_prcg / (4 * np.pi * r_prcg ** 3 / 3)\nrho_hkg = m_hkg / (4 * np.pi * r_hkg ** 3 / 3)\nd = np.linspace(0, 100, 101) * u.kpc\n\nc_1 = c(rho_h1, rho_prcg, r_h1, r_prcg)  # h1 to PRCG\nc_2 = c(rho_h1, rho_hkg, r_h1, r_hkg)  # h1 to HKG\nc_3 = c(rho_hkg, rho_prcg, r_hkg, r_prcg)  # HKG to PRCG\n\na_1 = a(r_h1, r_hkg, c_1)\na_2 = a(r_h1, r_prcg, c_2)\na_3 = a(r_hkg, r_prcg, c_3)\n\n\nrho0_1 = rho0(rho_h1, r_h1, a_1)\nrho0_2 = rho0(rho_h1, r_h1, a_2)\nrho0_3 = rho0(rho_hkg, r_hkg, a_3)\n\nrhoNSF_1 = rhoNSF(rho0_1, d, a_1)\nrhoNSF_2 = rhoNSF(rho0_2, d, a_2)\nrhoNSF_3 = rhoNSF(rho0_3, d, a_3)\n\nplt.plot(d, rhoNSF_1, label=\"HI and PRCG\")\nplt.plot(d, rhoNSF_2, label=\"HI and HKG\")\nplt.plot(d, rhoNSF_3, label=\"HKG and PRCG\")\nplt.xscale('log')\nplt.yscale('log')\nplt.legend()\nplt.xlabel(\"Distance [kpc]\")\nplt.ylabel(\"Density [$M_{\\\\odot}$ / kpc$^3$]\")\nplt.show()\n\n5.53e+10 solMass\n1.17e+11 solMass\n1.31e+11 solMass\n\n\nc:\\Users\\qwert\\anaconda3\\envs\\pyscope-dev\\Lib\\site-packages\\astropy\\units\\quantity.py:671: RuntimeWarning:\n\ndivide by zero encountered in divide"
  },
  {
    "objectID": "courses/astrophysics/PS15.html",
    "href": "courses/astrophysics/PS15.html",
    "title": "Astrophysics PS15",
    "section": "",
    "text": "Problem 1\n\nUsing the recent formulation of the Cepheid period-luminosity relation from Riess et al. (2019), and the geometric distance to the Large Magellanic Cloud derived by Pietrzyński et al. (2019) (PDF of Nature article available on Moodle), re-cast the Cepheid Period-Luminosity relation in the Hubble Space Telescope’s F814W filter. It should take the form:\n\n\\[M_{F814W} = X \\cdot (P/days) + Y\\]\n\nWhat does this recasting imply the absolute magnitude should be for a Cepheid with a period of 1.1 days?\nPredict the apparent magnitude of the HST F814W filter of a Cepheid variable in the Local Volume galaxy Leo P with the same oscillation period. Use the distance to Leo P determined by McQuinn et al. (2015).\n\nYou may assume reddening-free situations for the Cepheids, and neglect the Wesenheit indices.\n\nAnswer: Given the magnitude\n\n\nimport numpy as np\nimport astropy.units as u\n\n# b)\nY = 16.854 - 5 * np.log10(49.59E3) + 5\nX = -2.96\nP = 1.1\nM = X * P + Y\nprint(M)\n\n# c)\nm = 5 * np.log10(1.62E6) - 5 + M\nprint(m)\n\n-4.878970541455551\n21.168604531257603\n\n\n\n\nProblem 2\nThe Zwicky Transient Facility (ZTF) has revolutionized our understanding of pulsating stars. It has now discovered the majority of known pulsators of multiple classes. The manuscript by Chen et al. (2020) presents the ZTF catalog of pulsating stars. Read the manuscript to learn about the many types of pulsating stars.\n\nUsing Table 2 of Chen et al. (2020), create a plot of pulsation period (in units of days) versus r-band magnitude for five different types of pulsating stars: Cepheid I, Cepheid II, RR Lyrae ab, RR Lyrae c, and Delta Scuti. Colorize and label each type of star in your plot. Use a logarithmic x-axis, with limits of 0.025 to 80 days. Use a linear axis for the magnitude, from +22 to +11.\n\nIf you’d rather start from presorted data, text files with the Table 2 data are provided for each pulsating star type are provided on Moodle.\n\nComment on the differences between these pulsating stars, and what the expanded sample size provided by ZTF may help future research endeavors uncover or better understand.\n\n\nAnswer: If we measure the magnitude\n\n\nfrom astropy.io import ascii\nimport matplotlib.pyplot as plt\ndata = ascii.read(\"https://content.cld.iop.org/journals/0067-0049/249/1/18/revision1/apjsab9caet2_mrt.txt\")\n\ncepI = data.group_by(\"Type\").groups[1]\ncepII = data.group_by(\"Type\").groups[2]\nRRab = data.group_by(\"Type\").groups[7]\nRRc = data.group_by(\"Type\").groups[8]\nD = data.group_by(\"Type\").groups[3]\n\n\n# plot r-band mag vs period for all stars\n# color and label by star type\n# log x axis, limits 0.025 to 80 days, linear magnitude axis, from 22 to 11\nplt.scatter(D[\"Per\"], D[\"rmag\"], label = \"Delta Scuti\", alpha = 0.5)\nplt.scatter(cepI[\"Per\"], cepI[\"rmag\"], label = \"Cep I\", alpha = 0.5)\nplt.scatter(cepII[\"Per\"], cepII[\"rmag\"], label = \"Cep II\", alpha = 0.5)\nplt.scatter(RRab[\"Per\"], RRab[\"rmag\"], label = \"RR ab\", alpha = 0.5)\nplt.scatter(RRc[\"Per\"], RRc[\"rmag\"], label = \"RR c\", alpha = 0.5)\nplt.xlabel(\"Period (days)\")\nplt.ylabel(\"r magnitude\")\nplt.legend()\nplt.xscale(\"log\")\nplt.xlim(0.025, 80)\nplt.ylim(22, 11)"
  },
  {
    "objectID": "courses/astrophysics/PS13.html",
    "href": "courses/astrophysics/PS13.html",
    "title": "Astrophysics PS13",
    "section": "",
    "text": "Problem 1\nThe relationships for the Jeans mass, the Jeans length, and the free-fall timescale are fundamental. In this problem, you will explore these relationships over a range of physical properties. Consider a cloud of pure molecular hydrogen. Create two plots to demonstrate the behaviors of these fundamental relations:\n\nJeans Mass versus temperature: Plot the Jeans Mass (in units of Solar masses) versus temperature over the range 5 K \\(\\leq T \\leq\\) 100 K. Show curves for five different densities, starting at \\(1\\times10^{-17}\\) kg m\\(^{-3}\\) and increasing by a factor of two for each subsequent calculation. Clearly label each curve with its value of density and also with its characteristic free-fall timescale (in units of Myr).\nJeans Length versus temperature: Plot the Jeans Length (in units of pc) versus temperature over the range 5 K \\(\\leq T \\leq\\) 100 K. Show curves for five different densities, starting at \\(1\\times10^{-17}\\) kg m\\(^{-3}\\) and increasing by a factor of two for each subsequent calculation. Clearly label each curve with its value of density and also with its characteristic free-fall timescale (in units of Myr).\nIn a few sentences describe the trends you observe in each of the plots you created.\n\n\nAnswer: The Jeans Mass increases as temperature increases, but increases more rapidly for lower density material. The lowest density material also has the longest time to collapse. The Jeans Radius increases more slowly at higher temperatures across all densities, however the Jeans Radius for lower density material is higher than the Jeans Radius for the higher density material across all temperatures.\n\n\nimport astropy.units as u\nimport astropy.constants as const\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef jeans_mass(T, mu, rho):\n    M_j = ((5 * const.k_B * T) / (const.G * mu * const.m_p\n        )) ** (3/2) * (3 / (4 * np.pi * rho)) ** (1/2)\n    return M_j.to(u.M_sun)\n    \ndef jeans_radius(T, mu, rho):\n    R_j = ((15 * const.k_B * T) / (4 * np.pi * const.G * mu * const.m_p * rho)) ** (1/2)\n    return R_j.to(u.pc)\n\ndef t_ff(rho):\n    t = np.sqrt((3 * np.pi)/(32 * const.G * rho))\n    return t.to(u.Myr)\n\nT = np.linspace(5, 100, 96) * u.K\nmu = 1\nrho = 1E-17 * u.kg / u.m ** 3\n\nplt.figure(figsize=(8,6))\nplt.subplot(2,1,1)\nplt.plot(T, jeans_mass(T, mu, rho), label = f\"{rho}, t_ff = {t_ff(rho):.2f}\")\nplt.plot(T, jeans_mass(T, mu, 2 * rho), label = f\"{2 * rho}, t_ff = {t_ff(2 * rho):.2f}\")\nplt.plot(T, jeans_mass(T, mu, 4 * rho), label = f\"{4 * rho}, t_ff = {t_ff(4 * rho):.2f}\")\nplt.plot(T, jeans_mass(T, mu, 8 * rho), label = f\"{8 * rho}, t_ff = {t_ff(8 * rho):.2f}\")\nplt.plot(T, jeans_mass(T, mu, 16 * rho), label = f\"{16 * rho}, t_ff = {t_ff(16 * rho):.2f}\")\nplt.xlabel(\"Temperature [K]\")\nplt.ylabel(\"Jeans Mass [Solar Mass]\")\nplt.title(\"Jeans Mass vs Temperature For Various Densities\")\nplt.legend()\n\nplt.subplot(2,1,2)\nplt.plot(T, jeans_radius(T, mu, rho), label = f\"{rho}, t_ff = {t_ff(rho):.2f}\")\nplt.plot(T, jeans_radius(T, mu, 2 * rho), label = f\"{2 * rho}, t_ff = {t_ff(2 * rho):.2f}\")\nplt.plot(T, jeans_radius(T, mu, 4 * rho), label = f\"{4 * rho}, t_ff = {t_ff(4 * rho):.2f}\")\nplt.plot(T, jeans_radius(T, mu, 8 * rho), label = f\"{8 * rho}, t_ff = {t_ff(8 * rho):.2f}\")\nplt.plot(T, jeans_radius(T, mu, 16 * rho), label = f\"{16 * rho}, t_ff = {t_ff(16 * rho):.2f}\")\nplt.xlabel(\"Temperature [K]\")\nplt.ylabel(\"Jeans Radius [pc]\")\nplt.title(\"Jeans Radius vs Temperature For Various Densities\")\nplt.subplots_adjust(hspace=0.5)\n\n\n\n\n\n\n\n\n\n\nProblem 2\nPick a recently published paper that investigates the IMF Here is a link to a refined ADS search that may be a good place to start.\nAnswer the following questions:\n\nHow do these astronomers investigate the IMF? (Describe data, models, etc.)\nWhat do they conclude about the IMF?\nHow do their results compare to what we talked about in class?\n\n\nAnswer: We read: Early Enrichment Population Theory at High Redshift by Blackwell and Bregman, published in The Astrophysical Journal on January 20, 2025.\n\nThe visible galaxies and stars in a galaxy cluster cannot have produced the measured ICM metallicity. In order to have the observed intracluster medium metallicity of glaxy clusters, a theory exists that says a population of stars must have existed at high redshifts that populated the clusters with the missing metals. Previous work argued the IMF of this theory must be flatter than lower redshift IMFs, but they didn’t have that high precision. This work used supernovae, delay time distributions, and luminosity functions of galaxy clusters to derive two best-fit IMFs that are flatter than standard IMFs.\nTheir model can successfully produce the observed ICM metallicity and agrees with observations. Interestingly, the model also predicts a rise in the Type Ia supernova rate at increasing redshift.\nIn class, we only talked about the interstellar medium. This paper introduced the idea of an intrcluster medium, which is like the ISM but on much larger scales. Overall, it seems the process for determining the two is just a scaled mapping; instead of looking at clusters of stars, like we do for the ISM, we look at clusters of galaxies."
  },
  {
    "objectID": "courses/astrophysics/PS11.html",
    "href": "courses/astrophysics/PS11.html",
    "title": "Astrophysics PS11",
    "section": "",
    "text": "Problem 1\n\nUsing the equation that gives the FWHM at a given wavelength thermal and turbulent motions (given below. Note in class, we just saw the factor by which the line is broaden), estimate full width at half-maximum of the hydrogen H\\(\\alpha\\) absorption line due to random thermal motions in the Sun’s photosphere. Assume the temperature is the Sun’s effective temperature.\nUsing H\\(\\alpha\\) redshift data for solar granulation (discussed in Ch. 11 of the textbook), estimate the FWHM when convective turbulent motions are included with thermal motions.\nWhat is the ratio of \\(v^2_{turb}\\) to \\(2kT/m\\)?\nDetermine the relative change in the FWHM due to Doppler broadening when turbulence is included. Does turbulence make a significant contribution to FWHM in the solar photosphere?\n\n\nAnswer: The following code chunk calculates the answers to the questions above. Given the relative change in part d), turbulence does not make a significant contribution to FWHM in the solar photosphere.\n\n\nimport astropy.units as u\nimport astropy.constants as c\nimport numpy as np\n\nT = 5777 * u.K\nhalpha = 656 * u.nm\nv_turb = 0.4 * u.km / u.s\nm = c.m_p\nk = c.k_B\n\n# a\nfwhm_no_turb = 2 * halpha / c.c * np.sqrt(2 * k * T / m * np.log(2))\nprint(f\"fwhm without turbulence: {fwhm_no_turb.to(u.angstrom):.4f}\")\n\n# b\nfwhm = 2 * halpha / c.c * np.sqrt((2 * k * T / m + v_turb ** 2) * np.log(2))\nprint(f\"fwhm with turbulence: {fwhm.to(u.angstrom):.4f}\")\n\n# c\nratio = v_turb ** 2 / (2 * k * T / m)\nprint(f\"v_turb^2 is {100 * ratio.to(u.dimensionless_unscaled):.4f}% of 2kT/m\")\n\n# d\nrelative_change = (fwhm - fwhm_no_turb) / fwhm_no_turb\nprint(f\"relative change in FWHM with turbulence: {100 * relative_change:.4f}%\")\n\nfwhm without turbulence: 0.3558 Angstrom\nfwhm with turbulence: 0.3561 Angstrom\nv_turb^2 is 0.1678% of 2kT/m\nrelative change in FWHM with turbulence: 0.0838%\n\n\n\n\nProblem 2\nSuppose you are attempting to make observations through an optically thick gas that has a constant density and temperature. Assume that the density and temperature of the gas are \\(2.2\\times10^{-4}\\) kg m\\(^{-3}\\) and 5777 K, respectively, typical of the values found in the Sun’s photosphere.\na) If the opacity of the gas at one wavelength (\\(\\lambda_1\\)) is \\(\\kappa_{\\lambda 1} = 0.026\\) m\\(^2\\) kg\\(^{-1}\\) and the opacity at another wavelength (\\(\\lambda_2\\)) is $_{} = 0.030 $ m\\(^2\\) kg\\(^{-1}\\), calculate the distance into the gas where the optical depth equals 2/3 for each wavelength.\nb) At which wavelength can you see farther into the gas? c) How much farther?\n\nAnswer: The code chunk below calculates the distances for \\(\\lambda_1\\) and \\(\\lambda_2\\) from the given the opacities, density, and optical depths. From this result, we find that \\(\\lambda_1\\) has the larger distance, thus we can see further into the gas at \\(\\lambda_1\\) by about 15 km.\n\n\ntau_1, tau_2 = 2/3, 2/3\nrho = 2.2E-4 * u.kg / u.m ** 3\nkappa_1 = 0.026 * u.m ** 2 / u.kg\nkappa_2 = 0.030 * u.m ** 2 / u.kg\n\ns_1 = tau_1 / (kappa_1 * rho)  # distance for lambda 1\ns_2 = tau_2 / (kappa_2 * rho)  # distance for lambda 2\nprint(f\"s1: {s_1:.2e}\")\nprint(f\"s2: {s_2:.2e}\")\nprint(f\"difference: {s_1 - s_2:.2f}\")\n\ns1: 1.17e+05 m\ns2: 1.01e+05 m\ndifference: 15540.02 m\n\n\n\n\nProblem 3\nAssume that a large solar flare erupts in a region where the magnetic field strength is 0.03 T and that it releases 1025 J in one hour. a) What was the magnetic energy density in that region before the eruption began? b) What minimum volume would be required to supply the magnetic energy necessary to fuel the flare? c) Assuming for simplicity, that the volume involved in supplying the energy for the flare eruption was a cube, compare the length of the one side of the cube with the typical size of a large flare. d) How long would it take an Alfven wave to travel the length of the flare?\n\nAnswer: The code chunk below calculates the answers the questions above. The energy density was found using \\[u_m = \\frac{B^2}{2\\mu_0},\\]\nwhich allowed us to find the volume by dividing the given energy by our answer for energy density. Assuming the volume was a cube, then a side length is the cube root of the total volume, which results in a length \\(1.42\\cdot^{-8}\\) times the size of a large flare (100,000 km).\nThe alfven speed of a wave is given by \\[v_a = \\frac{B}{\\sqrt{\\mu_0 \\rho}},\\]\nwhere \\(\\rho\\) is the density of the photosphere, which is given in the textbook to be \\(4.9\\cdot10^{-6}\\) kg / m\\(^3\\). Dividing the length of our cube volume by the alfven speed gives the total time for an alfven wave to traverse the flare, which we estimate to be around \\(1.17\\cdot10^{-4}\\) s.\n\n\nB = 0.03 * u.T\nenergy = 1025 * u.J\ndensity = 4.9E-6 * u.kg / u.m ** 3\n\n# a\nu_m = B ** 2 / (2 * c.mu0)\nprint(f\"energy density of flare: {u_m.to(u.J / u.m ** 3):.2e}\")\n\n# b\nvolume = energy / u_m\nprint(f\"minimum volume of flare: {volume.to(u.m ** 3):.2e}\")\n\n# c\nlength = volume ** (1/3)\nlarge_flare_length = 1E5 * u.km\nratio = (length / large_flare_length).to(u.dimensionless_unscaled)\nprint(f\"length of cube with volume above: {length.to(u.m):.2e}\")\nprint(f\"the cube length is {ratio:.2e} times the size of a large flare\")\n\n# d\nv_alfven = B / np.sqrt(c.mu0 * density)\ntime = length / v_alfven\nprint(f\"alfven speed of wave: {v_alfven.to(u.m / u.s):.2e}\")\nprint(f\"time for alfven wave to traverse length of flare: {time.to(u.s):.2e}\")\n\nenergy density of flare: 3.58e+02 J / m3\nminimum volume of flare: 2.86e+00 m3\nlength of cube with volume above: 1.42e+00 m\nthe cube length is 1.42e-08 times the size of a large flare\nalfven speed of wave: 1.21e+04 m / s\ntime for alfven wave to traverse length of flare: 1.17e-04 s"
  },
  {
    "objectID": "courses/astrophysics/PS1.html",
    "href": "courses/astrophysics/PS1.html",
    "title": "Astrophysics PS1",
    "section": "",
    "text": "Problem 1\nFor Winer Observatory,\n\nFind the altutude of the Sun along the meridian on the first day of summer and\n\n\nAnswer: The first day of summer is the corresponds to the summer solstice, at which the Sun is at a declination of around 23.5 degrees. Winer observatory is located at a latitude of 31.67 degrees. The following diagram can be made for an observer at Winer.\n\nFrom this geometry, we can construct the following relationship between the zenith \\(z\\), declination \\(\\delta\\), and altitude \\(A\\)\n\\[A = 90 - z + \\delta\\]\nUsing this equation, we can plug in the zenith/latitude of Winer and the declination of the Sun at the summer solstice to find the altitude of the Sun at Winer on the summer solstice.\n\\[ A = 90 - 31.67 + 23.5 = \\boxed{81.83 \\text{ deg}} \\]\n\n\nthe maximum altidue of the sun on the first day of winter.\n\n\nAnswer: Using the equation above, we can plug in the declination of the Sun on the winter solstice (-23.5 degrees) to solve for the altitude from Winer.\n\\[ A = 90 - 31.67 - 23.5 = \\boxed{34.83 \\text{ deg}} \\]\n\n\nHow do you expect these altitudes to change for an observer at Macalester?\n\n\nAnswer: Macalester is at a higher latitude, so we would expect the respective altitudes to be lower. Using the nifty equation we found early, we can find how much lower exactly.\nFor some target at Winer, the altitude is\n\\[A_w = 90 - z_w + \\delta \\]\nFor the same target at Mac, the altitude is\n\\[A_m = 90 - z_m + \\delta\\]\nThus, the difference between altitudes at Mac and Winer is\n\\[A_m - A_w = z_w - z_m = 31.67 - 45 = -13.33\\]\nand\n\\[\\boxed{A_m = A_w - 13.33}\\]\nSo the altitudes at Mac are 13.33 degrees lower than the altitudes at Winer.\n\n\n\nProblem 2\nCongratulations! You’ve been awarded 4 hours of observing time on the Subaru Telescope on Manua Kea on March 18 (yay, spring break trip to Hawaii!).\n\nWhat is the Subaru telescope? What portions of the electromagnetic spectrum will you observe in?\n\n\nAnswer: The Subaru telescope is an 8.2 meter optical and near infrared telescope located near the summit of Maunakea, Hawaii, operated by the National Astronomical Observatory of Japan (NAOJ), National Institutes of Natural Sciences. We could abserve in visible and near infrared portions of the electromagnetic spectrum.\n\n\nOf the following objects, which will you be able to observe if your time starts at 10:00 PM? NGC 13, NGC 2213, NGC 3019, NGC 4214, NGC 5808, NGC 6680, NGC 7111. You may find the NASA/IPAC Extragalactic Database (aka NED, https://ned.ipac.caltech.edu/) helpful.\n\n\nAnswer: Below is a table of target names and coordinates.\n\nTarget list\n\n\nname\n\\(\\alpha\\) [deg]\n\\(\\delta\\) [deg]\n\n\n\n\nNGC 13\n2.1987\n33.4333\n\n\nNGC 2213\n92.6753\n-71.5286\n\n\nNGC 3019\n147.5299\n12.7461\n\n\nNGC 4214\n183.9132\n36.3269\n\n\nNGC 5808\n223.5115\n73.1317\n\n\nNGC 6680\n279.9332\n22.3165\n\n\nNGC 7111\n325.4739\n-6.7088\n\n\n\nMarch 18th is nearly the vernal equinox, which means the Sun will have a right ascension around 0 deg. Thus, we can consider the bad range of \\(\\alpha\\) values to be 270-90 deg and the good range of \\(\\alpha\\) values to be 90-270 deg. This allows us to get rid of NGC 13, NGC 6680, and NGC 7111 due to their \\(\\alpha\\) values near the Sun.\nWe can also get rid of NGC 2213, as it has a declination of -71 degrees, which is unobservable from the Subaru telescope given its latitude of 19.8255 degrees North (19.8255 - 90 = -70 = minimum declination for possible observation).\n\nRevised target list\n\n\nname\n\\(\\alpha\\) [deg]\n\\(\\delta\\) [deg]\n\n\n\n\nNGC 3019\n147.5299\n12.7461\n\n\nNGC 4214\n183.9132\n36.3269\n\n\nNGC 5808\n223.5115\n73.1317\n\n\n\nSince we are near the vernal equinox, at noon we expect the Sun to be directly overhead, making the LST around 0 hours. At midnight, we expect the LST to be around 12. Thus two hours earlier, when our observation block starts, we expect to have an LST of around 10. We know the following relation.\n\\[HA = LST - \\alpha\\]\nIf we convert our \\(\\alpha\\)’s to hours from degrees, we can solve for the hour angles of each target to determine if we can see them in our observing block.\n\\[HA = 10 - \\begin{pmatrix}9.8 \\\\ 12.2 \\\\ 14.9\\end{pmatrix} = \\begin{pmatrix}0.2 \\\\ -2.2 \\\\ -4.9\\end{pmatrix}\\]\nIn other words, at 10 pm local time at the Subaru Telescope, NGC 3019 will be almost directly overhead; two hours later, NGC 4214 will be transiting; and at the end of our session, NGC 5808 will be rising/an hour away from transiting.\n\n\n\nProblem 3\nUse Simbad (https://simbad.cds.unistra.fr/simbad/) to look up the parallax and J2000 coordinates to the three brightest stars in the constellation Orion’s belt: Alnilam, Alnitak, and Mintaka.\n\nWhat can you learn about the motion of these stars on Simbad\n\n\nAnswer: Alnilam is moving at 1.44 and -0.78 milliarcseconds/year in right ascension and declination, respectively, and away from us at 27.30 km/s. Alnitak is moving at 3.19 and 2.03 milliarcseconds/year in right ascension and declination, respectively, and away from us at 18.50 km/s. Mintaka is moving at 0.65 and -0.69 milliarcseconds/year in right ascension and declination, respectively, and away from us at 18.50 km/s.\n\n\nWhat are the distances to the stars in parsecs?\n\n\nAnswer: According to Simbad, the parallax of these stars is the following\n\n\n\nname\nparallax [mas]\n\n\n\n\nAlnilam\n1.65\n\n\nAlnitak\n4.43\n\n\nMintaka\n4.71\n\n\n\nUsing the relation between parallax \\(p\\) and distance \\(d\\),\n\\[d_{pc} = \\frac{1}{p_{arcsec}}\\]\nwe can find the disances for each star to be the following.\n\n\n\nname\nparallax [mas]\ndistances [pc]\n\n\n\n\nAlnilam\n1.65\n606\n\n\nAlnitak\n4.43\n226\n\n\nMintaka\n4.71\n212\n\n\n\n\n\nFor the two stars with the most similar distances, calculate their angular separation and minimum physical separation (assuming they are both at a distance equal to that of the nearest of the two stars).\n\n\nAnswer: Angular separation between two points on the celestial sphere is defined by \\[(\\Delta \\theta)^2 = (\\Delta\\alpha\\cos\\delta)^2 + (\\Delta\\delta)^2\\]\nThe physical separation can be found by the small angle formula, namely\n\\[\\theta_{rad} = \\frac{D}{d}\\]\nThe following code chuck solves for the angular and physical separation given the coordinates of the two stars and the distance to the closest star.\n\n\nimport numpy as np\nimport astropy.units as u\nalnitak = (085.18969443 * u.deg,-01.94257359 * u.deg)\nmintaka = (083.00166706 * u.deg,-00.29909511 * u.deg)\nmintaka_parallax = 4.71 * u.arcsec * 10**-3\nara, adec = alnitak\nmra, mdec = mintaka\ndra = ara - mra\nddec = adec - mdec\n\ntheta = np.sqrt((dra * np.cos(adec))**2 + ddec**2).to(u.rad)\ndistance = mintaka_parallax.to(u.parsec, equivalencies=u.parallax())\nseparation = theta * distance\n\nprint(f\"\"\"Alnitak and Mintaka are separated by {theta.to(u.deg):.2f}, \nwhich corresponds to a physical separation of {separation / u.rad:.2f}\"\"\")\n\nAlnitak and Mintaka are separated by 2.74 deg, \nwhich corresponds to a physical separation of 10.14 pc"
  },
  {
    "objectID": "courses/astrophysics/Final.html",
    "href": "courses/astrophysics/Final.html",
    "title": "Astrophysics Final Exam",
    "section": "",
    "text": "Problem 1\nThe circular velocity of a star that is located 11.5 kpc from the Galactic Center is 220 km/s.\n\nIf all of the interior mass were concentrated into a black hole, what would be the radius of the event horizon (in units of pc)?\nDescribe two other observations that also suggest and/or confirm the presence of a supermassive black hole in the Galactic Center. This should include a description of the observations and how they are related to the supermassive black hole.\n\n\nAnswer: Since the star is moving in circular motion, we can derive the mass by rearranging the following equation of circular velocity for mass.\n\\[v_c = \\sqrt{\\frac{GM}{r}}\\longrightarrow M = \\frac{v_c^2r}{G}.\\]\nAdditionally, we know that the radius of the event horizon is described by the Schwartzchild Radius\n\\[r_s = \\frac{2GM}{c^2}.\\]\nSubstituting in our value for the mass algibraically gives\n\\[r_s = \\frac{2v_c^2r}{c^2},\\]\nwhich is dimensionally consistent. The following code chunk estimates the event horizon radius to be 0.0124 pc.\n\n\nimport astropy.units as u\nimport astropy.constants as const\nimport numpy as np\n\nr = 11.5 * u.kpc\nv_c = 220 * u.km / u.s\n\nr_s = 2 * v_c ** 2 * r / const.c ** 2\nprint(f\"radius of event horizon: {r_s.to(u.pc):.4f}\")\n\nradius of event horizon: 0.0124 pc\n\n\n\nAdditionally, we believe there to be a supermassive black hole at the center of our galaxy based on orbits of stars near the center and x-ray and gamma ray emissions. If we look at the galactic center in the infrared, we can peer through the gas and dust to see the physical locations of the stars near the galactic center. If we track the positions of the stars across a range of observations, we can model the orbit of the stars about the common center of mass and use Kepler’s 3rd Law to determine the mass given the period and semimajor axis. The mass we calculate is on the order of \\(10^6\\) \\(M_\\odot\\), which is too large for any star. Combining this with x-ray and gamma ray observations that are cospatial on the region of this ultra-high mass concetration give us evidence for a supermassive black hole, where the high energy emissions are the result of matter falling onto the black hole.\n\n\n\nProblem 2\n\nWhat is an NFW profile? Describe how it is used in astrophysics.\nConsider a galaxy with an NFW density profile that has the normalization factors of \\(\\rho_0 = 2\\times10^8\\) Solar masses per cubic kiloparsec and \\(a = 13\\) kiloparsecs. What is the mass density at \\(r=17.18\\) kpc in appropriate units of Solar masses per cubic kiloparsec?\n\n\nAnswer: A NFW, or Navarro-Frenk-White, profile is a typical model for dark matter halos. Assuming the halos are spherical in nature, the NFW profile describes the density of dark matter as a function of radius, and only depends on two other parameters. To ellaborate on the usefulness of this model more, it gives us an estimate how much dark matter we expect at there to be at any point in space, which is extremely useful given how dark matter tends to not interact with normal baryonic matter, making it impossible to see by conventional methods.\nThe equation itself is\n\\[\\rho(r) = \\frac{\\rho_0}{\\left(\\frac{r}{a}\\right)\\left(1+\\frac{r}{a}\\right)^2},\\]\nwhere \\(rho_0\\) and \\(a\\) are normalization factors. The following code chunk calculates the density of dark matter at \\(r=17.18\\) kpc with the given normalization factors to be \\(2.81\\times10^7\\) Solar masses per cubic kpc.\n\n\nrho0 = 2E8 * u.M_sun / u.kpc ** 3\na = 13 * u.kpc\nr = 17.18 * u.kpc\n\nrho = rho0 / ((r / a) * (1 + r / a) ** 2)\nprint(f\"density: {rho.to(u.M_sun / u.kpc ** 3):.2e}\")\n\ndensity: 2.81e+07 solMass / kpc3\n\n\n\n\nProblem 3\nTwo elliptical galaxies, A and B, have measured central velocity dispersions of \\(\\sigma_A = 97\\) km/s and \\(\\sigma_B = 122\\) km/s. Which of these galaxies is more intrinsically luminous, and by how many magnitudes?\n\nAnswer: Elliptical galaxies follow the Faber-Jackson relation where the velocity dispersion is proportional to the luminosity. More specifically,\n\\[\\sigma^4 \\propto L.\\]\nBy this relation, we would expect galaxy B to be more luminous. Using the relationship for the difference in magnitude,\n\\[M_1 - M_2 = -2.5 \\log_{10}\\left(\\frac{L_1}{L_2}\\right),\\]\nwe can derive the difference in magnitude between A and B to be\n\\[M_A - M_B = -2.5 \\log_{10}\\left(\\frac{L_A}{L_B}\\right) = -2.5 \\log_{10}\\left(\\frac{\\sigma_A^4}{\\sigma_B^4}\\right)\\]\nThe following code chunk calculates difference in magnitudes to be 0.996 mag, thus B is around a magnitude brighter than A.\n\n\nsigmaA = 97 * u.km / u.s\nsigmaB = 122 * u.km / u.s\n\ndmag = -2.5 * np.log10(sigmaA ** 4 / sigmaB ** 4)\nprint(f\"A and B differ by {dmag:.3f} mag\")\n\nA and B differ by 0.996 mag\n\n\n\n\nProblem 4\nThe plot shows the observed apparent magnitude of a star in the V-band. This variable star obeys the period-luminosity relation given in the legend.\n\n\nCalculate the absolute magnitude of this star.\nBriefly describe the physical phenomena behind this period-luminosity relationship.\n\n\nAnswer: The absolute magnitude of the star can be calculated using the relation given in the lengend of the plot. The code chunk below calculates just that.\n\n\nP = 3.5 * u.d\nM = -2.97 * np.log10(P / u.d) - 1.73\nprint(f\"M = {M:.2f}\")\n\nM = -3.35\n\n\n\nMore broadly, period-luminosity relations, like those of Cepheid variables or RR Lyrae variables, are useful relationships for distance determinations. The general idea is that a star (high mass for Cepheid, low mass for RR Lyrae) is undergoing a period of pulsation that is directly related to the luminosity of the star. By measuring the period, we can back-out the luminosity and calcualte the distance to the object. For Cepheids, this pulsation period occurs as they are becoming red giants, while RR Lyrae stars are traveling along the horizontal branch.\n\n\n\nProblem 5\nThe plot to the right shows one calibration of the baryonic Tully-Fisher relation (BTFR). Three individual galaxies are labeled. Next consider the four objects shown in the images below; note that the scale bar in each image sets the angular size. Using your knowledge of the properties of galaxies, match the images of three of these galaxies to their positions on the BTFR. Be sure to justify your assignments, including why the fourth object is not included on the plot.\n\n\n\nAnswer: The baryonic Tully-Fisher relation (BTFR) only applies for spiral galaxies because the relationship was derived under the assumption of circular motion. Thus, the ellipitical galaxy, represented as Image #3, does not appear on the plot. The BTFR says that a spiral galaxy’s rotational velocity is related to it’s mass, wherein low mass galaxies spin at slower rates than high mass galaxies. By this metric, we would expect the spiral galaxy shown in Image #4 to align with galaxy A. Additionally, Leo P is one of the lowest dwarf galaxies that still lies along the BTFR, which would put Image #1 aligning with galaxy C. That would leave galaxy B to align with Image #2, which makes sense when comparing the amount of stellar matierial we see in Image #2 to Images #1 and #4; Image #2 has more mass than #1, but less mass than #4.\n\n\n\nProblem 6\nThank you for a wonderful semester! While we didn’t get through all of astrophysics, we did cover a lot of it. As you reflect back on the material that we discussed together, I’d love to hear a brief response (a few sentences each) to the following questions. Please include physical arguments in your responses.\n\nWhich topic interested you the most, and why?\nWhich area of astrophysics seems most worthy of focused effort in the years to come? Why?\n\n\nAnswer: Cosmology and the (brief) introduction we got into the Big Bang were the some of the most intereesting topics we covered, primarily because they are our best answer to those basic questions of “how did we get here” and “where are we going”. I know that’s a pretty cheesy answer, but I’ve always been interested in understanding the Universe on the large scale and the equations that govern how we see things toda, but I feel like I’ve only ever learned surface level information. Learning about the Einstein Field Equations, the Friedmann Equation, and being exposed to tensors felt new and exciting. I wish we had a class that covered the material. Learning about the Big Bang and how fast the Universe evolved was really fun to talk about, but I wish we could have spent more time on it.\nI can’t choose just one area of astrophysics to devote more focused research in because they all feel so interconnected. Studying dwarf galaxies inveitably results in you applying some constraints to dark matter. Studying cosmology requires you to know how stars form to understand what proportions of elements we expect there to be throughout cosmic history. Personally, I want to devote my work to cosmology and large-scale structure, but I imagine I will be dabbling in other fields along the way."
  },
  {
    "objectID": "courses/astrophysics/Exam1.html",
    "href": "courses/astrophysics/Exam1.html",
    "title": "Astrophysics Exam 1 Revision",
    "section": "",
    "text": "Problem 1\nThe spring constellation, Leo, hosts many galaxies in its vicinity on the sky, including several Messier objects. Find the angular separation of the two spiral galaxies M95 and M96 located just under the bent coat hanger, I mean, lion. The J2000 coordinates for the two objects are:\nM95: $(\\alpha, \\delta) = $ 10:43:57.7, +11:42:13\nM96: $(\\alpha, \\delta) = $ 10:46:45.7, +11:49:12\n\nOriginal Answer (7/10): \\(\\Delta\\theta \\approx 1.8631\\) deg\nRevised Answer (10/10): I forgot that the declination angle was given in “degrees:arcminutes:arcseconds”. Instead, I incorrectly thought it was given in “degrees:minutes:seconds” meaning I would need to multiply the last two terms by a factor of 15 to convert them to degrees from hours. This results in my declinations terms being off by approximately 10 degrees for both measurements. If I had the correct numbers for the declinations in degrees, I would have calculated the correct answer to within half an arcsecond (the rest of my math was correct).\nI will say that forgetting declination is in degrees:ARCminutes:ARCseconds is a pretty big mistake, but it was also my only mistake on a problem with a lot of algebra, so I would only take off 3 points from my original answer.\nThe code below correctly calculates the angular separation between the two galaxies using Astropy’s SkyCoords class and separation method.\n\n\nimport astropy.units as u\nfrom astropy.coordinates import SkyCoord\nimport numpy as np\nM95 = SkyCoord(\"10:43:57.7 +11:42:13\", unit=(u.hourangle, u.deg))\nM96 = SkyCoord(\"10:46:45.7 +11:49:12\", unit=(u.hourangle, u.deg))\n\nsep = M96.separation(M95)\nsep.to(u.deg)\n\n\\(0^\\circ41{}^\\prime42.41536068{}^{\\prime\\prime}\\)\n\n\n\n\nProblem 2\nA hypotheical dwarf planet follows and eccentric orbit about the Sun \\((\\epsilon = 0.32)\\), and is estimated to have an orbital period of 650 years.\n\nIf this object were on a perfectly circular orbit, at what distance would it be located from the Sun?\nWhat is the closest that this object gets to the Sun?\nWhat is the furthest that this object gets from the Sun?\n\n\nOriginal Answer (10/10):\n\n\\(a \\approx 75.037\\) AU\nperihelion \\(\\approx 51.025\\) AU\naphelion \\(\\approx 99\\) AU\n\nRevised Answer (10/10): Since the dwarf planet is orbiting the Sun, we can use Kepler’s Third Law, which I did for my original answer. Additionally, the perihelion and aphelion are the locations in an orbit where an object is closest and furthest from the principle focus (in this case the Sun), respectively, and are given by the following equations\n\\[r_{p} = a - ae, \\quad r_{a} = a + ae,\\]\nwhere \\(a\\) is the semi-major axis. The code below verifies my original calculations.\n\n\ne = 0.32\nP = 650 * u.yr\n\n# a) kepler's 3rd law\na = (P ** (2/3)).value * u.au\nprint(f\"a = {a:.2f}\")\n\n# b) perihelion\nr_p = a - a * e\nprint(f\"perihelion: {r_p:.2f}\")\n\n# c) aphelion\nr_a = a + a * e\nprint(f\"aphelion: {r_a:.2f}\")\n\na = 75.04 AU\nperihelion: 51.03 AU\naphelion: 99.05 AU\n\n\n\n\nProblem 3\nYou are attempting to resolve two galaxies using observations of a spectral line with rest wavelength 31.065 cm. These galaxies, both located at a distance of 14.7 Mpc, are physically separated by 61.9 kpc. Assuming a circular aperture, what diameter must your telescope have (in units of meters) in order to resolve these sources?\n\nOriginal Answer (6/10): \\(D = 2.79\\cdot 10^{6}\\) m\nRevised Answer (8/10): Originally, I solved the angular resolution formula for diameter, then used parallax to get half the angular separation, doubled it to get the angle, plugged in the result to get the diameter.\nI now think the problem is simply geometry. Specifically, the angle of the separation of these two galaxies at their given distance is the minimum resolving angle. The code chunk below performs this calculation and results in an answer that matches intuition; our answer is slightly larger than that of the diameter of green bank, which is a radio telescope that typically observes the 21 cm line (31 cm is slightly larger, so maybe we’d expect a slightly larger telescope).\nMy original answer the right equation, but wrong understanding. However, I did check my work and conclude it was unphysical in the exam, so I would give myself 6/10.\nI give myself 8/10 now, because I am not quite sure why parallax doesn’t work in this case, especially since the foundation for parallax is a geometric scenario very similar to what I used below.\n\n\nl_rest = 31.065 * u.cm\ndist = 14.7 * u.Mpc\nsep = 61.9 * u.kpc\n\ntheta = (sep / (2 * dist)).to(u.m/u.m)\ndiameter = 1.22 * l_rest / theta\nprint(f\"diameter: {diameter.to(u.m):.2f}\")\n\ndiameter: 180.01 m\n\n\n\n\nProblem 4\nIn a gas of neutral Hydrogen atoms, at what temperature (in units of Kelvin) is the number of atoms in the second excited state equal to 2.38% of the number of atoms in the ground state?\n\nOriginal Answer (9/10): 29485 K\nRevised Answer (10/10): In my original answer, I rearranged the correct formula to solve for temperature\n\\[\\frac{N_b}{N_a} = \\frac{g_b}{g_a}e^{-\\frac{\\Delta E}{kT}}\\longrightarrow T = \\frac{-\\Delta E}{k\\ln\\frac{N_b}{N_a}\\frac{g_a}{g_b}}\\]\nbut my end result differs from my revised answer. I likely entered the numbers into the calculator wrong during the exam, as my revised answer is the same order of magnitude as my original answer.\n\n\nimport astropy.constants as c\ng_a = 2 * 1 ** 2  # ground state degeneracy\ng_b = 2 * 3 ** 2  # second excited state degeneracy\n\nratio = 0.0238  # N_b / N_a\n\nE_a = -13.6 * u.eV\nE_b = E_a / 3 ** 2\ndE = E_b - E_a\n\nT = - dE / (c.k_B * np.log(ratio * g_a / g_b))\n\nprint(f\"Temperature: {T.to(u.Kelvin):.2f}\")\n\nTemperature: 23635.85 K\n\n\n\n\nProblem 5\nA main sequence star has a measured parallax angle of 0.0129 arcseconds. This star has an apparent bolometric magnitude of 3.749.\n\nWhat is the distance to the star?\nWhat is the distance modulus of this star?\nWhat is the absolute magnitude of this star?\nWhat is the ratio of the luminosity of this star to that of the Sun?\nIf the distance to this star were to increase by a factor of three, what would be its apparent magnitude?\n\n\nOriginal Answer (10/10):\n\n\\(d = 77.5194\\) pc\n\\(m-M = 4.4471\\)\n\\(M = -0.6981\\)\n149.7063 times more luminous than the Sun\n\\(m_{3d} = 6.1346\\)\n\nRevised Answer (10/10): All of my revised answers match my original answers.\n\n\np = 0.0129 * u.arcsec\nm = 3.749\nM_Sun = 4.74\n\n# a)\ndistance = p.to(u.pc, equivalencies=u.parallax())\nprint(f\"distance = {distance:.3f}\")\n\n# b)\ndist_mod = 5 * np.log10(distance.value) - 5\nprint(f\"m-M = {dist_mod:.3f}\")\n\n# c)\nM = m - dist_mod\nprint(f\"M = {M:.3f}\")\n\n# d)\nL_Lsun = 10 ** ((M - M_Sun) / -2.5)\nprint(f\"{L_Lsun:.2f} times more luminous than the Sun\")\n\n# e)\nm_3d = 5 * np.log10(3 * distance.value) - 5 + M\nprint(f\"m at 3d = {m_3d:.2f}\")\n\ndistance = 77.519 pc\nm-M = 4.447\nM = -0.698\n149.70 times more luminous than the Sun\nm at 3d = 6.13\n\n\n\n\nProblem 6\nWhat is your favorite book?\n\nOriginal Answer: The Expanse series of Three Body Problem.\nRevised Answer: The Dark Forest from the Three Body Trilogy\n\n\n\nProblem 7\nThe plot below shows an observing plan at the Macalester Observatory. THe elevations of the three sources (the Sun in red, the Moon in dashed gray, and the target of interest in blue) are plotted as functions of time, with midnight central time being at 0 hours. The graph is color-coded to show the sky darkness. Using this plot, and recalling that the Macalester Observatory is located at a latitude of +45 degrees, find\n\nthe declination of the “Target” object in units of degrees, and\nestimate when during the year this observing plan may have been scheduled.\n\n\nOriginal Answer (3/10):\n\nAround +10 degrees\nAround March 19th\n\nRevised Answer (7/10): I confused the target’s transit with the Moon’s transit.\nIf we are observing from Macalester, our zenith is +45 degrees, so the horizon corresponds to 45 - 90 = -45 degrees declination. If our target reaches a 40 degree altitude, then the target is 40 degrees above the horizon. Thus, the declination of the target is -45 + 40 = -5 degrees declination.\nThe Sun is also at an altitude of 40 degrees, which corresponds to a declination of -5 degrees, so we are either approaching March 20 or September 23 recently passed. I cannot think of any way to further determine the time of year from this plot, but I imagine there is a way to determine if it were vernal or autumnal, so I give myself only 7/10 for this problem.\n\n\n\nProblem 8\nAn eclipsing, double-line spectroscopic binary system consisting of two main sequence stars has an orbital period of 7.17 years. By analyzing the spectra, you determine wavelength shifts of 0.011 nm and 0.064 Angstroms for the two stars.\n\nWhat is the mass ratio of the larger to the smaller star?\nThe spectral energy distribution of the cooler star suggests a masss of 1.13 Solar masses. What is the mass of the hotter star (in Solar masses)?\nDescribe the observations and measurements needed to determine the relative temperatures and radii of these stars?\n\n\nOriginal Answer (10/10):\n\n\\(\\frac{m_1}{m_2} = 1.7188\\)\n\\(m_1 = 1.9422 M_{\\odot}\\)\nTemps? –&gt; Need lightcurve. Radii? –&gt; Need lightcurve. [Included labeled diagram and equations.]\n\nRevised Answer (10/10): We expect the ratio of the heavier to lighter star to be proportional to the lighter star’s shift to the heavier star’s shift.\n\\[\\frac{m_1}{m_2} = \\frac{\\Delta\\lambda_2}{\\Delta\\lambda_1}\\]\nSince they are main sequence stars, we expect the more massive star to be hotter. The code chuck below calculates the answers for parts a) and b)\n\n\ndl1 = 0.064 * u.angstrom  # smaller shift, more massive\ndl2 = 0.011 * u.nm  # larger shift, less massive\n\n# a)\nmass_ratio = dl2 / dl1\nprint(f\"The larger star is {mass_ratio.to(u.m/u.m):.2f} more massive\")\n\n# b)\nm_2 = 1.13 * u.M_sun\nm1 = mass_ratio * m_2\nprint(f\"Mass of larger star: {m1.to(u.M_sun):.2f}\")\n\nThe larger star is 1.72 more massive\nMass of larger star: 1.94 solMass\n\n\n\nTo elaborate on part c), since this is an eclipsing binary, we should be able to obtain a lightcurve for the system, wherein we could measure the baseline brightness \\(B_0\\), the brightness of the primary dip \\(B_p\\), and the brightness of the secondary dip \\(B_s\\) to extract the ratio of the temperature of the smaller star \\(T_s\\) to that of the larger star \\(T_l\\) though the following relationship.\n\\[\\frac{B_0 - B_p}{B_0 - B_s} = \\left(\\frac{T_s}{T_l}\\right)^4\\]\n\n\nFrom the transits, we can measure the times when the smaller star begins/ends transitting infront/behind the larger star. We can find the radii of the two stars from the following equations,\n\\[r_s = \\frac{V}{2}(t_b - t_a), \\quad r_l = \\frac{V}{2}(t_c - t_a) = r_s + \\frac{V}{2}(t_c - t_b)\\]\nwhere \\(V = V_s + V_1\\) (which can be determined from the mass ratio) and \\(t_a\\), \\(t_b\\), and \\(t_c\\) are the start time of transit of primary dip, start of primary dip constancy, and end primary dip constancy, respectively.\n\n\n\nProblem 9\nThe plot shows the observed radial velocities of two stars in a gravitationally bound orbit abuot their common center of mass. Four full orbital periods are shown.\n\nWhat can be said about the eccentricity of the orbits? If it can be determined, give the eccentricity. If not, explain why not, and provide possible constraints.\nWhat can be said about the inclination of the system? If it can be determined, give the inclination. If not, explain why not, and provide possible constraints.\nIf the less massive star has a mass of 2.17 Solar masses, then what is the mass of the other star (in Solar masses)?\n\n\nOriginal Answer (10/10):\n\nBoth observed \\(v_r\\)’s are very periodic and sinusoidal which implies circular orbits (\\(e\\approx0\\)).\nInclination affects amplitude of the plot. Need separation and distance to determine \\(i\\) but could assume \\(\\sin^3i \\approx 2/3\\)\nlarger star mass \\(\\approx 5.58\\) solar masses\n\nRevised Answer (10/10): None of my answers changed in my revision, so I would give myself a 10/10 for both attempts.\nAs I mentioned in my original answer, the orbits are very sinusoidal with no skewness. This implies an orbit with low eccentricity.\nI agree with my answer for part b), in that inclination affects the amplitude of the plot, but we would need more information to determine the inclination. Most binary systems we detect have \\(\\sin^3i \\approx 2/3\\), so we could assume that applies to this system, but would need to know masses, distances, and either the semimajor axes lengths or angles subtended by the semimajor axes.\nThe code chunk below calculates the mass ratio from the velocity ratio of the two stars governed by the following equation.\n\\[\\frac{m_A}{m_B} = \\frac{v_B}{v_A}\\]\nMultiplying the mass ratio by the mass of the lower star, we can determine the mass of the larger star.\n\n\nv_A = 7\nv_B = 18\nm_s = 2.17 * u.M_sun\nratio = v_B / v_A\nm_l = m_s * ratio\nprint(f\"larger star mass: {m_l:.2f}\")\n\nlarger star mass: 5.58 solMass\n\n\n\n\nProblem 10\nThe plot shows the number ratio of atoms in the second stage of ionization versus the total numbe rof atoms for various hypothetical atomic species, “A”, “B”, “C”, and “D”. The four species have identical partition functions for all stages of ionization and they all have exactly the same ionization energy for the first stage fo ionization. The only parameter that differs is the ionization energy for the second stage of ionization.\n\nWhat is the name of the equation used to build these curves.\nWhich atomic species has the smallest ionization energy? Explain.\nWhich atomic species has the greatest ionization energy? Explain.\n\n\nOriginal Answer (6/10):\n\nSaha equation\n\\(A\\) has smallest ionization energy because it peaks at a lower \\(\\lambda\\). In other words, the fraction of ionized particles to total reaches 1 at lower temperatures so it takes less energy to ionize \\(A\\).\nFor a similar argument, \\(D\\) must have the highest ionization energy since \\(N_a/N_{total}\\) peaks at higher temperatures.\n\nRevised Answer (10/10): The only thing I would change is my typo; \\(\\lambda\\) should be temperature. I don’t know why I said \\(\\lambda\\) as it’s not even on the plot, but if we make the replacement with temperature, the answer for part b) reads correctly: “\\(A\\) has the smallest ionization energy because it peaks at a lower temperature.” Since my typo reads as a lack of conceptual understanding and I incorporate my argument from b) into c), I would give my original answer a 6/10.\n\n\n\nProblem 11\nThe plot shows the Maxwell-Boltzmann distributions of two parcels of gas, “A” and “B”. Each parcel of gas has an identical composition of pure Hydrogen atoms.\n\nWhat is the temperature of the gas in parcel “A”?\nWhat is the average kinetic energy of a gas particle in parcel “B”?\n\n\nOriginal Answer (4/10):\n\nWe don’t know the actual temp, but we do know \\(A\\) is cooler than \\(B\\) bc distribution is averages at lower velocities than \\(B\\).\nSimilarly, we don’t know the exact value, but we know the average is higher than \\(A\\) and is related by \\(v = \\sqrt{\\frac{3kT}{m}}\\longleftrightarrow KE = \\frac{3}{2}kT\\)\n\nRevised Answer (10/10): I think my original answers were based on the understanding that temperature can be found from velocity, but we were given a velocity distribution, so picking one velocity as the “gas velocity” would result in a different temperature depending on what velocity you pick.\nWhat I failed to catch was we could use the most probable velocity to determine the temperature, then use that temperature to calculate the average kinetic energy of the gas.\n\\[v_{mp} = \\sqrt{\\frac{2kT}{m}} \\longrightarrow T = \\frac{v_{mp}^2 m}{2k}, \\quad KE_{avg} = \\frac{3}{2}kT\\]\nThe following code chunk calculates the temperature of gas \\(A\\) and the average kinetic energy of a particle in gas \\(B\\).\n\n\nv_mpA = 10.5 * u.km / u.s\nv_mpB = 13 * u.km / u.s\nm = c.m_p  # proton/Hydrogen mass\n\n# a)\nT_A = v_mpA ** 2 * m / (2 * c.k_B)\nprint(f\"Temperature of Gas A: {T_A.to(u.K):.2f}\")\n\n# b)\nT_B = v_mpB ** 2 * m / (2 * c.k_B)\nKE_avg_B = 3/2 * c.k_B * T_B\nprint(f\"Average Kinetic Energy of Particle in B: {KE_avg_B.to(u.eV):.2f}\")\n\nTemperature of Gas A: 6678.26 K\nAverage Kinetic Energy of Particle in B: 1.32 eV\n\n\n\n\nReflection\n\nWhat did you feel the most and least confident about going into the exam?\n\n\nAnswer: I felt most confident in my ability to do algebraic manipulation on equations to derive the answer, but least confident in my ability to not make calculator mistakes under the time crunch. I had a comprehensive equation sheet and I felt confident in my ability to interpret them, but I was worried I wouldn’t do as well on conceptual questions.\n\n\nWhat did “grading”/“correcting” your exam help you to understand better?\n\n\nAnswer: I will always remember that declination is given in degrees:arcminutes:arcseconds, not degrees:minutes:seconds. I also solidified my belief that python should be trusted for number crunching than a handheld calculator, at least in my hands. The main thing grading the quiz helped me with was understanding my errors because it forced me to redo the problems.\n\n\nWhat topics from the first unit do you feel like you still have the most room for improvement?\n\n\nAnswer: As I go into more detail in my revisions for the specific problems, I still have some confusion on why parallax didn’t work for the Problem 3 and want to understand how I can interpret the plot in Problem 7 fully. Regarding topics in general, I feel fairly confident in my understanding for most of the material except for orbital mechanics, spectral features, and observing plans. For orbital mechanics, I think I’m mainly confused on what parameters we can determine from each given binary type. The class on spectral features was one that I missed for a swim meet, so I’m worried I missed something. Even though I took observational astronomy, we never had to determine if an observation was feasible that quickly/without help; I would like to get a stronger intuition regarding the time an observation could be observed. Basically, Problem 7 made me realize I don’t understand equinoxes and LST well.\n\n\nIf 10 pts per problem (except problem 6), what would your original grade be versus the version you’re turning in now?\n\n\nAnswer: Excluding problem 6, I would have received a 75/100. With revisions, I believe I have corrected my exam up to a 95/100."
  },
  {
    "objectID": "courses/astrophysics/Exam2.html",
    "href": "courses/astrophysics/Exam2.html",
    "title": "Astrophysics Exam 2 Revision",
    "section": "",
    "text": "Problem 1\n\nWhat is convection?\nIn our current models of stellar structure, how is it decided whether a particular zone within a stellar interior is convective or not? Be as descriptive and quantitative as possible.\nWhere is convection important in the Sun, and why?\nWhere is convection important in higher mass stars, and why?\n\n\nOriginal Answer (7/10):\n\nConvection is mass flow of particles that travel in a group at approximately the same speed.\nTemperature gradient. If the actual temperature gradient is greater than the adiabatic temperature gradient, then convection will occur.\nConvection happens in the outer most region from \\(R=0.714 R_\\odot\\) to \\(1R_\\odot\\). This region is imporant because convection causes B-field lines and sunspots and the differential rotation causes these B-field lines to flip every 11 years. We primary care about convection because it influenced B-field lines which can cause solar activity that directly affect Earth.\nConvection is important everywhere in higher mass stars becase they are so hot that they need convection to transport energy. We want to understand convection in stars because they produce heavy elements and the ISM.\n\nRevised Answer (10/10): Other than part c), I would reword my answers slightly to the following:\n\nConvection describes the energy transport process when energy is moved in large flows or currents.\nOur current models for stellar evolution predict a zone will be convective if the temperature gradient in that region is greater than the adiabatic temperature gradient.\nSame as original answer.\nHigher mass stars undergo the CNO cycles which, due to the cycle’s large temperature dependence, encourages energy to leave the core via convection. This convection allows higher mass stars to be more efficient mixers of elements, prolonging their lives on the main sequence.\n\nGiven that most of my final answers are rewordings of my original, I would give myself a 7/10 for my original work on this problem.\n\n\n\nProblem 2\nConsider the dense core of a molecular cloud. This inner core has an average mass density of \\(1.6\\times10^{-17}\\) kg m\\(^{-3}\\), a total mass of 11.2 Solar masses, and a mean molecular weight of \\(\\mu = 2.1\\). Assume the temperature is 10 K.\n\nWhat is the free-fall timescale in years?\nWhat is the Jeans’ length in pc?\nWhat is the Jeans’ Mass in solar masses?\nWhat is the radius of the cloud?\nBased on these calculations, what do you expect to hapen to this cloud? E.g., is it stable or unstable, and what does that mean?\n\n\nOriginal Answer (7/10):\n\n\\(t_{ff} = \\left(\\frac{4\\pi}{32}\\frac{1}{G\\rho_0}\\right)^{1/2} = \\left[(0.2945)(9.366\\cdot10^{26})\\right]^{1/2} = 1.66\\cdot10^{13}\\) s \\(\\longrightarrow\\) to yr\n\\(R_J = \\left(\\frac{15kT}{4\\pi G\\mu m_H \\rho_0}\\right)^{1/2} = \\left(\\frac{2.07\\cdot10^{-21}}{4.705\\cdot10^{-53}}\\right) = 6.6329\\cdot10^{15}\\) m \\(\\longrightarrow\\) to pc\n$M_J = (){3/2}(){1/2} = $\n\\(R = \\left(\\frac{3M}{4\\pi\\rho_0}\\right)^{1/3}\\)\ncollapse if \\(M &gt; M_J\\)\n\nRevised Answer (10/10): For most of the calculations on this exam, I was able to set up the problem and plug in the numbers, but I was not able to solve for the final value. Specifically for parts a) and b) of this question, however, I was able to derive an equivalent final value, just not converted to the final units asked for in the problem. The code chunks below complete the calculations I was not able to accomplish during the exam. Given that the cloud mass is larger than the Jeans’ Mass and the clouds length is larger than Jeans’ length, we expect this cloud to collapse.\n\n\nimport astropy.units as u\nimport astropy.constants as c\nimport numpy as np\n\n# define given values\nrho0 = 1.6E-17 * u.kg / u.m ** 3\nM = 11.2 * u.M_sun\nmu = 2.1\nT = 10 * u.K\nm_H = c.m_p\n\n# a)\nt_ff = (3 * np.pi / (32 * c.G * rho0)) ** 0.5\nprint(f\"free-fall timescale: {t_ff.to(u.yr):.2e}\")\n\n# b)\nR_J = (15 *c.k_B * T / (4 * np.pi * c.G * mu * m_H * rho0)) ** 0.5\nprint(f\"Jeans' length: {R_J.to(u.pc):.2f}\")\n\n# c)\nM_J = (5 * c.k_B * T / (c.G * mu * m_H)) ** (3/2) * (3 / (4 * np.pi * rho0)) ** 0.5\nprint(f\"Jeans' mass: {M_J.to(u.M_sun):.2f}\")\n\n# d)\nR = (3 * M / (4 * np.pi * rho0)) ** (1/3)\nprint(f\"Radius of cloud: {R.to(u.pc):.2f}\")\n\nfree-fall timescale: 5.26e+05 yr\nJeans' length: 0.21 pc\nJeans' mass: 9.82 solMass\nRadius of cloud: 0.22 pc\n\n\n\nI knew what equations to use for my original answers and I was able to plug in the right values to derive the correct result. The only things I wasn’t able to accomplish on this problem was calculating parts c), d), and e), and converting units. If I had more time/was more proficient at typing values into the handheld calculator, I would have been able to complete this problem as I understood the process behind each question, thus, I would give myself a 7/10 for my original answer.\n\n\n\nProblem 3\nThe plot below shows the Jeans stability criterion, with mass plotted as a function of density. Two individula clouds, labeled “Cloud A” and “Cloud B”, are shown. You man assume that each cloud has the same temperature and the same composition.\n\nIdentify if one, both, or neither of clouds would be considered stable against gravitational collapse. Explain your answer.\nWhat is the radius of cloud A?\nWhat is the radius of cloud B?\n\n\nOriginal Answer (8/10):\n\nA cloud collapses if \\(M_c &gt; M_J\\), thus we expect the green cloud (Cloud A) to collapse while the blue dot (Cloud B) would not.\n\\(R_A = \\left(\\frac{3M}{4\\pi\\rho}\\right)^{1/3} = \\left(\\frac{3\\cdot13\\cdot1.99\\cdot10^{30}}{4\\pi\\cdot10^{-17}}\\right)^{1/3} = 4.98\\cdot10^{15}\\) m\n\\(R_B = \\left(\\frac{3M}{4\\pi\\rho}\\right)^{1/3} = \\left(\\frac{3\\cdot9\\cdot1.99\\cdot10^{30}}{4\\pi\\cdot5\\cdot10^{-17}}\\right)^{1/3} = 4.406\\cdot10^{15}\\) m\n\nRevised Answer (10/10): My original answer for a) is correct. If the purple line shows the Jeans stability criterion with mass on the y axis, then any points that fall below the line would have masses less than the Jeans mass, and therefore be stable from collapse. If any points were above the line, then the cloud would have a mass greater than the Jeans mass and collapse as a result. In other words, Cloud A would collapse and Cloud B would not.\nAs the code chunk below shows, I had the right order of magnitude but slightly different answers for part b) and c). This discrepancy is due to a calculation error in my original work: I switched the masses of the clouds and incorrectly estimated the densities from the graph.\n\n\n# define values given in problem\nM_A = 9 * u.M_sun\nrhoA = 6E-17 * u.kg / u.m ** 3\nM_B = 13 * u.M_sun\nrhoB = 2E-17 * u.kg / u.m ** 3\n\n# b)\nR_A = (3 * M_A / (4 * np.pi * rhoA)) ** (1/3)\nprint(f\"Radius of Cloud A: {R_A.to(u.m):.2e}\")\n\n# c)\nR_B = (3 * M_B / (4 * np.pi * rhoB)) ** (1/3)\nprint(f\"Radius of Cloud B: {R_B.to(u.m):.2e}\")\n\nRadius of Cloud A: 4.14e+15 m\nRadius of Cloud B: 6.76e+15 m\n\n\n\nSince my errors for parts b) and c) were only computation errors, I would give myself an 8/10 for my original work.\n\n\n\nProblem 4\nA Source has a luminosity of \\(6\\times10^{13}\\) watts. You observe this source from a distance of 6.25 km. The intervening material has a density of 0.012 kg per cubic meter and an opacity of 0.02 m\\(^2\\) kg\\(^{-1}\\).\n\nWhat is the optical depth of the medium?\nIn the absence of extinction, what is the measured flux in W/m\\(^2\\)?\nIn the presence of extinction, what is the measured flux in W/m\\(^2\\)?\n\n\nOriginal Answer (7/10):\n\n$_= _0^sds = s = (0.02)(0.012)(6.25^3) = $\n$F = = = $\n\\(m_\\lambda - m_{\\lambda,0} = 1.086 \\tau_\\lambda = -2.5\\log_{10}\\left(\\frac{F}{F_{\\lambda,0}}\\right)\\) solve for F?\n\nRevised Answer (10/10): I set up the problem correctly for parts a) and b) in my original answer, and I was going down the right path in part c), however, I did not calculate a final answer. The code chunk below calculates the final answers. Note that for part c), we had to rearrange\n\\[1.086 \\tau_\\lambda = -2.5\\log_{10}\\left(\\frac{F}{F_{\\lambda,0}}\\right)\\]\nto solve for \\(F\\). Another valid way to estimate \\(F\\) is under the relation\n\\[I = I_0 e^{-\\tau}\\]\nand substituting in \\(F\\) and \\(F_0\\) for \\(I\\) and \\(I_0\\), respectively.\n\n\n# define variables\nL = 6E13 * u.W\nd = 6.25 * u.km\nrho = 0.012 * u.kg / u.m ** 3\nkappa = 0.02 * u.m ** 2 / u.kg\n\n# a)\ntau_l = kappa * rho * d\nprint(f\"optical depth: {tau_l.to(u.dimensionless_unscaled)}\")\n\n# b)\nF_0 = L / (4 * np.pi * d ** 2)\nprint(f\"flux (no extinction): {F_0.to(u.W / u.m ** 2):.2e}\")\n\n# c)\nF = F_0 * 10 ** (1.086 * tau_l / -2.5)\nF_alt = F_0 * np.exp(-tau_l)\nprint(f\"flux (with extinction): {F.to(u.W / u.m ** 2):.2e}\")\nprint(f\"flux (with extinction, alt. method): {F_alt:.2e}\")\n\noptical depth: 1.5\nflux (no extinction): 1.22e+05 W / m2\nflux (with extinction): 2.73e+04 W / m2\nflux (with extinction, alt. method): 2.73e+10 W / km2\n\n\n\nBecause I had the correct process for the majority of the problems, and I just didn’t have enough time to enter the final numbers into the calculator, I would give myself a 7/10 for my original answer.\n\n\n\nProblem 5\nThe plot below shows extinction in magnitudes as a function of wavelength along the sightlines to three objects, labeled “A”, “B”, and “C”. The individual points represent individual measurements in specific filters; this is real data and there is uncertainty in the measurements. Assume distances to each object as follows: \\(D_A = 611\\) kpc; \\(D_B = 6.2\\) Mpc; \\(D_C = 2.35\\) Mpc.\n\nWhich sightline has the largest extinction?\nWhich sightline experiences the smallest extinction?\nA star with absolute magnitude -2.47 is observed at 800 nm in each of the objects. What are the apparent magnitudes of each star, “A”, “B”, “C”?\n\n\nOriginal Answer (8/10):\n\nC has the highest extinction across all \\(\\lambda\\)\nA has the smallest extinction across all \\(\\lambda\\)\nWould need to calculate \\(m_\\lambda\\) and \\(m_\\lambda - m_{\\lambda,0} = A_\\lambda = 1.068 \\tau_\\lambda\\), so would either need to know unextinct values (\\(m_{\\lambda,0}\\)) or \\(\\tau_\\lambda\\implies \\kappa, \\rho\\)\n\nRevised Answer (10/10): The extiction of an object is \\(A_\\lambda\\), thus, my answers for a) and b) still hold; C has the highest extiction (\\(A_\\lambda\\)) across all wavelengths and A has the lowest extinction across all wavelengths. The first sentence of part c) was commented out when the test was passed out, so the code chunk below calculates the apparent magnitudes for the three stars. Note that the \\(A_\\lambda\\) values were estimated from the plot at \\(\\lambda = 800 \\text{ nm } = 0.8 \\text{ micron}\\).\n\n\n# define variables\nD_A = 611 * u.kpc\nD_B = 6.2 * u.Mpc\nD_C = 2.35 * u.Mpc\nA_A = 0.6\nA_B = 1.6\nA_C = 2.4\nM = -2.47\n\n# c)\nm_A = M + 5 * np.log10(D_A.to(u.pc).value) - 5 + A_A\nm_B = M + 5 * np.log10(D_B.to(u.pc).value) - 5 + A_B\nm_C = M + 5 * np.log10(D_C.to(u.pc).value) - 5 + A_C\nprint(f\"m_A: {m_A:.2f}\")\nprint(f\"m_B: {m_B:.2f}\")\nprint(f\"m_C: {m_C:.2f}\")\n\nm_A: 22.06\nm_B: 28.09\nm_C: 26.79\n\n\n\nGiven that we were missing some of the question in the exam, I would give myself a 8/10 for the original version of this problem as I knew we were missing something, but I didn’t mention we were missing the absolute magnitude directly.\n\n\n\nProblem 6\nWhat is something in 2025 that you are proud of?\n\nOriginal/Revised Answer: I published a Research Note to the AAS.\n\n\n\nProblem 7\nConsider the following plot, which shows the hypothetical Macalester initial mass function (IMF). This differs in significant ways from the Salpeter IMF in that the Macalester IMF has different slopes in different mass ranges; the different colors label the different slopes, and the vertical dashed lines show the positions of the changes. In contrast, the Salpeter IMF has a constant \\(\\gamma = -2.35\\) over all masses. Compare these IMFs under the assumption that the star-forming environments are identical.\n\nWhich IMF predicts an higher efficiency of formation of the most very massive stars?\nWhich IMF results in the production of more low-mass stars?\nDescribe the pros and cons of each model with respect to their potential of accurately predicting the distribution of stars formed when a modelular cloud collapses.\n\n\nOriginal Answer (9/10):\n\nGiven the Salpeter intersects at red-blue region, the Mac IMF produces a higher efficiency of more massive stars\nFor the same rational as above, the Salpeter IMF predicts more low mass stars\nBoth are potentially valid IMFs, but the Macalester one might be more applicable due to its piecewise nature.\n\nRevised Answer (10/10): I would maintain the same answers for parts a) and b), but I would revise my answer for part c) to the following: Both models could potentially describe the number of stars found across the range of stellar masses, but, as parts a) and b) have shown, it is possible that either model over/underestimates the production of low/high-mass stars. Additionally, the piecewise nature of the Macalester IMF might be more applicable, especially given the new papers (shown in class) that have captured the turnover of the IMF.\nI would give my original answer a 9/10, as I feel I should have elaborated more on part c).\n\n\n\nProblem 8\nConsider an individual radial zone of a 1 Solar mass stellar model (i.e., a model of the Sun). This zone covers a narrow range of radii and thus enables a plane-parallel approximation of physical parameters. The mass contained in this zone is 0.9% of the total mass and the position of the zone is at 61% of the toal radius. Adopt a mean molecular weight that is appropriate for ionized gas, \\(\\mu = 0.754\\). Assume that convection is the primary method of energy transport, and that the gas can be adequately described by the ideal gas law. What is the temperature gradient across this region, in units of Kelvin per meter?\n\nOriginal Answer (7/10):\n\\[\\frac{dT}{dr} = -\\left(1 - \\frac{1}{\\gamma}\\right)\\frac{\\mu m_H}{k}\\frac{GM_r}{r^2} = -(1-\\frac{1}{5/3})\\frac{(0.754)(1.67\\cdot10^{-27}\\text{ kg})}{(1.38\\cdot10^{-23}\\text{ J/K})}(6.67\\cdot10^{-11}\\frac{\\text{Nm}^2}{\\text{kg}^2})\\cdot\\left(\\frac{0.9\\cdot1.99\\cdot10^{30}\\text{ kg}}{(0.61\\cdot6.95\\cdot10^{8}\\text{ m})^2}\\right)\\]\nRevised Answer (10/10): I wrote down the equation and substituted in the given values in my original answer, but I didn’t have enough time to simplify to a final answer, like I do in the code chunk below. For this reason, I would give myself a 7/10 for my original answer.\n\n\n# define variables\nM_r = 0.9 * u.M_sun\nR = 0.61 * u.R_sun\nmu = 0.754\ngamma = 5/3\n\n# calculate convection temperature gradient\ntempgrad = -(1 - 1/gamma) * (mu * m_H * c.G * M_r) / (c.k_B * R ** 2)\nprint(f\"temperature gradient: {tempgrad.to(u.K / u.m):.4f}\")\n\ntemperature gradient: -0.0242 K / m\n\n\n\n\nProblem 9\n\nDescribe the different modes of fusion that occur in stars on the main sequence.\nWhich mode of fusion is more important in higher mass stars than in lower mass stars, and why?\nThe Sun is believed to be more luminous now that when it first arrrived on the main sequence. Explain why.\n\n\nOriginal Answer (8/10):\n\nIn MS stars, hydrogen is fusing to helium in the core, but the process by which it happens could differ depending on the mass of the star. For moderate to low mass stars, like the Sun, the core is fusing through radiation, while in high and low mass stars, the core is fusing through convections.\nHigher mass stars have fusion through convection in their core because they are so much more hotter, so the most efficient energy transport mechanism is through convection. Convection allows for the CNO cycle to take place as well, which can help produce helium more efficiently than through the radiative p-p chain.\nThe Sun is more luminous than when it first arrived on the MS because its composition has changed. He is now the dominant element in the core, which means the core’s mean molecular weight has increased, causing contraction and subsequent temperature increase.\n\nRevised Answer (10/10): I would revise my answers to the following.\n\nIn MS stars, hydrogen is fusing to helium in the core, but the process by which fusion occurs differs depending on the mass of the star. For moderate to low mass stars, like the Sun, the core is fusing through radiation, while in high and very low mass stars, the core is fusing through convections.\nHigher mass stars have fusion through convection in their core because they have extremely high temperatures, which causes the more efficient energy transport mechanism, convection, to take over. Convection allows for the CNO cycle to take place as well, which can help produce helium more efficiently than through the radiative p-p chain at the high mass star’s given temperature. Additionally, convection allows a more efficient mixing of elements in the star, which extends the lifetime of the star as it is able to use hydrogen and helium that are not just in the nearest regions to the core.\nThe Sun is more luminous than when it first arrived on the MS because its composition has changed. He is now the dominant element in the core, which means the core’s mean molecular weight has increased, causing contraction and subsequent temperature increase.\n\nGiven my revisions primarily elaborated on ideas already expressed in my original answer, I would give my original answers an 8/10.\n\n\n\nProblem 10\nConsider the evolutionary path shown in the HR diagram below.\n\nWhat mass star is shown in this evolutionary tract?\nDescribe what is happening between each letter.\n\n\nOriginal Answer (10/10):\n\nGiven the slope of the horizontal branch, this is likely a lower mass star \\(\\sim1M_\\odot\\).\n\nStar is in MS, burning H to He. (b) H core exhausted, H-shell stars to burn, start of subgiant branch: He core contracts, core temp increases, envelope expands, \\(T_{eff}\\) decreases, He ash falls on core, core contracts, cycle repeats till S-C limit. (c) S-C limit reached, start of red giant branch, convection begins near surface growing inward, we get our first dredge up of materials (can see \\(^3_2\\)He and \\(^14_7\\)N while C and lithium move in). (d) He core flash: He core has become more \\(e^-\\) degenerate, neutrino loss in core causes temperature inversion, \\(3\\alpha\\) process starts and is nearly explosive, He shell activated, core joins, temp inversion removed and interior is as luminous as galaxy for short time, but outer layers absorb energy. (e) After He core flash, star temp is increasing and luminosity decreasing, and we begin the horizontal branch, blueward. Envelope is contracting and we have convection in core and envelope. Blue motion is analogous to H fusion in MS, just with He. At bluest point, \\(\\mu\\) is increasing and the core contracts, envelope exands and cools, moving redward. Eventually He core is exhausted and CO core continues to contract. He shell narrows, temperature increases, layers expand, cooling (pulsations). H shell turns off temporarily, CO core contracts, neutrino production increases, core temperature decreases. (e-to-f) We get to early asymptotic giant branch: He shell burns like H-shell in RGB and the envelope absorbs initially, then convection causes a second dredge up. The H shell turns back on, He shell turns off/on again and again through similar process to He core flash (i.e. ash from H shell) and \\(e^-\\) degenerate core causes based for shell to expand and repeat the process in the thermal pulse AGB phase. (f-to-g) Mass loss prevents collapse of star, core makes additional elements, we can get superwind, eventually we loose enough mass that the envelope form superwind is optically thin and the remnant degenerate core is revealed (but that isn’t show in the plot).\n\n\nRevised Answer (10/10): After checking in the book, I think I could have revised by answer to a) to say something about it being a 1-5\\(M_\\odot\\) star, but that is just extra clarification that isn’t too necessary. For that reason, I would give myself a 10/10 for my original answer to this problem.\n\n\n\nProblem 11\nHow would the evolutionary tract change if the star was 5-10 times more massive than the star shown in the HR diagram above? Provide as much detail as possible, again commenting on what would be similar or different between each letter.\n\nOriginal Answer (7/10): If stars were more massive, we would expect no He core flash, and the horizontal branch would appear more horizontal. We’d also get a 3rd dredge up in the thermal pulse AGB and a more flat subgiant branch.\nRevised Answer (10/10): Given that we infer the star to be around 1 solar mass, this question is asking about the stellar evolution of a ~5-10 solar mass star. I would argue that my original answers hold, but they only touched on the differences as the rest of the evolution of a ~5-10 solar mass star is quite similar to that of a 1 solar mass star. Directly copying the description of stellar evolution from Problem 10 to replace a few phrases was too time consuming at the time of the exam, but I do so now below.\n\nStar is in MS, burning H to He. (b) H core exhausted, H-shell stars to burn, start of subgiant branch: He core contracts, core temp increases, envelope expands, \\(T_{eff}\\) decreases, He ash falls on core, core contracts, cycle repeats till S-C limit. (c) S-C limit reached, start of red giant branch, convection begins near surface growing inward, we get our first dredge up of materials (can see \\(^3_2\\)He and \\(^14_7\\)N while C and lithium move in). (d-to-e) No He core flash, rather a more He core brning. Star temp is increasing and luminosity decreasing, and we begin the horizontal branch, blueward. Envelope is contracting and we have convection in core and envelope. Blue motion is analogous to H fusion in MS, just with He. At bluest point, \\(\\mu\\) is increasing and the core contracts, envelope exands and cools, moving redward. Eventually He core is exhausted and CO core continues to contract. He shell narrows, temperature increases, layers expand, cooling (pulsations). H shell turns off temporarily, CO core contracts, neutrino production increases, core temperature decreases. (e-to-f) We get to early asymptotic giant branch: He shell burns like H-shell in RGB and the envelope absorbs initially, then convection causes a second dredge up. The H shell turns back on, He shell turns off/on again and again through similar process to He core flash (i.e. ash from H shell) and \\(e^-\\) degenerate core causes based for shell to expand and repeat the process in the thermal pulse AGB phase. Convection zones between He shell and H shell combine with convection zones in the envelope and we get our third dredge up, where we are able to see carbon spectroscopically, known as “carbon stars”. (f-to-g) Mass loss prevents collapse of star, core makes additional elements, we can get superwind, eventually we loose enough mass that the envelope form superwind is optically thin and the remnant degenerate core is revealed (but that isn’t show in the plot).\n\n\n\n\nReflection\n\nWhat did you feel the most and least confident about going into the exam?\n\n\nAnswer: After the first exam, I felt most confident about the style of questions to expect and my ability to cover as many questions as possible in the time allowed. I think I did a great job with that, as I recognized question 10 as requiring a lot of writting and thus devoted most of my intial work to writing my answer out. I also felt confident in my ability to understand extinction as I spent an independent study with Jay on the topic.\nI felt least confident in my knowledge of the Sun. Namely, I was unsure what quantitative questions could be asked on the Sun, so I was thinking questions on the topic would be more qualitiative so I refrained from putting much information on the Sun on my equation sheet.\n\n\nWhat did “grading”/“correcting” your exam help you to understand better?\n\n\nAnswer: Most directly, I think grading my exam helped me understand the answers to all the questions I didn’t have time to calculate, and specifically part c) of Problem 5 which I didn’t fully understand when I took the exam. More indirectly, I think grading my exam helped me solidify my understanding of the Jeans criterion and the initial mass function. As with the last exam, I also solidified my understanding that I am more proficient at calculations with Python than handheld calculators.\n\n\nWhat topics from the second unit do you feel like you still have the most room for improvement?\n\n\nAnswer: I think I have the most room for improvement on the physics during the stages of stellar evolution. My equation sheet had the entire stellar evolution process written out for each stage and a corresponding stellar evolution HR diagram for both low and moderate mass stars. The information for the unit is extremely dense and understanding it is vital because it quite literally describes how stars evolve, and a better understanding of how stars evolves improves our understanding of galaxies and larger structures. All of that is to say, I found remembering concepts from the stellar evolution section difficult because of how dense the section is.\n\n\nIf 10 pts per problem (except problem 6), what would your original grade be versus the version you’re turning in now?\n\n\nAnswer: Excluding problem 6, I would have received a 78/100. With revisions, I believe I have corrected my exam to around 100/100."
  },
  {
    "objectID": "courses/astrophysics/index.html",
    "href": "courses/astrophysics/index.html",
    "title": "Astrophysics",
    "section": "",
    "text": "Problem Sets\n\nPS1\nPS2\nPS3\nPS4\nPS5\nPS6\nPS7\nPS9\nPS10\nPS11\nPS12\nPS13\nPS14\nPS15\nPS16\nPS17\n\n\n\nExams\n\nExam 1\nExam 2\nFinal Exam"
  },
  {
    "objectID": "courses/astrophysics/PS10.html",
    "href": "courses/astrophysics/PS10.html",
    "title": "Astrophysics PS11",
    "section": "",
    "text": "Problem 1\n\nTaking into consideration the Maxwell-Boltzmann velocity distribution, what temperature would be required for two protons to collide if quantum mechanical tunneling is neglected? You may assume that nuclei having velocities 8x the RMS values for the Maxwell-Boltzmann distribution can overcome the Coulomb barrier. Compare your answer with the estimated central temperature of the Sun.\nUsing the Maxwell-Boltzmann distribution, determine the ratio of the number of protons having velocities 8x the RMS value to those moving at the RMS velocity.\nAssuming (incorrectly) that the Sun is pure hydrogen, estimate the number of hydrogen nuclei in the Sun. Could there be enough protons moving with speed 8x the RMS value to account for the Sun’s luminosity? How long could these reactions sustain the current Solar luminosity? Hint: you may assume that the number of protons with velocity \\(\\sim v_{rms}\\) is most of the hydrogen atoms.\n\n\nimport astropy.units as u\nimport astropy.constants as c\nimport numpy as np\n\nT = c.e.value ** 2 / (384 * c.k_B * np.pi * c.eps0 * c.a0 * 2)\nT\n\n\\(1644.6616 \\; \\mathrm{\\frac{K}{F\\,J}}\\)\n\n\n\n\nProblem 2\n\nCalculate the Eddington luminosity for a 0.072 solar mass star and a 120 solar mass star.\nCompare the Eddington luminosities to their actual luminosities and discuss whether or not radiation pressure is likely to play a role in the star’s evolution.\n\n\nFor the low-mass star: log10(L/Lsolar) = -4.3, and opacity = 0.001 m2 kg-1\nFor the high-mass star: log10(L/Lsolar) = 6.252, and opacity is dominated by electron scattering (see equation 9.27)\n\n\n# M_low = \n# M_high = \n# kappa_low = \n# kappa_high = \n# L_edd_low = 4 * np.pi * c.G * c.c * M_low / kappa_low"
  },
  {
    "objectID": "courses/astrophysics/PS12.html",
    "href": "courses/astrophysics/PS12.html",
    "title": "Astrophysics PS12",
    "section": "",
    "text": "Problem 1\nIn a certain part of the North American Nebula, the amount of interstellar extinction in the visual wavelength band is 1.1 magnitudes. The thickness of the nebula is estimated to be 20 pc, and it is located 700 pc from Earth. Suppose that a B spectral class main-sequence star is observed in the direction of the nebula and that the absolute visual magnitude of the star is known to be \\(M_V = -1.1\\) from spectroscopic data. Neglect any other sources of extinction between the observer and the nebula.\n\nFind the apparent visual magnitude of the star if it is lying just in front of the nebula.\nFind the apparent visual magnitude of the star if it is lying just behind the nebula.\nWithout taking the existence of the nebula into consideration, based on its apparent magnitude how far away does the star in part b) appear to be? What would be the percentage error in determining the distance if interstellar extinction were neglected?\n\n\nimport astropy.units as u\nimport astropy.constants as c\nimport numpy as np\nA = 1.1\nnebula_thickness = 20 * u.pc\ndistance = 700 * u.pc\nM = -1.1\n\n# part a) in front of cloud\nm_front = M + 5 * np.log10(distance.value) - 5\nprint(m_front)\n\n# part b) behind cloud\nm_behind = M + 5 * np.log10(distance.value + nebula_thickness.value) - 5 + A\nprint(m_behind)\n\n# part c) behind cloud, no dust\nd_nodust = 10 ** ((m_behind - M + 5) / 5) * u.pc\npercent_error = np.abs(d_nodust - distance) / distance\nprint(d_nodust)\nprint(percent_error)\n\n8.125490200071285\n9.286662482156343\n1194.9025733550443 pc\n0.7070036762214919\n\n\n\n\nProblem 2\nPart 1) Using the Boltzmann factor, estimate the temperature required for a hydrogen atom’s electron and proton to go from being anti-aligned to being aligned. Are the temperatures in H I clouds sufficient to produce this low-energy excited state?\nPart 2) A cold H I cloud produces a 21-cm line with an optical depth at its center of \\(\\tau_\\lambda = 0.5\\). a) Where on the curve of growth do you expect this line to lie? b) Check your answer in part a) to equation 12.7 in the book. If the temperature of the gas is 100 K, the line’s full width at half-maximum is 10 km/s, and the average atomic number density is \\(10^7\\) m\\(^{-3}\\), how thick is the cloud? Express your answer in pc. c) What other observations could you conduct to verify this thickness?\n\n# part 1\nenergy = c.h * c.c / (21 * u.cm)\np_ratio = 1E-6\nT = -energy / (c.k_B * np.log(p_ratio))\nprint(T.to(u.K))  # diffuse HI clouds T~30-80K\n\n# part 2a\n\n\n# part 2b\ntau_H = 0.5\ntemp = 100 * u.K\nfwhm = 10 * u.km / u.s\nn = 1E7 / (u.m ** 3)\nN_H = tau_H * temp * fwhm / 5.2E-23\ns = N_H / n\nprint(s.to)\n\n# part 2c\n\n0.004959149671348803 K\n&lt;bound method Quantity.to of &lt;Quantity 9.61538462e+17 m3 K km / s&gt;&gt;\n\n\n\n\nProblem 3\n\nEquations 12.10 and 12.11 in the book illustrate a cooling mechanism for a molecular cloud accomplished through the excitation of oxygen atoms. Explain why the excitation of hydrogen rather than oxygen is not an effective cooling mechanism.\nWhy are the temperatures of hot cores significantly greater than dense cores?\nConsidering the cooling mechanisms discussed for molecular clouds, explain why dense cores are generally cooler than the surrounding giant molecular clouds, and why GMCs are cooler than diffuse molecular clouds.\n\n\n\nProblem 4\n\nIn the case where the magnetic energy density is much greater than the thermal energy density, the pressure support for an interstellar cloud will be dominated by magnetic pressure. For this situation, derive the critical mass needed for gravity to win and collapse the cloud. (Hint: instead of starting with the virial theorem, compare the total magnetic energy to the gravitational potential energy…).\nEstimate the gravitational energy per unit volume in the dense core of the giant molecular cloud in the book’s example 12.2.1, and compare that with the magnetic energy density that would be contained in the cloud if it had a magnetic field of uniform strength, B = 1 nT. Could the magnetic fields play a significant role in the collapse of the cloud? At what strength would the magnetic fields either start or stop playing a significant role in the collapse of the cloud?"
  },
  {
    "objectID": "courses/astrophysics/PS14.html",
    "href": "courses/astrophysics/PS14.html",
    "title": "Astrophysics PS14",
    "section": "",
    "text": "Problem 1\nThe “Padova” stellar evolution models are among the most sophisticated and frequently used. These models allow the user to examine the physical properties of stars in different evolutionary phases. The most recent updates to the models are described in Pastorelli et al. (2020); the various references included therein allow a complete perspective on the historical development of the models.\nThe Padova models have been used to create the plots on the following two slides, which show a model of a 1 Solar mass star. In the first plot, the luminosity is plotted versus surface temperature, with points color-coded by age. In the second plot, mass loss rate is plotted against age.\nUsing your understanding of stellar evolution, write a complete description of the physical changes that are occurring as the star evolves in this diagram. What stage(s) of stellar evolution are being tracked? How do you know? What happened to the star immediately before this part of its evolution? What do you expect to happen immediately after? Your response should include all aspects of the physical processes at work in the star.\n \n\nAnswer: The plots above are tracking the evolutionary stages of a 1 solar mass star after He core flash (end of RGB) and into early AGB. Below is a list that describes the evolutionary stages of the model in order.\n\nHe Core Flash\n\n\nHe core continued to collapse during the RGB, becoming increasingly electron degenerate.\nSignificant neutrino loss caused a negative temperature gradient (temperature inversion)\nDensity and temperature increse enough for triple-alpha process to start, causing a nearly explosive energy release\nHe shell is activated, with the core joining shortly after, removing temperature inversion\nLuminosity generated by He burning core is comperable to that of an entire galaxy, but only lasts for a couple seconds, with most of the energy never reaching the surface, rather getting absorbed by the overlying layers.\n\n\nHorizontal Branch\n\n\nThe envelop is contracting, increasing effective temperature, and causing convection zones to arise in both the core and envelope.\nBlueward motion is essentially helium-burning analog of hydrogen-burning in MS, just on a much shorter timescale\nAt bluest point, the mean molecular weight has increased such that the core begins to contract, causing the envelop to expand and cool.\nShortly after starting the move redward, the helium core is exhausted, and the CO core continues to collapse.\nPulsations can occur due to instabillities.\nThe He shell narrows and the temperature increases, which causes the surrounding layers to expand and cool, causing a temporary turn-off for H-burning shell.\nHe exhausted core contracts, neutrino production increases, core cools slightly\n\n\nEarly Asymptotic Giant Branch\n\n\nHe-burning-shell analog to H-burning-shell RGB; He-burning shell dominates energy output (H-burning shell is nearly inactive)\nExpanding envelop initially absorbs most of the energy, but as the effective temperature continues to decrease, the convenctive envelope deepens again, creating a second dredge-up that increases He and N contents of the envelope\n\n\n\n\nProblem 2\nThe Padova evolutionary models calculate the masses in the cores and envelopes of each model star in each time interval. Here we will consider the model stars that are in the TP-AGB phase after log(t)=8.04 years; these are stars whose initial masses were 5.11 Solar masses or larger. More massive stars have evolved completely, and less massive stars have not yet reached the TP-AGB phase. Using the file “Padova_log_age_8.04.dat” on Moodle, create two plots:\n\nShow the ratio of core mass (column labeled “McoreTP”) to remaining star mass (column labeled “Mass”) as a function of remaining star mass. Bonus “points” if you colorize the points by age!\nShow the mass loss rate (column labeled “Mloss”) as a function of remaining star mass. Bonus “points” if you colorize the points by age!\nAs remaining stellar mass increases, does the ratio of core mass to total mass increase or decrease? How should this be interpreted, given the results we see in the mass loss versus time plot?\n\n\nAnswer: The code below generates the plots in parts 1) and 2).\nAs the remaining stellar mass increases, the ratio of core mass to total mass decreases. In other words, as we lose mass, the core to total mass ratio increases, indicating that the core is becoming the dominant mass in the star.\n\n\nfrom astropy.io import ascii\nimport matplotlib.pyplot as plt\nimport numpy as np\ndata = ascii.read(\"Padova_log_age_8.04.dat\")\ndata[\"McoreTP/Mass\"] = data[\"McoreTP\"] / data[\"Mass\"]\n\n# 2a\nplt.figure()\nplt.scatter(data[\"Mass\"], data[\"McoreTP/Mass\"], c=data[\"logTe\"])\nplt.plot(data[\"Mass\"],data[\"McoreTP/Mass\"])\nplt.colorbar(label = \"Log Effective Temperature\")\nleft, right = plt.xlim()\nplt.xlim(right, left)\nplt.xlabel(\"Remining Stellar Mass\")\nplt.ylabel(\"Core Mass / Remaining Stellar Mass\")\n\n# 2b\nplt.figure()\nplt.plot(data[\"Mass\"],data[\"Mloss\"])\nplt.xlabel(\"Remaining Stellar Mass\")\nplt.ylabel(\"Mass Loss\")\nleft, right = plt.xlim()\nplt.xlim(right, left)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProblem 3\nThe Padova evolutionary models calculate the mass loss rate (in units of Solar masses per year) using a Reimers approximation at each time interval:\n\\[\\frac{dM}{dt} = (-4\\times10^{-13})\\frac{\\eta\\cdot L}{g\\cdot R}\\]\nwhere L, R, and g are in Solar units. From the second graph in the first problem, we see that the mass loss rate goes very high in the last time intervals. The model values at the last calculated time (\\(\\log t = 10.05\\), where t is in units of years) are as follows:\n\\[\n\\begin{align*}\n    \\log\\left(\\frac{L}{L_\\odot}\\right) = 1.345 \\\\\n    \\log\\left(\\frac{T}{\\text{Kelvin}_\\odot}\\right) = 3.6634 \\\\\n    \\log\\left(\\frac{g}{g_\\odot}\\right) = 3.676 \\\\\n\\end{align*}\n\\]\nThe surface gravity is expressed units of cm sec\\(^{-2}\\).\nUsing a value of \\(\\eta = 0.25\\), find\n\nThe radius of the model star in solar radii (Is it what you would expect?)\nThe value of the mass loss rate in solar radii.\n\n\nAnswer: The code chunk below calculates the stellar radius using the Stefan-Boltzmann Law\n\\[L = 4\\pi R^2\\sigma T^4.\\]\nUsing this equation, we get an estimated radius of around 7.4 solar radii, which is reasonable considering we are dealing with a start that was initially around 5 solar masses and it is going through periods of pulsation.\nUsing the calculated radius in part a) and solving for the luminosity, temperature, and surface gravity in the equations provided, we can use the given formula for mass loss,\n\\[\\frac{dM}{dt} = (-4\\times10^{-13})\\frac{\\eta\\cdot L}{g\\cdot R},\\]\nto solve for part b). Note that we get a value that is essentially zero. This is in agreement with what we see in our plots for Question 2, where most of the mass of the star is in the core, so there is no fuel for the core to do fusion, so there are no thermal pulses, so there is no mass loss.\n\n\nimport astropy.units as u\nimport astropy.constants as c\nT = 10 ** 3.6634 * u.K\nL = 10 ** 1.345 * u.L_sun\ng = 10 ** 3.676 * u.cm / u.s ** 2\neta = 0.25\n\n# a\nR = np.sqrt(L / (4 * np.pi * c.sigma_sb * T ** 4))\nprint(f\"{R.to(u.R_sun):.2f}\")\n\n# b\ndMdt = -4E-13 * eta * L / (g * R.to(u.R_sun))\nprint(f\"{dMdt.to(u.M_sun / u.yr):.2e}\")  # basically zero\n\n7.39 solRad\n-5.52e-20 solMass / yr"
  },
  {
    "objectID": "courses/astrophysics/PS16.html",
    "href": "courses/astrophysics/PS16.html",
    "title": "Astrophysics PS16",
    "section": "",
    "text": "Problem 1\nAssume that the 1.1 Solar mass core of a star with a zero age main sequence mass of 13 Solar masses collapses in a type II supernova event. Assume that 100% of the energy released by the collapsing core is converted into neutrinos, and that 2.5% of the neutrinos are absorbed by the overlying layers in order to power the ejection of the remnant. Here you will estimate the radius of the collapsed core remnant if sufficient energy is liberated to just barely eject the remaining non-core mass to a distance of infinity. Use values for this supergiant star from Appendix G of the text.\n\nWhat is a reasonable initial radius for the 1.1 Solar mass core? Justify your choice.\nUsing the radius you described in part a), what is the radius of the collapsed core remnant? –&gt; NS from WD radius\nCompare the value in b) to size of the compact core remaining from type II SNe. Does it seem reasonable? Why or why not?\n\n\nAnswer: All stars less than 8 solar masses evolve to white dwarfs. Thus, before our 13 ZAMS star undergoes core collapse, we would expect the uncollapsed core to be electron degenerate and have a radius on the order of that of a white dwarf of similar mass. In this case, our 1.1 Solar mass core that we are interpreting as a white dwarf would have a radius around that of the radius of the Earth.\nFor part b) we know that the total energy to eject the material is 2.5% of the change in gravitational potential energy of the core:\n\\[E_{eject} = 0.025 \\Delta E_c\\]\nIf we expand the equation, we get the following:\n\\[\\frac{GM_cM_e}{r_{avg}} = 0.025\\left(\\frac{GM_c^2}{r_f} - \\frac{GM_c^2}{r_i}\\right)\\]\nwhich, solving for \\(r_f\\), simplifies to\n\\[r_f = \\frac{0.025 r_{avg} M_c r_i}{M_e r_i + 0.025 r_{avg} M_c}\\]\nThe code chunk below estimates \\(r\\) to approximately 6000 km, which is 2 orders of magnitude larger than what we expect for a neutron star (~10 km).\n\n\nimport astropy.units as u\nimport astropy.constants as c\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nr_avg = 78 * u.R_sun\nr_i = 1 * u.R_earth\nM_star = 13 * u.M_sun\nM_c = 1.1 * u.M_sun\nM_e = M_star - M_c\n\nr_f = 0.025 * r_avg * M_c * r_i / (M_e * r_i + 0.025 * r_avg * M_c)\nprint(r_f.to(u.km))\n\n6069.401274017802 km\n\n\n\n\nProblem 2\nImagine that the energy released during the decay of a particular radioactive species is 2.89 MeV. If 0.057 Solar masses of this species is produced in a SN event, and if this species has a mass of 56 atomic mass units and a radioactive half-life of 88.8 days, then calculate the following quantities:\n\nThe total number of atoms of this species\nThe decay constant of this species\nThe number of decays per second\nThe luminosity of the remnant immediately after the SN event in Solar luminosities\nThe luminosity of the remnant 1 year after the SN event in Solar luminosities\nThe amount of time required for the remnant to fade by 6.191 magnitudes\n\n\nAnswer: The code chunk below calculates the answer to each question.\n\n\nM_net = 0.057 * u.M_sun\nm_amu = 56 * u.u\nE_amu = 2.89 * u.MeV\nhalf_life = 88.8 * u.d\n\n# a)\nn_atoms = M_net / m_amu\nprint(f\"a) total number of atoms: {n_atoms.to(u.kg / u.kg):.2e} atoms\")\n\n# b)\ndecay_const = np.log(2) / half_life\nprint(f\"b) decay constant: {decay_const:.2e}\")\n\n# c)\nprint(f\"c) num decays/sec: {decay_const.to(1 / u.s):.2e}\")\n\n# d)\nL = E_amu * decay_const * n_atoms \nprint(f\"d) L (init): {L.to(u.L_sun):.2e}\")\n\n# e)\nnew_amount = n_atoms * np.exp(-decay_const * 1 * u.yr)\nL_yr = E_amu * decay_const * new_amount\nprint(f\"e) L (1yr): {L_yr.to(u.L_sun):.2e}\")\n\n# f)\nT = np.log(100 ** (6.191/5)) / decay_const\nprint(f\"f) Time to fade 6.191 mag: ~{T.to(u.yr):.2f}\")\n\na) total number of atoms: 1.22e+54 atoms\nb) decay constant: 7.81e-03 1 / d\nc) num decays/sec: 9.03e-08 1 / s\nd) L (init): 1.33e+08 solLum\ne) L (1yr): 7.70e+06 solLum\nf) Time to fade 6.191 mag: ~2.00 yr\n\n\n\n\nProblem 3\nIndividual “A” stands (miraculously and unphysically) on the surface of the more massive neutron star that gave rise the GW170817 event. Individual “B” is in a starship located 0.5 AU away from the neutron star; there is no relative motion between the starship and the neutron star.\n\nIf one hour of time elapses for individual “A”, how much time elapses for individual “B”?\nThinking that one hour is quite enough time to decide that the surface of a neutron star is a very unpleasant place, individual “A” sends an electromagnetic plea for help to individual “B”. This message is broadcast by individual “A” at the rest wavelength of the Hydrogen Balmer alpha transition. To what wavelength (in nm) should individual “B” tune their receiver in order to receive this message?\n\n\nAnswer: The code chunk below calculates the values for part a) and b). Note that a radius \\(r_0=10\\) km was assumed as that is the typcially accepted value of a neutron star. For both parts, the following equation was used:\n\\[\\frac{\\Delta t_0}{\\Delta t_\\infty} = \\frac{\\nu_\\infty}{\\nu_0} = \\left(1 - \\frac{2GM}{r_0 c^2}\\right)^{1/2}\\]\nwhere the 0 and \\(\\infty\\) index indicate individual “A” and “B”, respectively.\nNote that for part b), we can solve for \\(\\lambda\\) by \\(c=\\lambda\\nu\\), which adds the following relation to our equation above.\n\\[\\frac{\\lambda_0}{\\lambda_\\infty} = \\frac{\\Delta t_0}{\\Delta t_\\infty} = \\frac{\\nu_\\infty}{\\nu_0} = \\left(1 - \\frac{2GM}{r_0 c^2}\\right)^{1/2}\\]\n\n\nr0 = 10 * u.km  # radius of the neutron star\nM = 1.48 * u.M_sun\nt_A = 1 * u.hr\n\n# a)\nt_B = t_A / (1 - 2 * c.G * M / (r0 * c.c ** 2)) ** 0.5\nprint(f\"t_B: {t_B}\")\n\n# b)\nlambda_A = 656.28 * u.nm\nratio = t_A / t_B\nlambda_B = lambda_A / ratio\nprint(f\"lambda_B: {lambda_B}\")\n\nt_B: 1.3328370313320161 h\nlambda_B: 874.7142869225755 nm\n\n\n\n\nProblem 4\nThe Event Horizon Telescope has revolutionized our understanding of the regions surrounding supermassive black holes that reside in the dynamical centers of many massive galaxies.\n\nUse the mass of the M87 black hole derived in the manuscript by the Event Horizon Telescope Collaboration (2019) to calculate the Schwarzschild radius of this black hole. How does this compare to the observed “shadow”?\nIt is important to debunk the myth that black holes are furious beasts that devour galaxies. Instead they are rather boring objects that behave like benign point sources unless material ventures within the Schwarzschild radius. To demonstrate this, create a log-log plot of the Schwarzschild radius (y-axis) as a function of the mass of the black hole (x-axis). Allow the mass to range from 1 Solar mass to \\(10^{10}\\) Solar masses.\n\nOn this plot, show the following physical scales with horizontal lines: 1 Earth radius, 1 Solar radius, 1 AU, and 100 AU. Clearly label each line. Plot the position of the M87 black hole on this diagram. Upload this image in the attachments section\n\nAnswer: The code chunk below calculates the Schwarzchild radius of the black hole to be around \\(2.7\\cdot10^4\\) solar radii, and the shadow of the black hole to be around \\(7.59\\cdot10^4\\) solar radii. We expect the shadow to be on the order of the Schwarzchild radius.\nThe code chunk below also creates the desired log-log plot in part b) of the Schwarzchild radius as a function of black hole mass.\n\n\nM87 = 6.5E9 * u.M_sun\ndist87 = 16.8 * u.Mpc\nangDiam87 = 42E-6 * u.arcsec\n\n# a)\nR_s_87 = 2 * c.G * M87 / c.c ** 2\ndiam87 = angDiam87.to(u.rad) * dist87 / u.rad\nrad87 = diam87 / 2\nprint(f\"Schwarzchild raius: {R_s_87.to(u.R_sun):.2e}\")\nprint(f\"Radius of shadow: {rad87.to(u.R_sun):.2e}\")\n\n# b)\nmass = np.linspace(1, 1E10, 1000) * u.M_sun\nR_s = 2 * c.G * mass / c.c ** 2\nplt.plot(mass, R_s.to(u.R_sun))\nplt.axhline((1 * u.R_sun).value, linestyle=\"--\", color=\"orange\", label=\"1 $R_{\\\\odot}$\")\nplt.axhline((1 * u.au).to(u.R_sun).value, linestyle=\"--\", color=\"green\", label=\"1 AU\")\nplt.axhline((100 * u.au).to(u.R_sun).value, linestyle=\"--\", color=\"purple\", label=\"100 AU\")\nplt.plot(M87.to(u.M_sun), R_s_87.to(u.R_sun), 'o', label=\"M87\", )\nplt.legend()\nplt.xlabel(\"Mass [$M_{\\\\odot}$]\")\nplt.ylabel(\"$R_S$ [$R_{\\\\odot}$]\")\nplt.xscale(\"log\")\nplt.yscale(\"log\")\n\nSchwarzchild raius: 2.76e+04 solRad\nRadius of shadow: 7.59e+04 solRad"
  },
  {
    "objectID": "courses/astrophysics/PS2.html",
    "href": "courses/astrophysics/PS2.html",
    "title": "Astrophysics PS2",
    "section": "",
    "text": "Problem 1\nConsider the extrasolar planet system known as TRAPPIST-1 (the system with the most Earth-size habitable zone planets!). Refer to https://arxiv.org/abs/1703.01424 for relevant orbital parameters. Rank the orbital angular momentum of each planet from highest to lowest. Before you do any calculations take a stab at the ranking using your intuition and the derived equation. Compare your guess to the final ranking. Any surprises?\n\nAnswer: For our intuition approach, looking at the angular momention equation\n\\[L = \\mu\\sqrt{GMa(1-e^2)}\\]\nallows us to make some simplifications given our system’s properties. For starters, the system mass \\(M\\) is approximately the same for all orbits due to the mass of the star in comparison to the exoplanets. Additionally, the constant \\(G\\) will not change for each system. If we consider the reduced mass \\(mu\\), we can make the following simplification for a system with a very large (\\(m_1\\)) and very small mass (\\(m_2\\)).\n\\[\\mu = \\frac{m_1 m_2}{m_1 + m_2} \\approx \\frac{m_1 m_2}{m_1} = m_2\\]\nThus, the primary characteristics for determining the angular momentum, by intuition, is the mass of the exoplanet, the semi-major axis, and the eccentricity.\n$$\\(L \\sim m_2 \\sqrt{a(1-e^2)}\\)\nUsing the values given in the paper, we predict the following.\n\ng (largest L)\nc\nb\nf\ne\nd (smallest L)\n\nChecking our values with a calculation…\n\n\nimport astropy.units as u\nimport astropy.constants as c\n\nstar_m = 0.0802 * c.M_sun\n\ndef L(m, a, e):\n    return ((star_m * m)/(star_m + m)) * (c.G * (star_m + m) * a * (1 - e**2)) ** 0.5\n\n# m, a, e\ntrappist_b = [0.85 * c.M_earth, 11.11 * u.au * 10 ** -3, 0.081]\ntrappist_c = [1.38 * c.M_earth, 15.21 * u.au * 10 ** -3, 0.083]\ntrappist_d = [0.41 * c.M_earth, 21.44 * u.au * 10 ** -3, 0.070]\ntrappist_e = [0.62 * c.M_earth, 28.17 * u.au * 10 ** -3, 0.085]\ntrappist_f = [0.68 * c.M_earth, 37.1 * u.au * 10 ** -3, 0.063]\ntrappist_g = [1.34 * c.M_earth, 45.1 * u.au * 10 ** -3, 0.061]\n\nlb = L(trappist_b[0], trappist_b[1], trappist_b[2]).to(u.kg * u.m ** 2 / u.s)\nlc = L(trappist_c[0], trappist_c[1], trappist_c[2]).to(u.kg * u.m ** 2 / u.s)\nld = L(trappist_d[0], trappist_d[1], trappist_d[2]).to(u.kg * u.m ** 2 / u.s)\nle = L(trappist_e[0], trappist_e[1], trappist_e[2]).to(u.kg * u.m ** 2 / u.s)\nlf = L(trappist_f[0], trappist_f[1], trappist_f[2]).to(u.kg * u.m ** 2 / u.s)\nlg = L(trappist_g[0], trappist_g[1], trappist_g[2]).to(u.kg * u.m ** 2 / u.s)\n\nprint(f\"L trappist b: {lb:.2e}\")\nprint(f\"L trappist c: {lc:.2e}\")\nprint(f\"L trappist d: {ld:.2e}\")\nprint(f\"L trappist e: {le:.2e}\")\nprint(f\"L trappist f: {lf:.2e}\")\nprint(f\"L trappist g: {lg:.2e}\")\n\nL trappist b: 6.73e+38 m2 kg / s\nL trappist c: 1.28e+39 m2 kg / s\nL trappist d: 4.51e+38 m2 kg / s\nL trappist e: 7.81e+38 m2 kg / s\nL trappist f: 9.85e+38 m2 kg / s\nL trappist g: 2.14e+39 m2 kg / s\n\n\n\nwe get agreement with the largest and smallest values,\n\ng (largest L)\nc\nf\ne\nb\nd (smallest L)\n\nwhich is relatively expected for a simple intution guess with no calculations.\n\n\n\nProblem 2\nComet C/2013 A1 (Siding Spring) was discovered in January 2013. Refer to https://arxiv.org/abs/2012.12172 for relevant orbital parameters. Calculate the following five orbital properties: orbital period, perihelion distance in AU, aphelion distance in AU, perihelion velocity in km/s, and aphelion velocity in km/s. You may ignore uncertainties.\n\nAnswer: The equations for orbital period, aphelion, perihelion, aphelion velocity, and perihelion velocity are shown below, respectively.\n\\[P^2 = \\frac{4\\pi^2}{G(m_1 + m_2)}a^3\\] \\[\\text{perihelion} = a - ae\\] \\[\\text{aphelion} = a + ae\\] \\[v_p^2 = \\frac{GM}{a}\\left(\\frac{1+e}{1-e}\\right)\\] \\[v_a^2 = \\frac{GM}{a}\\left(\\frac{1-e}{1+e}\\right)\\]\nUsing the equations above, and the values for \\(a\\), and \\(e\\) in the paper provided (in addition to some constants), we can solve for the five orbital properties listed earlier.\n\n\nimport math\na = 2.590376582621090E4 * u.au\ne = 0.9999458751571103\n\nP = ((4 * math.pi ** 2) * a ** 3 / (c.G * c.M_sun)) ** 0.5\nperihelion = a - a * e\naphelion = a + a * e\nv_p = (c.G * c.M_sun * ((1 + e)/(1 - e)) / a) ** 0.5\nv_a = (c.G * c.M_sun * ((1 - e)/(1 + e)) / a) ** 0.5\n\nprint(f\"Orbital Period: {P.to(u.yr):.2e}\")\nprint(f\"Perihelion: {perihelion.to(u.au):.2e}\")\nprint(f\"Aphelion: {aphelion.to(u.au):.2e}\")\nprint(f\"Perihelion Velocity: {v_p.to(u.km / u.s):.2e}\")\nprint(f\"Aphelion Velocity: {v_a.to(u.km / u.s):.2e}\")\n\nOrbital Period: 4.17e+06 yr\nPerihelion: 1.40e+00 AU\nAphelion: 5.18e+04 AU\nPerihelion Velocity: 3.56e+01 km / s\nAphelion Velocity: 9.63e-04 km / s\n\n\n\n\nProblem 3\nA planet with a mass 33.3 times larger than that of the Earth orbits a 1.7 Solar mass star with an orbital period of 8.88 years. What are the semimajor axis of the orbit (in AU) and the reduced mass (in Solar masses)?\n\nAnswer: Using the equation for orbital period (see above), we can rearrange to solve for the semimajor axis.\n\\[a = \\left(\\frac{P^2 G(m_1 + m_2)}{4\\pi^2}\\right)^{1/3}\\]\nAdditionally, we can solve for the reduced mass using the equation outlined in Problem 1. Using the equations above the values provided, we can write a simple script to solve for the semimajor and reduced mass\n\n\nm1 = 33.3 * c.M_earth\nm2 = 1.7 * c.M_sun\nP = 8.88 * u.yr\n\na = (P ** 2 * c.G * (m1 + m2) / (4 * math.pi ** 2)) ** (1/3)\nmu = ((m1 * m2)/(m1 + m2))\n\nprint(f\"Semimajor axis: {a.to(u.au):.4e}\")\nprint(f\"Reduced mass: {mu.to(u.M_sun):.4e}\")\n\nSemimajor axis: 5.1179e+00 AU\nReduced mass: 1.0001e-04 solMass"
  },
  {
    "objectID": "courses/astrophysics/PS4.html",
    "href": "courses/astrophysics/PS4.html",
    "title": "Astrophysics PS4",
    "section": "",
    "text": "Problem 1\nThe Milky Way disk is approximately 50 kpc in diameter. Imagine that you observe a spaceship, which is 100 meters in length, traveling at 0.994 c.\n\nWhile the ship is in flight, how long does it appear to you?\nWhat is the diameter of the galaxy as measured by someone on the spaceship?\nHow long does the journey across the galaxy take, as measured by you?\nHow long does the journey across the galaxy take, as measured by someone on the spaceship?\n\n\nAnswer:\n\n\nimport astropy.units as u\nimport astropy.constants as c\n\nD_mw = 50 * u.kpc\nD_s = 100 * u.m\nv = 0.994 * c.c\n\ngamma = 1 / (1 - v**2 / c.c ** 2) ** 0.5\n\n# moving rulers are compressed\nD_s_prime = D_s / gamma\n\nD_mw_prime = D_mw / gamma\n\n# Moving clocks run slow\nt_rest = D_mw / v\n\nt_moving = t_rest / gamma\n\nprint(f\"length of moving ship from MW reference frame: {D_s_prime:.2f}\")\nprint(f\"length of MW from ship perspective: {D_mw_prime:.2f}\")\nprint(f\"time for ship to cross MW from MW perspective: {t_rest.to(u.yr):.2f}\")\nprint(f\"time for ship to cross MW from ship perspective: {t_moving.to(u.yr):.2f}\")\n\nlength of moving ship from MW reference frame: 10.94 m\nlength of MW from ship perspective: 5.47 kpc\ntime for ship to cross MW from MW perspective: 164062.56 yr\ntime for ship to cross MW from ship perspective: 17945.17 yr\n\n\n\n\nProblem 2\nStarting with the equation for superluminal motion, derive the following equations:\n\nv/c &lt; 1 for angles satisfying \\(\\frac{\\frac{v_{app}^2}{c^2} - 1}{\\frac{v_{app}^2}{c^2} + 1} &lt; \\cos\\phi &lt; 1\\)\nThat the smallest possible value for v/c for the source is \\(\\frac{\\frac{v_{app}^2}{c^2} - 1}{\\frac{v_{app}^2}{c^2} + 1} &lt; \\cos\\phi &lt; 1\\)\nThe smallest possible v/c will occur at an angle \\(\\phi_{min}\\) such that \\(\\cot\\phi_{min} = \\frac{v_{app}}{c}\\)\nThe Lorentz factor corresponding to the minimum v/c is \\(\\gamma_{min} = \\frac{1}{\\sqrt{1 - v_{min}^2}} = \\sqrt{1 + v_{app}^2/c^2} = \\frac{1}{\\sin\\phi_{min}}\\)\nThe inner 6” of the jet from M87 is observed to have an apparent velocity of 4.5c https://ui.adsabs.harvard.edu/abs/2013ApJ…774L..21M/abstract. Using your newly derived equations, estimate the minimum velocity, minimum approaching angle, and Lorentz factor of the jet.\n\n\nAnswer:\n\n\n\nProblem 3\nUse the ADS and/or ArXiV search engine tools to identify a recent (within the past few years) peer-reviewed scholarly manuscript that deals with a relativistic concept and/or observations of superluminal motion. Provide a brief summary of the main results of the paper, including a discussion of how the material connects to the concepts that we discussed in Chapter 4 of the textbook.\n\nAnswer:"
  },
  {
    "objectID": "courses/astrophysics/PS6.html",
    "href": "courses/astrophysics/PS6.html",
    "title": "Astrophysics PS6",
    "section": "",
    "text": "Problem 1\nThe Event Horizon Telescope revolutionized our understanding of black holes by producing the first image of the immediate region surrounding one such structure in the galaxy M87. The theoretical diffraction limit of the EHT is given in the discovery paper.\nTo demonstrate how amazing this is, consider that normal human hair grows at the rate of a half an inch per month. Imagine you could use the EHT to observe a human standing on the surface of the moon at its perigee orbital position. Using Lunar parameters from this page, how long (in units of days) will you have to wait for this instrument to be able to measure the growth of that human’s hair?\n\nAnswer: The diffraction limit is esstentially the smallest angle we are able to see with a telescope. According the discovery paper, the diffraction limit of the EHT is \\(25\\mu\\)as. Now the question becomes: If we place a person on the Moon at perigee, how long will we have to wait before we can observe their hair growing?\nUsing trig, we can determine the length corresponding to our diffraction angle at a distance of the Moon’s perigee. Dividing that length by the hair growth rate will tell us how long we would have to wait before we can resolve the image of the hair.\nThe code below does just that.\n\n\nimport astropy.units as u\nimport astropy.constants as c\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndiff_limit_EHT = 25 * 1E-6 * u.arcsec\n\nhair_growth_rate = 0.5 / 39.37 * u.meter / (1/12 * u.yr)\n\nmoon_perigee = 0.3633 * 1E6 * u.km\n\nphysical_diff_lim = moon_perigee * np.tan(diff_limit_EHT)\n\ntime = physical_diff_lim / hair_growth_rate\n\n\nprint(f\"Wait time: {time.to(u.yr) * 365 / u.yr:.2f} days\")\n\nWait time: 105.46 days\n\n\nNote that this time is approximately 3 months, which would correspond to hair that is 1.5 inches in length. Putting this all into context, the EHT can resolve the 1.5 inch long hair of a person on the Moon at perigee.\n\n\nProblem 2\nConsider a radio telescope operating in the S-band, which covers the frequency range of 2-4 GHz. Imagine that the telescope’s frequency dependence is triangular, meaning that the sensitivity of the detector is zero at the edges and maximal at band center.\nThis filter function is expressed as\n\\[f_\\nu = \\frac{\\nu}{\\nu_m - \\nu_l} - \\frac{\\nu_l}{\\nu_m - \\nu_l}\\text{ for } \\nu_l\\leq\\nu\\leq\\nu_m\\] \\[f_\\nu = -\\frac{\\nu}{\\nu_u - \\nu_m} + \\frac{\\nu_u}{\\nu_u - \\nu_m}\\text{ for } \\nu_m\\leq\\nu\\leq\\nu_u\\] \\[f_\\nu = 0\\text{ for all other }\\nu\\]\nHere \\(f_\\nu\\) is the filter function, \\(\\nu_m\\) is the mid-point frequency, \\(\\nu_u\\) is the upper (higher) frequency, and \\(\\nu_l\\) is the lower frequency.\nYou observe with this system at a frequency of 3.2 GHz with a bandwidth of 80 MHz.\n\nWhat are \\(\\nu_m\\), \\(\\nu_u\\), and \\(\\nu_l\\)?\n\nAssume that the radio dish is 100% efficient over the observing bandwidth and that the diameter of the dish is 65 m. Assume that the source is a galaxy with with a constant spectral flux density of 3.17 mJy over the detector’s frequency range.\n\nWhat is the total power received by the telescope?\nAssume that the source of the emission is a galaxy located at distance of 23 Mpc that radiates isotropically. What is the power emitted by this source?\n\n\nAnswer: Given our observing frequency and bandwitdth, the corresponding mid, lower, and upper frequencies are simply the center and bounds created by our bandwidth, centered at the observing frequency. Thus,\n\n\nf = 3.2 * u.GHz\nbw = 80 * u.MHz\nf_m = f\nf_l = f - 0.5 * bw\nf_u = f + 0.5 * bw\n\nprint(f\"mid-point frequency: {f}\")\nprint(f\"upper frequency: {f_u}\")\nprint(f\"lower frequency: {f_l}\")\n\nmid-point frequency: 3.2 GHz\nupper frequency: 3.24 GHz\nlower frequency: 3.16 GHz\n\n\nAssuming the radio dish is 100% effective, the total power is given by\n\\[P = S * A * \\Delta\\nu,\\]\nwhere \\(S\\) is the spectral flux density, \\(A\\) is the collecting area of the telescope, and \\(\\Delta\\nu\\) is the bandwidth.\n\nS = 3.17 * 1E-3 * u.Jy\nd = 65 * u.m\nD = 23 * u.Mpc\nA = np.pi * (d/2) ** 2\nP_obs = S * A * bw  # power (more like intensity?) received by telescope\n\nprint(f\"Power received by the telescope: {P_obs.to(u.W):.2e}\")\n\nPower received by the telescope: 8.42e-18 W\n\n\nThe ratio of the source power to the observed power is the same as the ratio of the area the intrinsic power is dispersed over (at Earch) to the area of the radio telescope.\n\\[\\frac{P_{source}}{P_{observed}} = \\frac{A_{source}}{A_{telescope}}\\]\nWe can find the power of the source by multiplying the ratio by the power observed by the telescope.\n\\[P_{source} = \\frac{A_{source}}{A_{telescope}} * P_{observed}\\]\n\n# power at source, ratio is the same\nA_source = 4 * np.pi * D ** 2\nA_telescope = np.pi * (d/2) ** 2\nratio = A_source / A_telescope\nP_source = P_obs * ratio\n\nprint(f\"Power emitted by the source: {P_source.to(u.L_sun):.2f}\")\n\nPower emitted by the source: 41.93 solLum\n\n\nThis is a very low luminosity for a galaxy, but if we are observing a star or pulsar, this might be a more reasonable value.\n\n\nProblem 3\nThe Macalester Observatory, located on the roof of Olin-Rice Hall, is a 16-inch diameter telescope with a focal length of 3251 mm.\n\nWhat is the focal ratio?\nWhat is the plate scale?\nImagine that the Macalester Observatory is used to observe two objects that are separated by 3.3 arcseconds on the sky. What is the linear separation of these objects on the focal plane in microns?\n\n\nAnswer: The focal ratio \\(F\\) is simply the focal length \\(f\\) divided by the diameter \\(D\\) of the telescope.\n\\[F = \\frac{f}{D}\\]\nThe plate scale \\(S\\) is defined as\n\\[S = \\frac{206265 \\text{ [\"] }}{f}\\]\nDividng an angular separation by the plate scale gives the linear separation on the focal plane.\n\\[\\Delta x = \\frac{\\Delta\\theta}{S}\\]\nThe code below perfoms the processes described above for the given scenario.\n\n\nf = 3251 * u.mm\nD = 16 / 39.37 * u.m\n\n# focal ratio F\nF = f / D\n\n# Plate Scale S\nS = 206265 * u.arcsec / f\n\n# Linear separation from projection\nsep = 3.3 * u.arcsec\nlin_sep = sep / S\n\nprint(f\"focal ratio F: {F.to(u.m/u.m):.2f}\")\nprint(f\"plate scale S: {S:.2f}\")\nprint(f'linear separation: {lin_sep.to(u.micron):.2f}')\n\nfocal ratio F: 8.00\nplate scale S: 63.45 arcsec / mm\nlinear separation: 52.01 micron"
  },
  {
    "objectID": "courses/astrophysics/PS9.html",
    "href": "courses/astrophysics/PS9.html",
    "title": "Astrophysics PS9",
    "section": "",
    "text": "Problem 1\nDerive various important relationships that describe a blackbody radiation field.\n\nDerive an expression for the number density of blackbody photons (photons per unit volume) with wavelength \\(\\lambda\\) and \\(d\\lambda\\). This differential should be expressed in terms \\(d\\lambda\\).\nIntegrate the expression from a) over all wavelengths to obtain an expression for the total number density of blackbody photons. This results should include only numerical factors, fundamental constants, and the variable T (temperature).\nShow that, for a blackbody, the average energy per photon is a simple relationship involving only a numerical factor, a single fundamental constant, and the variable T (temperature).\n\n\nAnswer: The blackbody spectral energy density \\(u_\\lambda\\) for some \\(\\lambda+d\\lambda\\) is defined as \\[u_\\lambda d\\lambda = \\frac{4\\pi}{c}B_\\lambda d\\lambda= \\frac{8\\pi hc}{\\lambda^5}\\left(\\frac{1}{e^{hc/\\lambda k T}-1}\\right)d\\lambda.\\]\nAddititonally, we know the a photon with wavelength \\(\\lambda\\) has an energy described by \\[E = hf = \\frac{hc}{\\lambda}\\]\nDividing the blackbody spectral energy density by the photon energy will give us the number density of blackbody photons with wavelength \\(\\lambda\\) and \\(d\\lambda\\).\n\\[\\boxed{n_\\lambda d\\lambda = \\frac{u_\\lambda d\\lambda}{E} = \\frac{8\\pi}{\\lambda^4}\\left(\\frac{1}{e^{hc/\\lambda k T}-1}\\right)d\\lambda}\\]\nIntegrating the right hand side over all wavelengths to find the total number density of blackbody photons results in the following integral.\n\\[\\int_0^\\infty\\frac{8\\pi hc}{\\lambda^5}\\left(\\frac{1}{e^{hc/\\lambda k T}-1}\\right)d\\lambda\\]\nPlugging this integral into Mathematica gives the following result\n\\[n = \\int_0^\\infty\\frac{8\\pi hc}{\\lambda^5}\\left(\\frac{1}{e^{hc/\\lambda k T}-1}\\right)d\\lambda = 16\\pi\\zeta(3)\\left(\\frac{kT}{ch}\\right)^3\\]\nwhere \\(\\zeta(3)\\) is the Riemann zeta function which has an approximate value of 1.20206. Thus our total number density of blackbody photons is\n\\[\\boxed{n = 16\\pi\\zeta(3)\\left(\\frac{kT}{ch}\\right)^3 \\approx 19.2329\\pi\\left(\\frac{kT}{ch}\\right)^3}\\]\nThe total blackbody energy density over all wavelengths is \\[u = aT^4, \\quad a = \\frac{4\\sigma}{c} = 7.566\\cdot10^{-16}\\frac{\\text{J}}{\\text{m}^2 \\text{K}^4}\\]\nIf we divide the total blackbody energy density by our number density for blackbody photons found above, we should get the average energy per photon for a blackbody with temperature \\(T\\).\n\\[\\boxed{\\bar{E}_\\gamma = \\frac{u}{n} = \\frac{aT^4}{16\\pi\\zeta(3)\\left(\\frac{kT}{ch}\\right)^3} \\approx 2.70 T}\\]\n\n\n\nProblem 2\nA ground-based observatory makes two measurements of the specific intensity from a star:\n\n\\(I_1 = 7.51\\times10^{-12}\\) W m\\(^{-2}\\) is made at a zenith angle \\(\\theta_1=17.15\\) degrees\n\\(I_2 = 5.18\\times10^{-12}\\) W m\\(^{-2}\\) is made at a zenith angle \\(\\theta_2=37.86\\) degrees\n\n\nWhat is the vertical optical depth?\nWhat is the specific intensity above the Earth’s atmosphere (in W m$^-$2)?\n\n\nAnswer: For a ground based observer, the intensity as a function of optical depth goes as\n\\[I_\\lambda = I_{\\lambda,0} e^{-\\tau_{\\lambda,0}\\sec\\theta}\\]\nTaking the ratio of our measured intensities allows us to rearrange to solve for the intrinsic optical depth,\n\\[\\tau_{\\lambda,0} = \\frac{\\ln\\left(\\frac{I_{\\lambda,1}}{I_{\\lambda,2}}\\right)}{-\\sec\\theta_1 + \\sec\\theta_2}\\]\nwhere the vertical optical depth is airmass dependent.\n\\[\\tau_\\lambda = \\tau_{\\lambda,0}\\sec\\theta\\]\nOnce \\(\\tau_{\\lambda,0}\\) has been found, we can use either measured intensity and corresponding angle to determine the specific intensity before atmospheric attenuation.\n\\[I_{\\lambda,0} = \\frac{I_\\lambda}{e^{-\\tau_{\\lambda,0}\\sec\\theta}}\\]\nThe code chunk below performs the calculations described above.\n\n\nimport astropy.units as u\nimport numpy as np\n\nI1 = 7.51E-12 * u.W / u.m**2\nI2 = 5.18E-12 * u.W / u.m**2\ntheta1 = 17.15 * u.deg\ntheta2 = 37.86 * u.deg\n\ntau0 = np.log(I1 / I2) / (-(1/np.cos(theta1)) + (1/np.cos(theta2)))\ntau1 = tau0 / np.cos(theta1)\ntau2 = tau0 / np.cos(theta2)\nI0 = I1 / (np.exp(-tau0 / np.cos(theta1)))\n\nprint(f\"Intrinsic optical depth: {tau0:.2f}\")\nprint(f\"Vertical optical depth (I1): {tau1:.2f}\")\nprint(f\"Vertical optical depth (I2): {tau2:.2f}\")\nprint(f\"Specific intensity abover Earth's atmosphere: {I0:.2e}\")\n\nIntrinsic optical depth: 1.69\nVertical optical depth (I1): 1.77\nVertical optical depth (I2): 2.14\nSpecific intensity abover Earth's atmosphere: 4.39e-11 W / m2\n\n\n\n\nProblem 3\nIn this problem you will use the values of density and opacity at various points near the surface of a model star to calculate the optical depth. The model is that of a 1 Solar mass star with \\(T_{eff} = 5504\\) K. The data in stellar-opacity.dat (on Moodle) shows physical values in the outer 3.3% of the star’s radius; the surface of the star is at \\(r=7.1\\times10^8\\) m. We will use simple numerical integration to determine the optical depth in each region of the model. Recall that optical depth is defined by the relation Examining the data in the table, we see that we are given 42 model layers in the outer regions of the star. As one moves inward from the “surface” of the star to the interior, the values of \\(\\rho\\) and \\(\\kappa\\) change. We can determine the value of tau in each region by beginning with the outermost layer and working our way inward. To do this, we will apply the “Trapezoidal Rule”, a simple but effective numerical integration technique. The values of \\(\\tau\\) in each subsequent layer can be found by applying the relation\n\\[\\tau_{i+1} = \\tau_i - \\left(\\frac{\\kappa_i \\rho_i + \\kappa_{i+1} \\rho_{i+1}}{2}\\right)\\left(r_{i+1}-r_i\\right)\\]\nwhere \\(i\\) and \\(i+1\\) designate adjacent zones in the model. Find the optical depth at each point by numerically integrating the relation for tau using the trapezoidal rule as provided.\nUsing the software package of your choice, create six plots:\n\nkappa vs radius\nrho vs radius\ntau vs radius\nTemperature vs radius\ntau vs temperature\ntemperature vs physical depth into star\n\nEstimate graphically the depth that one “sees” as the “surface” of this model star by zooming in on the appropriate region of plot #6. Include a zoom and your numerical estimate for the depth in (km).\n\nAnswer: The code below calculates the optical depth using the trapezoidal rule numerical integration for \\(\\tau_{i+1}\\).\n\n\nfrom astropy.io import ascii\nimport astropy.units as u\nimport matplotlib.pyplot as plt\n\ndata = ascii.read(\"stellar-opacity.dat\")\n\nm = 1 * u.M_sun\nt_eff = 5504 * u.K\nr = 7.1E8 * u.m\n\ntau_values = [0]\n\ntau = 0\nfor i in range(len(data) - 1):\n    tau1 = tau - (data[\"kappa\"][i] * data[\"rho\"][i] + \n        data[\"kappa\"][i + 1] * data[\"rho\"][i + 1]) / 2 * (data[\"r\"][i + 1] - data[\"r\"][i])\n    tau = tau1\n\n    tau_values.append(tau)\n\ndata[\"tau\"] = tau_values\ndata[\"physical_depth\"] = np.ones(len(data)) * r.value - data[\"r\"]\n\n\n# eddington approximation\ndata[\"temp\"] = (0.75 * t_eff ** 4 * (data[\"tau\"] + (2/3))) ** (1/4)\n\n# plotting relationships\n# kappa vs radius\nplt.figure(figsize=(8,10))\nplt.subplot(3,2,1)\nplt.scatter(y = data[\"kappa\"], x = data[\"r\"])\nplt.xlabel(\"radius [m]\")\nplt.ylabel(\"kappa [m^2/kg]\")\n\n# rho vs radius\nplt.subplot(3,2,2)\nplt.scatter(y = data[\"rho\"], x = data[\"r\"])\nplt.xlabel(\"radius [m]\")\nplt.ylabel(\"rho [kg/m^3]\")\n\n# tau vs radius\nplt.subplot(3,2,3)\nplt.scatter(y = data[\"tau\"], x = data[\"r\"])\nplt.xlabel(\"radius [m]\")\nplt.ylabel(\"tau\")\n\n# temp vs radius\nplt.subplot(3,2,4)\nplt.scatter(y = data[\"temp\"], x = data[\"r\"])\nplt.xlabel(\"radius [m]\")\nplt.ylabel(\"temp [K]\")\n\n# tau vs temp\nplt.subplot(3,2,5)\nplt.scatter(y = data[\"tau\"], x = data[\"temp\"])\nplt.xlabel(\"temp [K]\")\nplt.ylabel(\"tau\")\n\n# temp vs physical depth\nplt.subplot(3,2,6)\nplt.scatter(y = data[\"temp\"], x = data[\"physical_depth\"])\nplt.xlabel(\"physical depth [m]\")\nplt.ylabel(\"temp [K]\")\n\nplt.subplots_adjust(wspace=0.5, hspace=0.5)\nplt.show()\n\n\n\n\n\n\n\n\n\nThe code below finds the value of \\(\\tau\\) closest to 2/3, which corresponds to the optical depth we “see” as the “surface” and plots vertical and horizontal lines corresponding to the approximate physical depth and temperature, respectively.\n\n\n# calculate physical depth of optical depth = 2/3\nmask = (data[\"tau\"] &lt; 1) & (data[\"tau\"] &gt; 1/3)\nsubdata = data[\"tau\",\"physical_depth\",\"temp\"][mask]\nprint(subdata)\n\nplt.figure()\nplt.scatter(y = data[\"temp\"], x = data[\"physical_depth\"])\nplt.xlabel(\"physical depth [m]\")\nplt.ylabel(\"temp [K]\")\nplt.xlim(0, 0.2e7)\nplt.ylim(0,10000)\nplt.vlines(subdata[\"physical_depth\"][1], linestyle = '--', color = \"orange\", \n            label = \"tau ~ 2/3\", ymax = subdata[\"temp\"][1], ymin = 0)\nplt.hlines(subdata[\"temp\"][1], linestyle = '--', color = \"orange\", \n            xmax = subdata[\"physical_depth\"][1], xmin = 0)\nplt.legend()\nplt.show()\n\n        tau         physical_depth        temp       \n                                           K         \n------------------- -------------- ------------------\n0.43573334109647177       904100.0 5248.4195311617905\n 0.6007624700982208       993800.0  5434.688576387231\n 0.8392023782404089      1091500.0  5674.015822750679\n\n\n\n\n\n\n\n\n\n\nIn this case, we can “see” (\\(\\tau \\sim 2/3\\)) to a physical depth of approximately \\(10^6\\) m into the star."
  },
  {
    "objectID": "courses/honors/eda.html",
    "href": "courses/honors/eda.html",
    "title": "Honors: EDA",
    "section": "",
    "text": "# import models\nfrom astroquery.vizier import Vizier\nfrom astropy.table import join, Table, Column\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nfrom mpl_toolkits.axes_grid1.inset_locator import mark_inset\nimport plotly.graph_objects as go\nfrom astropy.io import ascii\n\n\n# query vizier for the catalog\nvizier = Vizier()\nvizier.ROW_LIMIT = 40000  # max length of catalog is ~30,000\nalf_sdss_cat = vizier.get_catalogs(\"J/AJ/160/271\")\ndata = join(alf_sdss_cat[0], alf_sdss_cat[1], keys=\"AGC\")\ndata.info()\n\n&lt;Table length=31501&gt;\n   name     dtype          unit         format                                 description                                 n_bad\n---------- ------- ------------------- -------- -------------------------------------------------------------------------- -----\n       AGC   int32                                                                       [1/749512] AGC catalog identifier     0\n      Flag   uint8                                                                               [0/3] Photometry flag (1)     0\n     ObjID   int64                                                         Optical counterpart SDSS DR15 object identifier  1863\n   RAJ2000 float64                 deg {:10.6f}                                    [0.003/360] Right Ascension (J2000) (2)     0\n   DEJ2000 float64                 deg  {:9.5f}                                       [-0.21/36.4] Declination (J2000) (2)     0\n      RVel   int16              km / s                       [-430/17823] HI profile midpoint heliocentric radial velocity     0\n      Dist float32                 Mpc  {:5.1f}                        [0.3/260] Distance from Haynes+, 2018, J/ApJ/861/49     0\n    e_Dist float32                 Mpc  {:4.1f}                                               [0/30.9] Uncertainty in Dist     0\n      gext float32                 mag  {:5.2f}             [0.02/4.44]? Foreground Galactic extinction in SDSS g band (3)  1863\n      iext float32                 mag  {:5.2f}             [0.01/2.28]? Foreground Galactic extinction in SDSS i band (3)  1863\n       b/a float32                      {:5.2f}                                 [0.05/1]? Axial ratio b/a from SDSS r band  1863\n     e_b/a float32                      {:6.2f}                                         [0.01/865]? Uncertainty in b/a (4)  1863\n      imag float32                 mag  {:5.2f}                                 [9.58/30.89]? SDSS i band cmodel magnitude  1863\n    e_imag float64                 mag  {:8.2f}                                   [0.01/42134.68]? Uncertainty in imag (4)  1863\n     Dprop    str5                                                                            Derived properties (table 2)     0\n     Sloan    str5                                                     Display the online SDSS DR16 for the nearest object     0\n        Ag float32                 mag  {:5.2f}                            [0/1.91]? g band internal extinction factor (1)  3234\n        Ai float32                 mag  {:5.2f}                            [0/0.99]? i band internal extinction factor (1)  3234\n      iMAG float32                 mag  {:6.2f}                         [-24/-10]? Corrected absolute i band magnitude (2)  3234\n    e_iMAG float32                 mag  {:5.2f}                                             [0.02/86]? Uncertainty in iMAG  3234\n       g-i float32                 mag  {:5.2f}                                     [-1.97/3.8]? Corrected (g-i) color (2)  3234\n     e_g-i float32                 mag  {:6.2f}                                           [0.03/220.3]? Uncertainty in g-i  3234\n    logMsT float32        log(solMass)  {:5.2f}                           [4.49/12.45]? log Taylor method stellar mass (3)  3234\n  e_logMsT float32        log(solMass)  {:6.2f}                                          [0.02/158]? Uncertainty in logMsT  3234\n    logMsM float32        log(solMass)  {:5.2f}                           [4.4/11.94]? log McGaugh method stellar mass (3)  2203\n  e_logMsM float32        log(solMass)  {:5.2f}                                         [0.01/1.66]? Uncertainty in logMsM  2203\n    logMsG float32        log(solMass)  {:5.2f}                             [7.37/11.69]? log GSWLC-2 derived stellar mass 16776\n  e_logMsG float32        log(solMass)  {:5.2f}                                            [0/0.25]? Uncertainty in logMsG 16776\n  logSFR22 float32 log(solMass.yr**-1)  {:5.2f} [-5.57/2.17]? log Star Formatation Rate from 22{mu}m unWISE photometry (4)  7606\ne_logSFR22 float32 log(solMass.yr**-1)  {:5.2f}                                        [0.01/3.6]? Uncertainty in logSFR22  7606\n   logSFRN float32 log(solMass.yr**-1)  {:5.2f}      [-3.53/7.88]? log Star Formatation Rate from GALEX NUV photometry (4) 15387\n e_logSFRN float32 log(solMass.yr**-1)  {:5.2f}                                       [0.02/28.03]? Uncertainty in logSFRN 15388\n   logSFRG float32 log(solMass.yr**-1)  {:5.2f}                       [-2.94/1.47]? log Star Formatation Rate from GSWLC-2 16776\n e_logSFRG float32 log(solMass.yr**-1)  {:5.2f}                                         [0.0/1.08]? Uncertainty in logSFRG 16776\n    logMHI float32        log(solMass)  {:5.2f}                       [3.76/10.94] Haynes+, 2018, J/ApJ/861/49 log HI mass     0\n  e_logMHI float32        log(solMass)  {:5.2f}                                           [0.0/1.12] Uncertainty in logMHI     0\n\n\nWARNING: MergeConflictWarning: Cannot merge meta key 'ID' types &lt;class 'str'&gt; and &lt;class 'str'&gt;, choosing ID='J_AJ_160_271_table2' [astropy.utils.metadata.merge]\nWARNING: MergeConflictWarning: Cannot merge meta key 'name' types &lt;class 'str'&gt; and &lt;class 'str'&gt;, choosing name='J/AJ/160/271/table2' [astropy.utils.metadata.merge]\nWARNING: MergeConflictWarning: Cannot merge meta key 'description' types &lt;class 'str'&gt; and &lt;class 'str'&gt;, choosing description='Derived properties of cross-listed objects in the ALFALFA-SDSS catalog' [astropy.utils.metadata.merge]\n\n\n\n# convert degrees to radians for aitoff plot\nra_rad = np.deg2rad(data[\"RAJ2000\"])\ndec_rad = np.deg2rad(data[\"DEJ2000\"])\nwrapped_ra = np.arctan2(np.sin(ra_rad), np.cos(ra_rad))\nwrapped_dec = np.arctan2(np.sin(dec_rad), np.cos(dec_rad))\ndata[\"WRAPPED_RA\"] = wrapped_ra\ndata[\"WRAPPED_DE\"] = wrapped_dec \n\n# aitoff plot\nplt.figure(figsize=(8,6))\nplt.suptitle(\"ALFALFA-SDSS Sources\")\nplt.subplot(211, projection=\"aitoff\")\nplt.scatter(data[\"WRAPPED_RA\"], data[\"WRAPPED_DE\"], marker=\".\", s=1, alpha=0.1)\nplt.xlabel(\"RA [deg]\")\nplt.ylabel(\"Dec [deg]\")\nplt.grid(True)\n\n# unprojected plot\nplt.subplot(212)\nplt.scatter(data[\"RAJ2000\"], data[\"DEJ2000\"], marker=\".\", s=1, alpha=0.25)\nplt.xlabel(\"RA [deg]\")\nplt.ylabel(\"Dec [deg]\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# zeropoint plot\ndata.sort(\"DEJ2000\")\n\n# convert spherical coordinates to cartesian\ndata[\"px\"] = data[\"Dist\"] * np.cos(data[\"WRAPPED_RA\"]) * np.cos(data[\"WRAPPED_DE\"])\ndata[\"py\"] = data[\"Dist\"] * np.sin(data[\"WRAPPED_RA\"]) * np.cos(data[\"WRAPPED_DE\"])\ndata[\"pz\"] = data[\"Dist\"] * np.sin(data[\"WRAPPED_DE\"])\ndata[\"size\"] = 1\n\nnum_plots = 4  # change number of plots here\ndec_chunks = np.array_split(data[\"DEJ2000\"], num_plots)  # split the data into chunks\nplt.figure(figsize=(num_plots*2, num_plots*2))  # matplotlib subplots\nfig = make_subplots(rows=num_plots // 2, cols=num_plots // 2, \n    specs=[[{'type': 'scatter3d'}, {'type': 'scatter3d'}],\n           [{'type': 'scatter3d'}, {'type': 'scatter3d'}]])\n\nrow = 1\ncol = 1\nfor chunk, i in zip(dec_chunks, range(num_plots)):\n    # join chunk with rest of catalog to get all columns\n    chunk = Table(data = {\"DEJ2000\": chunk})    \n    merged = join(chunk, data, keys=\"DEJ2000\")\n    ascii.write(merged, f\"dec{np.abs(np.min(merged[\"DEJ2000\"])):.0f}to{np.max(merged[\"DEJ2000\"]):.0f}.dat\", format=\"commented_header\", overwrite=True)\n    # matplotlib version\n    plt.subplot(num_plots // 2, num_plots // 2, i + 1, projection=\"polar\")\n    plt.scatter(merged[\"WRAPPED_RA\"], merged[\"Dist\"], marker='.', s=1, alpha=0.5)\n    plt.xlabel(f\"{np.min(merged[\"DEJ2000\"]):.2f} &lt; Dec &lt; {np.max(merged[\"DEJ2000\"]):.2f}\")\n\n    # plotly version\n    px_merged = merged.to_pandas()\n    print(row, col)\n    fig.add_scatter3d(x=px_merged[\"px\"], y=px_merged[\"py\"], z=px_merged[\"pz\"], row=row, col=col, mode=\"markers\", marker=dict(size=px_merged[\"logMHI\"], color=px_merged[\"imag\"]))\n\n    if col &gt;= num_plots // 2:\n        row += 1  # move to next row\n        col = 1  # restart the column\n    else:\n        col += 1    \n\n\nplt.suptitle(f\"Redshift Plots: {num_plots} Splits\")\nplt.tight_layout()\nfig.update_layout(margin=dict(l=20, r=20, t=20, b=20),\n    scene=dict(aspectmode=\"cube\"))\n# fig.show(renderer=\"browser\")\n\n1 1\n1 2\n2 1\n2 2\n\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n\n\n\n\n\n\n\n\ndata.info()\n\n&lt;Table length=31501&gt;\n   name     dtype          unit         format                                 description                                    class     n_bad\n---------- ------- ------------------- -------- -------------------------------------------------------------------------- ------------ -----\n       AGC   int32                                                                       [1/749512] AGC catalog identifier MaskedColumn     0\n      Flag   uint8                                                                               [0/3] Photometry flag (1) MaskedColumn     0\n     ObjID   int64                                                         Optical counterpart SDSS DR15 object identifier MaskedColumn  1863\n   RAJ2000 float64                 deg {:10.6f}                                    [0.003/360] Right Ascension (J2000) (2) MaskedColumn     0\n   DEJ2000 float64                 deg  {:9.5f}                                       [-0.21/36.4] Declination (J2000) (2) MaskedColumn     0\n      RVel   int16              km / s                       [-430/17823] HI profile midpoint heliocentric radial velocity MaskedColumn     0\n      Dist float32                 Mpc  {:5.1f}                        [0.3/260] Distance from Haynes+, 2018, J/ApJ/861/49 MaskedColumn     0\n    e_Dist float32                 Mpc  {:4.1f}                                               [0/30.9] Uncertainty in Dist MaskedColumn     0\n      gext float32                 mag  {:5.2f}             [0.02/4.44]? Foreground Galactic extinction in SDSS g band (3) MaskedColumn  1863\n      iext float32                 mag  {:5.2f}             [0.01/2.28]? Foreground Galactic extinction in SDSS i band (3) MaskedColumn  1863\n       b/a float32                      {:5.2f}                                 [0.05/1]? Axial ratio b/a from SDSS r band MaskedColumn  1863\n     e_b/a float32                      {:6.2f}                                         [0.01/865]? Uncertainty in b/a (4) MaskedColumn  1863\n      imag float32                 mag  {:5.2f}                                 [9.58/30.89]? SDSS i band cmodel magnitude MaskedColumn  1863\n    e_imag float64                 mag  {:8.2f}                                   [0.01/42134.68]? Uncertainty in imag (4) MaskedColumn  1863\n     Dprop    str5                                                                            Derived properties (table 2) MaskedColumn     0\n     Sloan    str5                                                     Display the online SDSS DR16 for the nearest object MaskedColumn     0\n        Ag float32                 mag  {:5.2f}                            [0/1.91]? g band internal extinction factor (1) MaskedColumn  3234\n        Ai float32                 mag  {:5.2f}                            [0/0.99]? i band internal extinction factor (1) MaskedColumn  3234\n      iMAG float32                 mag  {:6.2f}                         [-24/-10]? Corrected absolute i band magnitude (2) MaskedColumn  3234\n    e_iMAG float32                 mag  {:5.2f}                                             [0.02/86]? Uncertainty in iMAG MaskedColumn  3234\n       g-i float32                 mag  {:5.2f}                                     [-1.97/3.8]? Corrected (g-i) color (2) MaskedColumn  3234\n     e_g-i float32                 mag  {:6.2f}                                           [0.03/220.3]? Uncertainty in g-i MaskedColumn  3234\n    logMsT float32        log(solMass)  {:5.2f}                           [4.49/12.45]? log Taylor method stellar mass (3) MaskedColumn  3234\n  e_logMsT float32        log(solMass)  {:6.2f}                                          [0.02/158]? Uncertainty in logMsT MaskedColumn  3234\n    logMsM float32        log(solMass)  {:5.2f}                           [4.4/11.94]? log McGaugh method stellar mass (3) MaskedColumn  2203\n  e_logMsM float32        log(solMass)  {:5.2f}                                         [0.01/1.66]? Uncertainty in logMsM MaskedColumn  2203\n    logMsG float32        log(solMass)  {:5.2f}                             [7.37/11.69]? log GSWLC-2 derived stellar mass MaskedColumn 16776\n  e_logMsG float32        log(solMass)  {:5.2f}                                            [0/0.25]? Uncertainty in logMsG MaskedColumn 16776\n  logSFR22 float32 log(solMass.yr**-1)  {:5.2f} [-5.57/2.17]? log Star Formatation Rate from 22{mu}m unWISE photometry (4) MaskedColumn  7606\ne_logSFR22 float32 log(solMass.yr**-1)  {:5.2f}                                        [0.01/3.6]? Uncertainty in logSFR22 MaskedColumn  7606\n   logSFRN float32 log(solMass.yr**-1)  {:5.2f}      [-3.53/7.88]? log Star Formatation Rate from GALEX NUV photometry (4) MaskedColumn 15387\n e_logSFRN float32 log(solMass.yr**-1)  {:5.2f}                                       [0.02/28.03]? Uncertainty in logSFRN MaskedColumn 15388\n   logSFRG float32 log(solMass.yr**-1)  {:5.2f}                       [-2.94/1.47]? log Star Formatation Rate from GSWLC-2 MaskedColumn 16776\n e_logSFRG float32 log(solMass.yr**-1)  {:5.2f}                                         [0.0/1.08]? Uncertainty in logSFRG MaskedColumn 16776\n    logMHI float32        log(solMass)  {:5.2f}                       [3.76/10.94] Haynes+, 2018, J/ApJ/861/49 log HI mass MaskedColumn     0\n  e_logMHI float32        log(solMass)  {:5.2f}                                           [0.0/1.12] Uncertainty in logMHI MaskedColumn     0\nWRAPPED_RA float64                 deg {:10.6f}                                    [0.003/360] Right Ascension (J2000) (2) MaskedColumn     0\nWRAPPED_DE float64                 deg  {:9.5f}                                       [-0.21/36.4] Declination (J2000) (2) MaskedColumn     0\n        px float64                 Mpc  {:5.1f}                        [0.3/260] Distance from Haynes+, 2018, J/ApJ/861/49 MaskedColumn     0\n        py float64                 Mpc  {:5.1f}                        [0.3/260] Distance from Haynes+, 2018, J/ApJ/861/49 MaskedColumn     0\n        pz float64                 Mpc  {:5.1f}                        [0.3/260] Distance from Haynes+, 2018, J/ApJ/861/49 MaskedColumn     0\n      size   int32                                                                                                               Column     0\n\n\n\n# plotly interactive 3d map\nfig = px.scatter_3d(data.to_pandas(), \"px\", \"py\", \"pz\", color=\"imag\", size=\"logMHI\", size_max=6, hover_name=\"AGC\", hover_data=[\"RAJ2000\", \"DEJ2000\", \"Dist\"])\nfig.add_scatter3d(x=[0], y=[0], z=[0], name=\"Milky Way\", marker=dict(size=0.015, color='blue'))\n# fig.show(renderer=\"browser\")\n\n                            \n                                            \n\n\n\ndef inset_position_plot(bg_data, ra_bounds, dec_bounds, x0, y0, width, height, loc1, loc2, ax):\n    minra, maxra = ra_bounds\n    mindec, maxdec = dec_bounds\n    mask = (bg_data[\"RAJ2000\"] &gt; minra) & (bg_data[\"RAJ2000\"] &lt; maxra) & (bg_data[\"DEJ2000\"] &gt; mindec) & (bg_data[\"DEJ2000\"] &lt; maxdec)\n    inset_data = data[mask]\n    # background plot\n    ax.scatter(bg_data[\"RAJ2000\"], bg_data[\"DEJ2000\"], marker=\".\", s=1, alpha=0.5)\n    ax.set_xlabel(\"RA [deg]\")\n    ax.set_ylabel(\"Dec [deg]\")\n\n    # create inset\n    ax_inset = ax.inset_axes([x0, y0, width, height])\n    ax_inset.scatter(inset_data[\"RAJ2000\"], inset_data[\"DEJ2000\"], c=inset_data[\"Dist\"], s=1)\n    ax_inset.set_xticks([])\n    ax_inset.set_yticks([])\n    # ax_inset.set_colorbar()\n    mark_inset(ax, ax_inset, loc1=loc1, loc2=loc2, fc=\"none\", ec=\"0.5\")\n\n    # ax.set_title(\"Position Plot with Inset Colored by Distance\")\n\n    return\n\n\nfig, axs = plt.subplots(3, 3, figsize=(14,14))\ninset_position_plot(data, (0, 50), (15, 90), 0.3, 0.3, 0.65, 0.65, 2, 3, axs[0][0])\ninset_position_plot(data, (0, 50), (0, 20), 0.4, 0.4, 0.55, 0.55, 2, 3, axs[0][1])\ninset_position_plot(data, (100, 150), (22, 33), 0.2, 0.01, 0.55, 0.55, 1, 2, axs[0][2])\ninset_position_plot(data, (175, 250), (20, 90), 0.2, 0.01, 0.55, 0.4, 1, 2, axs[1][0])\ninset_position_plot(data, (100, 175), (0, 14), 0.2, 0.45, 0.55, 0.50, 3, 4, axs[1][1])\ninset_position_plot(data, (125, 200), (0, 14), 0.2, 0.45, 0.55, 0.50, 3, 4, axs[1][2])\ninset_position_plot(data, (175, 250), (0, 14), 0.2, 0.45, 0.55, 0.50, 3, 4, axs[2][0])\ninset_position_plot(data, (320, 360), (15, 90), 0.2, 0.01, 0.55, 0.4, 1, 2, axs[2][1])\ninset_position_plot(data, (320, 360), (0, 20), 0.2, 0.01, 0.55, 0.4, 1, 2, axs[2][2])\n# inset_position_plot(data, (320, 360), (0, 17), 0.2, 0.01, 0.55, 0.4, 1, 2)  # bottom right\nplt.suptitle(\"Possible Structures in ALFALFA-SDSS, Colored by Distance\")\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nInteractive Structure Plot\n\n# # plotly interactive 3d map\n# fig = px.scatter(data.to_pandas(), \"RAJ2000\", \"DEJ2000\", size_max=1, opacity=0.25, hover_name=\"AGC\", hover_data=[\"RAJ2000\", \"DEJ2000\", \"Dist\"], color_continuous_scale=px.colors.sequential.Blues)\n# fig.show()\n\n# # plotly interactive 3d map\n# fig = px.scatter(data.to_pandas(), \"RAJ2000\", \"DEJ2000\", color=\"Dist\", size_max=1, opacity=0.25, hover_name=\"AGC\", hover_data=[\"RAJ2000\", \"DEJ2000\", \"Dist\"], color_continuous_scale=px.colors.sequential.Viridis)\n# fig.show()\n\n\n\n# Create a figure with subplots, linking the x and y axes\nfig = make_subplots(\n    rows=1,\n    cols=2,\n    subplot_titles=('Plain', 'Colored by Distance'),\n    shared_xaxes=True,\n    shared_yaxes=True\n)\n\n# Add the first scatter plot to the first subplot\nfig.add_trace(\n    go.Scatter(x=data[\"RAJ2000\"], y=data[\"DEJ2000\"], mode='markers', name='Data 1', marker=dict(\n            colorscale='Blues',\n            size=5,\n            opacity=0.25\n        )),\n    row=1,\n    col=1\n)\n\n# Add the second scatter plot to the second subplot\nfig.add_trace(\n    go.Scatter(x=data[\"RAJ2000\"], y=data[\"DEJ2000\"], mode='markers', name='Data 2', marker=dict(\n            color=data[\"Dist\"],\n            colorscale='Viridis',\n            size=5,\n            opacity=0.25\n        )),\n    row=1,\n    col=2\n)\n\n# Update layout for better appearance\nfig.update_layout(\n    showlegend=False\n)\n\nfig.show()\n\n                            \n                                            \n\n\n\n\n3D DisPerSE Output\n\n# import pandas as pd\n# import matplotlib.pyplot as plt\n# import plotly.graph_objects as go\n\n# df = pd.read_csv(\"simu_32_id.gad.NDnet_s3.52.up.NDskl.S010.a.segs\", sep=\" \", skiprows=[0,2]).rename(columns={\"#U0\": \"U0\"})\n\n\n# u_list = df[[\"U0\", \"U1\", \"U2\"]].to_numpy()\n# v_list = df[[\"V0\", \"V1\", \"V2\"]].to_numpy()\n\n# fig = go.Figure()\n# for start, end in zip(u_list, v_list):\n#     fig.add_trace(go.Scatter3d(\n#         x=[start[0], end[0]], \n#         y=[start[1], end[1]], \n#         z=[start[2], end[2]],\n#         mode='lines'\n#     ))\n\n# fig.show()\n# # fig.show(renderer=\"browser\")\n\n\n\n2D DisPerSE Input\n\n# import pandas as pd\n# import matplotlib.pyplot as plt\n# import numpy as np\n# import astropy.units as u\n# from astropy.coordinates import Angle\n\n# # circle parameters\n# center_x = 0\n# center_y = 0\n# radius = 10\n# num_points = 200 \n\n# noise = np.random.uniform(0, 1, num_points)\n# theta = np.linspace(0, 2 * np.pi, num_points)\n# x = center_x + radius * np.cos(theta)\n# y = center_y + radius * np.sin(theta)\n\n# ra = x + noise\n# dec = y + noise\n\n# plt.scatter(ra, dec)  # angle, distance\n\n# df = pd.DataFrame({\"ra\": ra, \"dec\": dec})  # convert to degrees\n# df.to_csv(\"circle_test.segs\", sep=\" \", index=False, header=\"# ra dec z\")\n\n\n\n2D DisPerSE Output\n\n# import pandas as pd\n# import matplotlib.pyplot as plt\n# import plotly.graph_objects as go\n# import matplotlib.image as mpimg\n\n\n# df = pd.read_csv(\"circle_test_nonperiodic.segs\", sep=\" \", skiprows=[0,2]).rename(columns={\"#U0\": \"U0\"})\n\n# plt.figure(figsize=(12,6))\n# plt.subplot(121)\n# for i in range(len(df)):\n#     plt.plot([df[\"U0\"].iloc[i], df[\"V0\"].iloc[i]], [df[\"U1\"].iloc[i], df[\"V1\"].iloc[i]])\n\n# plt.subplot(122)\n# circle = pd.read_csv(\"test.survey_ascii\", sep=\"\\t\")\n# print(circle)\n# plt.scatter(circle[\"ra\"], circle[\"dec\"])\n\n# plt.tight_layout()\n# plt.show()"
  },
  {
    "objectID": "courses/ML/index.html",
    "href": "courses/ML/index.html",
    "title": "Machine Learning Notes - Adapted from Macalester’s STAT 253 SP25 Course by Brianna Heggeseth",
    "section": "",
    "text": "Machine Learning: using algorithms that can learn from existing patterns and make predictions\n\n\n\nSupervised Learning\n\nmodel relationship between input \\(x = (x_1, x_2, \\cdots, x_p)\\) and output \\(y\\).\n\n\\[\\begin{align*}y &= f(x) + \\epsilon \\\\ &= \\text{ trend in relationship } + \\text{ residual deviation from trend }\\end{align*}\\]\n\nTypes:\n\nRegression (\\(y\\) is quantitative)\nClassification (\\(y\\) is categorical)\n\n\nUnsupervised Learning\n\nNo output variable\nGoal is to use \\(x\\) to understand/modify strucutre of data with respect to \\(x\\)\nTypes:\n\nClustering (similar \\(x\\)’s’)\nDimension Reduction (turn original set of \\(p\\) input varaibles into set with \\(k&lt;p\\) variables)"
  },
  {
    "objectID": "courses/ML/index.html#supervised-vs-unsupervised-learning",
    "href": "courses/ML/index.html#supervised-vs-unsupervised-learning",
    "title": "Machine Learning Notes - Adapted from Macalester’s STAT 253 SP25 Course by Brianna Heggeseth",
    "section": "",
    "text": "Supervised Learning\n\nmodel relationship between input \\(x = (x_1, x_2, \\cdots, x_p)\\) and output \\(y\\).\n\n\\[\\begin{align*}y &= f(x) + \\epsilon \\\\ &= \\text{ trend in relationship } + \\text{ residual deviation from trend }\\end{align*}\\]\n\nTypes:\n\nRegression (\\(y\\) is quantitative)\nClassification (\\(y\\) is categorical)\n\n\nUnsupervised Learning\n\nNo output variable\nGoal is to use \\(x\\) to understand/modify strucutre of data with respect to \\(x\\)\nTypes:\n\nClustering (similar \\(x\\)’s’)\nDimension Reduction (turn original set of \\(p\\) input varaibles into set with \\(k&lt;p\\) variables)"
  },
  {
    "objectID": "courses/ML/index.html#model-evaluation",
    "href": "courses/ML/index.html#model-evaluation",
    "title": "Machine Learning Notes - Adapted from Macalester’s STAT 253 SP25 Course by Brianna Heggeseth",
    "section": "Model Evaluation",
    "text": "Model Evaluation\n\nModel Evaluation\nOnce we have the model, we need to evaluate:\n\nIs the model wrong?\n\neg. Linear fit on quadratic data\nLook at residual plot\n\nIs the model strong?\n\nHow well does our model explain the variability in response?\nLook at \\(R^2\\), adjusted \\(R^2\\)\n\nDoes the model produce accurate predictions?\n\nLook at residual plot\nMSE, RMSE, MAE\n\nIs the model fair?\n\nWho collected/funded the data?\nHow/why did they collect it?\nImplications of analysis\n\n\n\n\nOverfitting\nAdding more predictors can result in an overfit model. Relying on in-sample metrics can lead to overfitting\n\nIn-sample metrics only tell you how the model performs on that dataset used to make the model.\nBiased!\nOverly optimisitc!\n\nPrevention\n\nTraining vs testing groups\nk-Fold Cross Validation\n\n\n\nCross-Validation\nAlgorithm:\n\nSplit data into \\(k\\) folds\nFor each fold:\n\nRemove the fold from the data set, this will be the testing set\nUse the rest of the data to train the model\nCalculate the error between the testing and training model\n\nAverage all error estimates from each fold\n\nThis is a more accurate value for what you expect your model to be off by when tested on new data.\nCommon values of \\(k\\): 7, 10\nNote: \\(k=n\\) is also known as Leave One Out Cross Validation (LOOCV)"
  },
  {
    "objectID": "courses/ML/index.html#model-selection",
    "href": "courses/ML/index.html#model-selection",
    "title": "Machine Learning Notes - Adapted from Macalester’s STAT 253 SP25 Course by Brianna Heggeseth",
    "section": "Model Selection",
    "text": "Model Selection\nHow can we decide what predictors to include in our model?\nMethods:\n\nVariable selection Identify a subset of predictors\n\nBest subset selection\nBackward-stepwise selection\nForward-stepwise selection\n\nShrinkage / regularization Shrink/regularize the coefficients of all predictors towards 0.\n\nLASSO, ridge regression, elastic net\n\nDimension Reduction Combine predictors into a smaller set of new predictors\n\nPrincipal componets regression\n\n\n\nModel Selection\nGreedy algorithms:\n\nBest Subset Selection Build all \\(2^p\\) models that use any combination of available predictors and identify best model based on chosen metric\n\nComputationally expensive\n\nBackward Stepwise Selection Build a model with all \\(p\\) predictors. Remove the predictor with largest p-value and build a model with the remaining. Repeat till you have \\(p\\) models. Identify best to some metric.\n\nGreedy\nOverestimates significance of remaining predictors\n\nForward Stepwise Selection Better than Best Subset Selection, but worse than Backwards Stepwise Selection.\n\nComputationally expensive\nCan produce odd combination of predictors\n\n\n\n\nLASSO: Shrinkage / Regularization\nLeast squares that penalizes predictors that add complexity.\nPros:\n\nhelps with model selection, preventing overfitting\nPredictors must be scaled (R does it for us)\nIsn’t greedy and doesn’t choose variables based on p-values\n\nTuning:\n\n\\(\\lambda = 0\\), LASSO = Least Squares\nAs \\(\\lambda\\) increases, variable coefficients go to zero\n\n```{r}\n#| message: false\n#| warning: false\n#| eval: false\n\n# model specificiation\nmodel_spec &lt;- linear_reg() %&gt;%\n  set_mode(\"regression\") %&gt;%\n  set_engine(engine = \"glmnet\") %&gt;%\n  set_args(mixture = 1, penalty = tune())\n\n# model recipe\ndata_recipe &lt;- recipe(y ~ ., data = ___) %&gt;%\n  step_dummy(all_nominal_predictors())  # normalization done by glmnet\n\n# model workflow\nmodel_wf &lt;- workflow() %&gt;%\n  add_recipe(data_recipe) %&gt;%\n  add_model(model_spec)\n\n# cross validation\nset.seed(___)\nmultiple_models &lt;- model_wf %&gt;%\n  tune_grid(\n    grid = grid_regular(\n      penalty(range = c(___, ___)),  # range is on log10 scale     \n      levels = ___  # how many models to build\n    ),\n    resamples = vfold_cv(___, v = ___),\n    metrics = metric_set(mae)\n)\n\n# model selection\nmultiple_models %&gt;%\n  collect_metrics()\n\nautoplot(multiple_models)\n\nbest_param &lt;- multiple_models %&gt;%\n  select_best(metric = \"mae\")\n\nparsimonious_param &lt;- multiple_models %&gt;%   \n  select_by_one_std_err(metric = \"mae\", \n                        desc(penalty))\n\n# final model\nfinal_model &lt;- model_wf %&gt;%\n  finalize_workflow(parameters = parimonious_param) %&gt;%\n  fit(data = ___)\n```"
  },
  {
    "objectID": "courses/ML/index.html#flexible-models",
    "href": "courses/ML/index.html#flexible-models",
    "title": "Machine Learning Notes - Adapted from Macalester’s STAT 253 SP25 Course by Brianna Heggeseth",
    "section": "Flexible Models",
    "text": "Flexible Models\nWhat if the relationship between \\(x\\) and \\(y\\) is more complicated? Use flexible models!\nParametric vs Nonparametric Models\n\nParametric Assumes relationship across domain of function\nNonparametric More flexibility in relationship between \\(x\\) and \\(y\\)\n\n\nNonparametric Models\nNeed to consider the scale of variables when calculating distances between observations with more than one predictor (NORMALIZE!)\n\nNormalization - an example of pre-processing, decisions wherein effect models and predictons\nMight need to preprocess data in variable recipe\n\n\n\nKNN Regression and the Bias-Variance Tradeoff\n\nNonparametric\nAssumption: Similar \\(x\\) values imply similar \\(y\\) values.\nfrom kknn package: knn predicts y from weighted average\n\nmore influence to closer neighbors\n\n\nAlgorithm:\n\nIdentify \\(K\\) nearest neighbors of \\(x\\) with respect to Euclidean distance\nObserve the \\(y\\) values of these neighbors\n\\(f(x) = \\text{average}(y \\text{ neighbors})\\)\n\n```{r}\n#| message: false\n#| warning: false\n#| eval: false\n\n# model specificiation\nmodel_spec &lt;- nearest_neighbors() %&gt;%\n  set_mode(\"regression\") %&gt;%\n  set_engine(engine = \"kknn\") %&gt;%\n  set_args(neighbors = tune())\n\n# model recipe\ndata_recipe &lt;- recipe(y ~ ., data = ___) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_normalization(all_numeric_predictors())\n\n# model workflow\nmodel_wf &lt;- workflow() %&gt;%\n  add_recipe(data_recipe) %&gt;%\n  add_model(model_spec)\n\n# cross validation\nset.seed(___)\nknn_models &lt;- model_wf %&gt;%\n  tune_grid(\n    grid = grid_regular(\n      neighbors(range = c(1, ___)),     \n      levels = ___  # how many models to build\n    ),\n    resamples = vfold_cv(___, v = ___),\n    metrics = metric_set(mae)\n)\n\n# model selection\nknn_models %&gt;%\n  collect_metrics()\n\nautoplot(knn_models)\n\nbest_knn &lt;- select_best(knn_models, metric = \"mae\")  # parsimony is irrelevant\n\n# final model\nfinal_model &lt;- model_wf %&gt;%\n  finalize_workflow(parameters = best_knn) %&gt;%\n  fit(data = ___)\n```\nBias-Variance Tradeoff\n\nBias How well does the model predict values?\n\nExtremes: \\(K=1\\), overfit, no bias; \\(K=K\\) just a line but smoothed due to weights\n\nVariance How does the model shape change from dataset to dataset?\n\nOverfit model: passes through each point in the data, high variance\n\n\n\n\nLOESS & Splines\nSplines\n\nFit polynomial models in small localized regions and make the boundaries smooth.\nCan only spline one quantitative predictor\nNatural vs basis\n\nNatural: pass deg_free argument where deg_free = # of knots + 1\n\nVariant of B-splines with additional constraint at boundaries to reduce variance\nKnots typically based on quantiles of predictor\n\nBasis: specify specific knots options = list(knots = c(x_1, x_2))\n\nYou choose the knots\nFunctions can be unstable at boundaries\n\n\n\n```{r}\n#| message: false\n#| warning: false\n#| eval: false\n\n# model specificiation\nmodel_spec &lt;- linear_reg() %&gt;%\n  set_mode(\"regression\") %&gt;%\n  set_engine(engine = \"lm\")\n\n# model recipe\ndata_recipe &lt;- recipe(y ~ ., data = ___) %&gt;%\n  step_ns(x, deg_free = ___)  # natural splines, deg_free = # knots + 1\n  step_bs(x, options = list(knots = c(___, ___)))  # basis splines, specific locations of knots\n\n# model workflow\nmodel_wf &lt;- workflow() %&gt;%\n  add_recipe(data_recipe) %&gt;%\n  add_model(model_spec)\n\n# cross validation\nmultiple_models &lt;- model_wf %&gt;%\n  fit_resamples(\n    resamples = vfold_cv(___, v = ___),\n    metrics = metric_set(mae)\n  )\n\n# final model\nfinal_model &lt;- model_wf %&gt;%\n  fit(data = ___)\n```\nLOESS\n\nFit regression models in small, localized regions, where nearby data have greater influence than far data\nCan only LOESS one quantitative predictor\nNonparametric\nAlgorithm:\n\nDefine the span \\(h \\in (0,1]\\). Perform the following to estimate \\(f(x)\\) at each possible predictor value \\(x\\)\n\nIdentify a neighborhood consisting of the \\(100\\times h\\%\\) of cases that are closes to \\(x\\)\nPutting more weight on closer \\(x\\) neighbors, fit a linear model in this neighborhood\nUse the local linear model to estimate \\(f(x)\\)"
  },
  {
    "objectID": "courses/ML/index.html#logistic-regression",
    "href": "courses/ML/index.html#logistic-regression",
    "title": "Machine Learning Notes - Adapted from Macalester’s STAT 253 SP25 Course by Brianna Heggeseth",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\nClassification to model binary categorical \\(y\\).\nInterpret in log(odds)\n\n\n\\[\\text{odds} = \\frac{p}{p-1},\\quad p = \\frac{\\text{odds}}{\\text{odds} + 1}\\]\n\n\n\n\nScale\nImpossible\n50/50\nCertain\n\n\n\n\np\n0\n0.5\n1\n\n\nodds\n0\n1\n\\(\\infty\\)\n\n\nlog(odds)\n-\\(\\infty\\)\n0\n\\(\\infty\\)\n\n\n\nIf we want to make more concrete predictions, we use the probability to make a classification through a classification rule. Can be based on:\n\nPredictor value\nProbability\nlog(odds)\n\nVisually, the classification rule partitions the data. If the data overlaps, our rule might not always be correct. We can create an in-smaple confusion matrix from our classification rule to calculate the following:\n\n\n\n\nTruth\n\n\n\n\n\nPrediction\nNo\nYes\n\n\nNo\nTN\nFN\n\n\nYes\nFP\nTP\n\n\n\nSensitivity (TP rate): \\(\\frac{TP}{TP + FN}\\)\nSpecificity (TN rate): \\(\\frac{TN}{TN + FP}\\)\nIf you want to improve sensitivity, lower the classification threshold. Note that sensitivity is inversely proportional to specificity, so increasing sensitivity will decrease specificity.\nTuning the classification rule: context\n\nConsequences of misclassification\nPrioritize high sensitivity to avoid FN / increase TP\nPrioritize high specificity to avoice FP / increase TN\n\nReciever Operating Characteristic (ROC) Curve:\n\nSensitivity vs 1 - Specificity (FPR)\n\nArea under the curve (AUC):\n\nProbability we correctly identified \\(y=0\\) and \\(y=1\\)\nMeasures overall effectiveness of classification model\nCloser to 1 the better (0.5 is when Sensitivity = 1 - Specificity)\n\n```{r}\n#| message: false\n#| warning: false\n#| eval: false\n\n# Ensure outcome is categorical\nsample_data &lt;- sample_data %&gt;% \n  mutate(y = as.factor(y))\n\n# relevel outcome if you want to look at different category\nsample_data &lt;- sample_data %&gt;%\n  mutate(y = relevel(y, ref = \"CATEGORY NOT INTERESTED IN\"))\n\n# Create logistic regression classification model specification \nmodel_spec &lt;- logistic_reg() %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(engine = \"glm\")\n\n# Make data recipe\ndata_rec &lt;- recipe(y ~ x, data = sample_data)\n\n# Create model workflow\nmodel_wf &lt;- workflow() %&gt;%\n  add_recipe(data_rec) %&gt;%\n  add_model(model_spec)\n\n# cross-validate with 10 folds\nmultiple_models &lt;- model_wf %&gt;%\n  fit_resamples(\n    resamples = vfold_cv(sample_data, v = 10),\n    control = control_resamples(save_pred = TRUE, entry_level = \"second\"),\n    metrics = metric_set(accuracy)  # could also specify sensitivity, specificity, roc_auc\n  )\n\n# finalize model\nfinal_model &lt;- model_wf %&gt;%\n  fit(data = sample_data)\n```"
  },
  {
    "objectID": "courses/ML/index.html#knn-trees",
    "href": "courses/ML/index.html#knn-trees",
    "title": "Machine Learning Notes - Adapted from Macalester’s STAT 253 SP25 Course by Brianna Heggeseth",
    "section": "KNN & Trees",
    "text": "KNN & Trees\nPro:\n\nNon-parametric classification\nFlexibility\nNo assumptions on shape of relationship\nGood with complicated relationships\n\nCon:\n\nLack of insights\nIgnores relationships that do exist\nComputationally intensive\n\n\nKNN\nGoal: build classification model of categorical response variable\nAlgorithm:\n\nID nearest neighbors w.r.t. \\(x\\) values\nGet \\(y\\) value of neighbors\nPredict the most common neighbor value\n\nPro:\n\nFlexible\nIntuitive\nMore than 2 outcome categories\n\nCon:\n\nLazy learner; must calculate distances at time of prediction for each point we classify\nComputationally intensive\nProvides classifications, but no real sense of relationships\n\n```{r}\n#| message: false\n#| warning: false\n#| eval: false\n\n# Ensure outcome is categorical\nsample_data &lt;- sample_data %&gt;% \n  mutate(y = as.factor(y))\n\n# Create KNN classification model specification \nmodel_spec &lt;- nearest_neighbor() %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(engine = \"kknn\") %&gt;%\n  set_args(neighbors = tune())\n\n# Make data recipe, convert all predictors to numbers and normalize\ndata_rec &lt;- recipe(y ~ x, data = sample_data) %&gt;%\n  step_dummy(all_nomial_predictors()) %&gt;%\n  step_normalize(all_numeric_predictors())\n\n# Create model workflow\nmodel_wf &lt;- workflow() %&gt;%\n  add_recipe(data_rec) %&gt;%\n  add_model(model_spec)\n\n# Cross-validate\nmultiple_models &lt;- model_wf %&gt;%\n  tune_grid(\n    grid = grid_regular(\n      neighbors(range = c(1, 7)),\n      levels = 10\n    ),\n    resamples = vfold_cv(sample_data, 10),\n    metrics = metric_set(accuracy)\n  )\n\n# plot the models\nmultiple_models %&gt;%\n  autoplot()\n\n# select best parameter\n# NOTE: parsimony is irrelevant\nbest_param &lt;- multiple_models %&gt;%\n  select_best(metric = \"accuracy\")\n\n# finalize model\nfinal_model &lt;- model_wf %&gt;%\n  finalize_workflow(best_param)\n  fit(data = sample_data)\n```\n\n\nClassification Trees\nPredict outcome from a series of yes/no questions.\nRecursive binary splitting algorithm:\n\nStart with all the data\nCreate the best binary split\n\n\nMinimizing Gini index (\\(G = p_1(1 - p_1) + \\dots\\)) -\nMaximizing node homogeneity\n\n\nRepeat 2 until (pruning steps = tuning parameters)\n\n\nReach max depth (# of splits from root to leaf)\nMin sample size in each node\nCost complexity \\(\\alpha\\) (like \\(\\lambda\\) for lasso) a. \\(\\alpha = 0\\) no penalty b. \\(\\alpha \\gg\\) only make split if very useful\n\nPro:\n\nMore insight into relationship\nEager learners - can use a tree to classify all new points\n\nCon:\n\nGreedy\nComputationally intesive\n\n\nNote: KNN and Trees are equal when \\(\\alpha = \\infty\\) and \\(K = N\\)\n\n```{r}\n#| message: false\n#| warning: false\n#| eval: false\n\n# Ensure outcome is categorical\nsample_data &lt;- sample_data %&gt;% \n  mutate(y = as.factor(y))\n\n# Create classification tree classification model specification \nmodel_spec &lt;- decision_tree() %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(engine = \"rpart\") %&gt;%\n  set_args(min_n = 2, tree_depth = 30, cost_complexity = tune())\n\n# Make data recipe\n# NOTE: preprocessing not absolutely necessary\ndata_rec &lt;- recipe(y ~ x, data = sample_data)\n\n# Create model workflow\nmodel_wf &lt;- workflow() %&gt;%\n  add_recipe(data_rec) %&gt;%\n  add_model(model_spec)\n\n# Cross-validate\nmultiple_models &lt;- model_wf %&gt;%\n  tune_grid(\n    grid = grid_regular(\n      cost_complexity(range = c(-5, 2)),  # log10 scale\n      levels = 10\n    ),\n    resamples = vfold_cv(sample_data, 10),\n    metrics = metric_set(accuracy)\n  )\n\n# plot the models\nmultiple_models %&gt;%\n  autoplot()\n\n# select best parameter\nbest_param &lt;- multiple_models %&gt;%\n  select_best(metric = \"accuracy\")\n\n# select parsimonious parameter\nparsimonious_param &lt;- multiple_models %&gt;%\n  select_by_one_std_err(metric = \"accuracy\", desc(cost_complexity))\n\n# finalize model\nfinal_model &lt;- model_wf %&gt;%\n  finalize_workflow(parsimonious_param)\n  fit(data = sample_data)\n```"
  },
  {
    "objectID": "courses/ML/index.html#bagging-and-random-forests",
    "href": "courses/ML/index.html#bagging-and-random-forests",
    "title": "Machine Learning Notes - Adapted from Macalester’s STAT 253 SP25 Course by Brianna Heggeseth",
    "section": "Bagging and Random Forests",
    "text": "Bagging and Random Forests\nIdea:\n\nBuild a bunch of trees with high variance and low bias (unpruned)\nAverage results to get 1 tree\n\nBoth bagging and random forests are examples of ensemble methods: combining outputs of multiple ML algorithms\n\nIn general: low variability; more stable predictions\n\n\nBagging\nBootstrap aggregation (bagging):\n\nEach tree is created from a resampling with replacement\n~2/3 of original data will show up in resample, ~1/3 will not\nAll \\(p\\) predictors are considered for each split\nUse resample to train, and remaining to test (out of bag error)\n\nForest = many unpruned tress from slighly different datasets (bootsrapping/resampling)\nOverall prediction is from the combined prediction across all trees\n\nClassification: Majority rules\nRegression: Averages\n\n```{r}\n#| message: false\n#| warning: false\n#| eval: false\n\n# Ensure outcome is categorical\nsample_data &lt;- sample_data %&gt;% \n  mutate(y = as.factor(y))\n\n# Create bagging classification model specification \nmodel_spec &lt;- rand_forest() %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(engine = \"ranger\") %&gt;%\n  set_args(\n    mtry = NULL, \n    min_n = 2, \n    trees = 500, \n    probability = FALSE, \n    importance = \"impurity\")\n\n# Make data recipe, preprocessing steps aren't absolutly necessary\ndata_rec &lt;- recipe(y ~ x, data = sample_data)\n\n# Create model workflow\nmodel_wf &lt;- workflow() %&gt;%\n  add_recipe(data_rec) %&gt;%\n  add_model(model_spec)\n\n# NOTE: no tuning needed - OOB estimation instead\n\n# finalize model\nfinal_model &lt;- model_wf %&gt;%\n  fit(data = sample_data)\n```\n\n\nRandom Forest\nAt each split in the tree, randomly select and consider only a subset of predictors\n\nTypical values are \\(p/2\\) and \\(\\sqrt{p}\\)\n\nPros:\n\nLow bias, low variance (maintine bias from trees, but reduce variance)\nLess computationally intesive (only consider a subset of predictors) compared to bagging\nLess greedy (surrogates get a chance to shine)\nDecorrelate trees (random subset of predictors)\n\nCons:\n\nComputationally expensive\nCan’t plot results, lose interpretability\n\n\nNote: A forest is the best algorithm for reducing variance.\n\n```{r}\n# The only thing you have to change for random forest from bagging code is model specification\n# Create random forest classification model specification \nmodel_spec &lt;- rand_forest() %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(engine = \"ranger\") %&gt;%\n  set_args(\n    mtry = ___, # number of predictors to consider at each split\n    min_n = 2, \n    trees = 500, \n    probability = FALSE, \n    importance = \"impurity\")\n```"
  },
  {
    "objectID": "courses/ML/index.html#clustering",
    "href": "courses/ML/index.html#clustering",
    "title": "Machine Learning Notes - Adapted from Macalester’s STAT 253 SP25 Course by Brianna Heggeseth",
    "section": "Clustering",
    "text": "Clustering\nGoal: Identify similarities amongst rows, with respect to variables (features).\n\nIn general, clustering is a DISCOVERY PROCESS!\n\n\nHierarchical Clustering\nGoal:\n\nCalculate euclidean distance\nGroup (cluster) together 2 closest points by chosen linkage type\nRepeat with new clusters till end with 1 cluster\n\nLinkage Types:\n\nComplete: Max distance between any pair of cases\nSingle: min distance\nAverage: average distance\nCentroid: centroid distance\n\nThis creates a dendrogram, which is NOT a classification tree (we start from the leaves and end at the root).\nGeneral properties of dendrograms:\n\nThe more similar a cluster is, the sooner it will fuse\nHorizontal distance between leaves in meaningless\nHeight = distance between 2 points\n\nNo “correct” way to cut a dendrogram\n\nHigher cut, fewer clusters, easier to understand, but maybe too simple and not very meaningful.\n\nDrawbacks:\n\nComputationally expensive: need to calculate distances between each pair of points\nGreedy: best local decision of which cluster might not be best split/most meaningful cluster\n\nCode:\n```{r}\n# Install packages\nlibrary(tidyverse)\nlibrary(cluster)      # to build the hierarchical clustering algorithm\nlibrary(factoextra)   # to draw the dendrograms\n\n# convert column to rowname\nsample_data &lt;- sample_data %&gt;%\n  column_to_rowname(\"id\")\n\n# run clustering algorithm\n# use \"complete\", \"single\", \"average\", or \"centroid\" linkage method\nhier_model &lt;- hclust(dist(scale(sample_data)), method = ___)\n\n# visualizing the clustering: heat map (not clustering)\nheatmap(scale(data.matrix(sample_data)), Colv = NA, Rowv = NA)\n\n# visualizing the clustering: heat map (ordered by dendrogram/clustering)\nheatmap(scale(data.matrix(sample_data)), Colv = NA)\n\n# Dentrogram (change font size with cex)\nfviz_dend(hier_model, cex = 1)\nfviz_dend(hier_model, horiz = True, cex = 1)  # plot horizontally to read longer labels\n\n# Visualizing clusters in a dendrogram\n# typically want to store in new dataset so cluster assignment isnt written into features used later\ncluster_data &lt;- sample_data %&gt;%\n  mutate(hier_cluster_k = as.factor(cutree(hier_model, k = ___)))\n\n# visualize the clusters on the dendrogram\nfviz_dend(hier_model, k = ___, cex = 1)\nfviz_dend(hier_model, k = ___, horiz = True, cex = 1)  # plot horizontally to read longer labels\n```\n\n\nK-Means Clustering\nGoal: Split cases into \\(k\\)-nonoverlapping homogeneous clusters\nHomogeneous: within cluster varience (\\(W(c_k)\\)) of each cluster (\\(c_1,\\dots, c_k\\)) = average squared distance between all pairs of cases in cluster.\nAlgorithm:\n\nPick \\(k\\) - from prior knowledge\nInitialize locations for \\(k\\) centroids, assign each case to nearest centroid\nCalculate new centroid\nReassign each case to cluster with nearest centroid\nRepeat 3 and 4 till stablized\n\nCode:\n```{r}\n# Install packages\nlibrary(tidyverse)\nlibrary(cluster)      # to build the hierarchichal clustering algorithm\nlibrary(factoextra)   # to draw the dendrograms\n\n# process the data\nsample_data &lt;- sample_data %&gt;% \n  column_to_rownames(\"id\")\n\n# K-means can't handle NA: 2 options\n# Omit missing cases (this can be bad if there are a lot of missing points!)\nsample_data &lt;- na.omit(sample_data)\n\n# Impute the missing cases\nlibrary(VIM)\nsample_data &lt;- sample_data %&gt;%\n  VIM::kNN(imp_var = FALSE)\n\n# Turn categorical features into dummy variables\n# DON'T DO IF YOU HAVE QUANTITATIVE OR LOGICAL FEATURES\nsample_data &lt;- data.frame(model.matrix(~ . - 1, sample_data))\n\n# Run K-means\nset.seed(___)\nkmeans_model &lt;- kmeans(scale(sample_data), centers = ___)\n\n# Get the \"the total within- cluster sum of squares\"\n# This is the total squared distance of each case from its assigned centroid\nkmeans_model$tot.withinss\n\n# Tuning K-means\ntibble(K = 1:___) %&gt;%\n  mutate(SS = map(K, ~ kmeans(scale(sample_data), centers = .x)$tot.withinss)) %&gt;% \n  unnest(cols = c(SS))\n\n# Assign each sample case to a cluster\n# We typically want to store this in a new dataset so that the cluster assignments aren't \n# accidentally used as features in a later analysis!\ncluster_data &lt;- sample_data %&gt;% \n  mutate(kmeans_cluster_k = kmeans_model$cluster)\n```"
  },
  {
    "objectID": "courses/ML/index.html#dimension-reduction",
    "href": "courses/ML/index.html#dimension-reduction",
    "title": "Machine Learning Notes - Adapted from Macalester’s STAT 253 SP25 Course by Brianna Heggeseth",
    "section": "Dimension Reduction",
    "text": "Dimension Reduction\nGoal: Find similarities amongst columns by collapsing/combining features\nMotivation: What if we have large data (especially \\(p&gt;n\\))? We might want to\n\nidentify structure and patterns among features\nconserve computational resources\nfacilitate regression and classification by eleminiated multicolinear predictors\n\nCombining linear predictors:\n\nIdea 1: kick one out; lose variablity in other predictor\nIdea 2: find direction of best picture (preserves variability/most information) of data\n\nIdea 2:\n\nPrincipal components are linear combinations of original features where loadings refer to the coefficients.\nPC1 contains the most variation.\nWe want to retain high variability and reduce correlation\n\n\nPrincipal Component Analysis\nGoal: Turn original set of predictors into \\(k&lt;p\\) uncorrelated principal components (PCs) preserving the majority of information (variability)\nTrade offs (relative to kicking out columns):\n\nPCA preserves more info, but features now lost meaning; difficult to interpret\n\nDetails:\n\nScore plots: examine relationships among PCs\nLoadings: coefficients of linear combinations\nLoading plots: understand how PCs are constructed, thus how to interpret PC\n\n```{r}\n# Install packages\nlibrary(tidyverse)\n\n# process the data\nsample_data &lt;- sample_data %&gt;% \n  column_to_rownames(\"id\")\n\n# PCA can't handle NAs\n# Omit missing cases (this can be bad if there are a lot of missing points!)\nsample_data &lt;- na.omit(sample_data)\n\n# Impute the missing cases\nlibrary(VIM)\nsample_data &lt;- sample_data %&gt;%\n  VIM::kNN(imp_var = FALSE)\n\n# Turn categorical features into dummy variables\n# DONT DO IF QUANTITATIVE OR LOGICAL\nsample_data &lt;- data.frame(model.matrix(~ . - 1, sample_data))\n\n# Run PCA\n# scale = TRUE, center = TRUE first standardizes the features\npca_results &lt;- prcomp(sample_data, scale = TRUE, center = TRUE)\n\n# ---------------------------------------------------------\n# PC visualizations\n# Get the loadings which define the PCs\npca_results %&gt;% \n  pluck(\"rotation\")\n\n# Plot loadings for first \"k\" PCs (you pick k)\nlibrary(reshape2)\nmelt(pca_results$rotation[, 1:k]) %&gt;% \n  ggplot(aes(x = Var1, y = value, fill = Var1)) +\n    geom_bar(stat = \"identity\") +\n    facet_wrap(~ Var2) + \n    labs(y = \"loadings\", x = \"original features\", fill = \"original features\")\n\n# Plot loadings for just the first PC\nmelt(pca_results$rotation) %&gt;%\n  filter(Var2 == \"PC1\") %&gt;% \n  ggplot(aes(x = Var1, y = value, fill = Var1)) +\n    geom_bar(stat = \"identity\") +\n    labs(y = \"loadings\", x = \"original features\", fill = \"original features\")\n\n# Loadings plot for first 2 PCs\nlibrary(factoextra)\nfviz_pca_var(pca_results, repel = TRUE)\n\n# ---------------------------------------------------------\n# Examine amount of variability captured by each PC\n# Load package for tidy table\nlibrary(tidymodels)\n\n# Numerical summaries: Measure information captured by each PC\npca_results %&gt;% \n  tidy(matrix = \"eigenvalues\")\n\n# Graphical summary 1: SCREE PLOT\n# Plot % of variance explained by each PC\npca_results %&gt;% \n  tidy(matrix = \"eigenvalues\") %&gt;% \n  ggplot(aes(y = percent, x = PC)) + \n    geom_point(size = 2) + \n    geom_line() + \n    labs(y = \"% of variance explained\")\n\n# Graphical summary 2: Plot cumulative % of variance explained by each PC\npca_results %&gt;% \n  tidy(matrix = \"eigenvalues\") %&gt;% \n  rbind(0) %&gt;% \n  ggplot(aes(y = cumulative, x = PC)) + \n    geom_point(size = 2) + \n    geom_line() + \n    labs(y = \"CUMULATIVE % of variance explained\")\n\n# ---------------------------------------------------------\n# Examine scores (i.e. PC coordinates for data points)\n# Numerical summary: check out the scores\npca_results %&gt;% \n  pluck(\"x\")\n\n# Graphical summary: Score plot\n# Plot PC1 scores (x-axis) vs PC2 scores (y-axis) of all data points\nfviz_pca_ind(pca_results, repel = TRUE)\n```\n\n\nPrincipal Component Regression\nIdea: Combining clustering and regression given some \\(y\\) and \\(p&gt;n\\)\nUsing PCR:\n\nIgnore \\(y\\) for now, do PCA\nKeep first \\(k\\) PCs that retain sufficient information from OG predictors\nModel \\(y\\) by those first PCs\n\nNote:\n\nPCA might not produce strongest possible predictors of \\(y\\)\nPartial least squares provides alternative\n\nCode:\n```{r}\nlibrary(tidymodels)\nlibrary(tidyverse)\n\n# STEP 1: specify linear regression model\nlm_spec &lt;- linear_reg() %&gt;%\n  set_model(\"regression\") %&gt;%\n  set_engine(\"lm\")\n\n# STEP 2: variable recipe\npcr_recipe &lt;- recipe(y ~ ., data = sample_data) %&gt;%\n  update_role(data_id, new_role = \"id\") %&gt;%\n  step_dummy(all_nomial_predictors()) %&gt;%\n  step_normalize(all_predictors()) %&gt;%\n  step_pca(all_predictors(), num_comp = tune())\n\n# STEP 3: workflow\npcr_workflow &lt;- workflow() %&gt;%\n  add_recipe(pcr_recipe) %&gt;%\n  add_model(lm_spec)\n\n# STEP 4: Estimate multiple PCR models with varying numbers of PCs\n# Largest range is p\n# Put same number in for levels\nset.seed(___)\npcr_models &lt;- pcr_workflow %&gt;%\n  tune_grid(\n    grid = grid_regular(num_comp(range = c(1, ___)), levels = ___),\n    resamples = vfold_cv(sample_data, v = 10),\n    metrics = metric_set(mae)\n  )\n\n# explore models\npcr_models %&gt;%\n  autoplot()\n\nbest_param &lt;- multiple_models %&gt;%\n  select_best(metric = \"mae\")\n\nparsimonious_param &lt;- multiple_models %&gt;%   \n  select_by_one_std_err(metric = \"mae\", \n                        desc( parameter name))\n\nfinal_model &lt;- pcr_workflow %&gt;% \n  finalize_workflow(___) %&gt;% # your chosen parameter\n  fit(data = ___)\n```"
  },
  {
    "objectID": "courses/quantum/PS1.html",
    "href": "courses/quantum/PS1.html",
    "title": "PHYS 481 - Quantum Mechanics",
    "section": "",
    "text": "Problem 1\nConsider the three vectors\n\\[x = \\begin{pmatrix} 3 \\\\ 0 \\\\ 2\\end{pmatrix}, \\hspace{1mm} y = \\begin{pmatrix} 2 \\\\ -2 \\\\ 0\\end{pmatrix}, \\hspace{1mm} z = \\begin{pmatrix} 5 \\\\ 0 \\\\ -1\\end{pmatrix}\\]\n\nAre \\(x\\), \\(y\\), and \\(z\\) (in the three-dimensional Euclidean space) linearly independent or dependent? Show your work.\nDo \\(x\\), \\(y\\), and \\(z\\) form a basis of the three-dimensional Euclidean space? Why or why not?\n\n\nResponse: For a), we can determine if \\(x\\), \\(y\\), and \\(z\\) are linearly independent by combining them into a matrix and putting them in RREF. If the RREF form is equivalent to the identity matrix, then the vectors are linearly independent because no one vector can be represented at a linear combination of the others. Putting the matrix in RREF will also reveal the span of the vectors by counting the number of pivots; if they are linearly independent, then they will span the a space the size of the number of pivots. This will be useful in answering b).\n\\[\\begin{align}  & \\begin{bmatrix} 3 & 2 & 5 \\\\ 0 & -2 & 0 \\\\ 2 & 0 & -1\\end{bmatrix} \\overset{R_1 = R_1 - R_3}{\\longrightarrow} \\\\\n& \\begin{bmatrix} 1 & 2 & 6 \\\\ 0 & -2 & 0 \\\\ 2 & 0 & -1\\end{bmatrix} \\overset{R_1 = R_1 + R_2}{\\longrightarrow} \\\\\n& \\begin{bmatrix} 1 & 0 & 6 \\\\ 0 & -2 & 0 \\\\ 2 & 0 & -1\\end{bmatrix} \\overset{R_3 = R_3 - 2R_1}{\\longrightarrow} \\\\\n& \\begin{bmatrix} 1 & 0 & 6 \\\\ 0 & -2 & 0 \\\\ 0 & 0 & -13\\end{bmatrix} \\overset{R_2 = -\\frac{1}{2}R_2,\\; R_3 = -\\frac{1}{13}R_3}{\\longrightarrow} \\\\\n& \\begin{bmatrix} 1 & 0 & 6 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1\\end{bmatrix} \\overset{R_1 = R_1 - 6R_3}{\\longrightarrow} \\\\\n& \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1\\end{bmatrix}\\end{align} = I_3\\]\nNote that since the RREF is equal to the identity matrix, vectors \\(x\\), \\(y\\), and \\(z\\) are linearly independent. This fact also means that they span the three-dimensional Euclidean space, forming a basis of the entire space, because all vectors in the space can be created by a linear combination of \\(x\\), \\(y\\), and \\(z\\).\n\n\n\nProblem 2\nConsider the function \\(f(x) = 3e^{-(x^2/a^2)}\\).\n\nSketch a graph of \\(f(x)\\) vs. \\(x\\) for \\(a=10\\). Be sure to indicate and scale the axes.\nDoes the function show any symmetries? What is the limit \\(x\\to\\pm\\infty\\)?\nWhat is the significance of the parameter \\(a\\) in the exponent?\n\n\nResponse:\n\nThe code chunk below plots \\(f(x)\\) for \\(a=10\\), \\(a=20\\), and \\(a=30\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe function is symmetric about the \\(y\\) axis and goes to 0 as \\(x\\to\\pm\\infty\\).\nAs we increase \\(a\\), the width of the curve increases.\n\n\n\n\nProblem 3\nThe second-order differential equation,\n\\[\\frac{d^2f(x)}{dx^2}=-k^2f(x)\\]\nhas two linearly independent solutions. These can be written in more than one way, and two convenient forms are\n\\[\\begin{align}\nf(x) &= A \\exp(ikx) + B\\exp(-ikx) \\\\\nf(x) &= a \\sin(kx) + b \\cos(kx)\n\\end{align}\n\\]\nVerify that both are solutions of the equation (2) above. Since both are equally good solutions, we must be able to determine \\(a\\) and \\(b\\) in terms of \\(A\\) and \\(B\\); do so.\n\nResponse: We can verify both are solutions by taking the second derivative. We can see below that both are valid solutions to the second-order differential equation.\n\\[\\begin{align} f(x) &= A e^{ikx} + B e^{-ikx} \\\\ f'(x) &= Aik e^{ikx} - Bik e^{-ikx} \\\\ f''(x) &= -Ak^2 e^{ikx} - Bk^2 e^{-ikx} \\\\ &= -k^2 \\left(A e^{ikx} + B e^{-ikx}\\right) \\\\ &= -k^2 f(x) \\\\ \\\\ f(x) &= a\\sin(kx) + b\\cos(kx) \\\\ f'(x) &= ak\\cos(kx) - bk\\sin(kx) \\\\ f''(x) &= -ak^2\\sin(kx) - bk^2\\cos(kx) \\\\ &= -k^2 \\left(a\\sin(kx) + b\\cos(kx) \\right) \\\\&= -k^2 f(x)\\end{align}\\]\nAs we can see below, we get the following relationship for \\(a\\) and \\(b\\), in terms of \\(A\\) and \\(B\\), when we set both functions equal to each other.\n\\[\\begin{align} A e^{ikx} + B e^{-ikx} &= a\\sin(kx) + b\\cos(kx) \\\\ A\\left[\\cos(kx) + i\\sin(kx)\\right] + B\\left[\\cos(-kx) + i\\sin(-kx)\\right] &= a\\sin(kx) + b\\cos(kx) \\\\ A\\cos(kx) + Ai\\sin(kx) + B\\cos(kx) - Bi\\sin(kx) &=  a\\sin(kx) + b\\cos(kx) \\\\ (A + B)\\cos(kx) + i(A - B)\\sin(kx) &= b\\cos(kx) + a\\sin(kx) \\\\ \\text{So } b &= A+B,\\\\ a &= i(A - B) \\end{align}\\]\n\n\n\nProblem 4\nLet \\(\\Psi(x,t)\\) be a solution of the (one-dimensional) Schrodinger equation\n\\[i\\hbar\\frac{\\partial}{\\partial t}\\Psi(x,t) = \\left[-\\frac{\\hbar^2}{2m}\\frac{\\partial^2}{\\partial x^2} + V(x,t)\\right]\\Psi(x,t)\\]\nSuppose you add a constant \\(V_0\\), which is independent of \\(x\\) and \\(t\\), to the potential energy term \\(V(x,t)\\).\n\nShow that the solution of the new Schrodinger equation is given by \\(\\Phi(x,t) = \\Psi(x,t)\\exp(-iV_0t/\\hbar\\).\nWhat effect does this time-dependent phase factor have on the probability density \\(|\\Phi(x,t)|^2\\) and the expectation value of the position \\(\\big&lt;x\\big&gt;\\)? Explain.\n\n\nResponse: The newly proposed Schrodinger equation is\n\\[i\\hbar\\frac{\\partial}{\\partial t}\\Phi(x,t) = \\left[-\\frac{\\hbar^2}{2m}\\frac{\\partial^2}{\\partial x^2} + V(x,t) + V_0\\right]\\Phi(x,t), \\; \\Phi(x,t) = \\Psi(x,t)e^{-iV_0t/\\hbar}\\]\nVerifying the proposed solution is valid requires us taking time and spatial derivatives.\nThe time derivative is\n\\[\\frac{\\partial \\Phi}{\\partial t} = \\left(e^{-iV_0t\\hbar}\\right)\\left(\\Psi\\left(\\frac{-iV_0}{\\hbar}\\right) + \\frac{\\partial \\Psi}{\\partial t}\\right)\\]\nThe second order spatial derivative is\n\\[\\frac{\\partial^2\\Phi}{\\partial x^2} = \\left(e^{-iV_0t/\\hbar}\\right)\\frac{\\partial^2\\Psi}{\\partial x^2}\\]\nSubsituting in these results into the new Schrodinger Equation and simplifying reveals the original Schrodinger Equation, thus, this is a valid solution.\n\\[\\begin{align} i\\hbar\\frac{\\partial}{\\partial t}\\Phi(x,t) &= \\left[-\\frac{\\hbar^2}{2m}\\frac{\\partial^2}{\\partial x^2} + V(x,t) + V_0\\right]\\Phi(x,t) \\\\ i\\hbar\\left[\\left(e^{-iV_0t\\hbar}\\right)\\left(\\Psi\\left(\\frac{-iV_0}{\\hbar}\\right) + \\frac{\\partial \\Psi}{\\partial t}\\right)\\right] &= -\\frac{\\hbar^2}{2m}\\left(\\left(e^{-iV_0t/\\hbar}\\right)\\frac{\\partial^2\\Psi}{\\partial x^2}\\right) + \\left[V(x,t) + V_0\\right]\\Psi(x,t)e^{-iV_0t/\\hbar} \\\\ i\\hbar\\frac{\\partial}{\\partial t}\\Psi(x,t) &= \\left[-\\frac{\\hbar^2}{2m}\\frac{\\partial^2}{\\partial x^2} + V(x,t)\\right]\\Psi(x,t)\\end{align}\\]\nAdditionally, if we look at the probability density, we can see that the time-dependent phase factor has no effect.\n\\[|\\Phi|^2 = |\\Phi^* \\Phi| = |\\Psi|^2 |e^{-iV_0t/\\hbar}e^{iV_0t/\\hbar}| = |\\Psi|^2\\]\nSimilarly, since the expectation value is defined in terms of the probability, we know that the expecation value also remains unchanged by the time-dependent phase factor.\n\\[\\big&lt;x\\big&gt; = \\int_{-\\infty}^{\\infty}x |\\Phi|^2 dx = \\int_{-\\infty}^{\\infty}x |\\Psi|^2 dx\\]\n\n\n\nProblem 5\nConsider the gaussian distribution,\n\\[\\rho(x) = Ae^{-\\lambda(x-a)^2}\\]\nwhere \\(A\\), \\(a\\), and \\(\\lambda\\) are positive real constants.\n\nUse the Normalization condition to determine \\(A\\).\nFind \\(\\big&lt;x\\big&gt;\\), \\(\\big&lt;x^2\\big&gt;\\) and \\(\\sigma\\).\nSketch the graph of \\(\\rho(x)\\). [Use Mathematica/Python to plot]\n\n\nResponse: If \\(\\rho(x)\\) is a probability distribution, then the area under the curve must add to one and we can normalize as follows\n\\[\\begin{align} 1 &= A\\int_{-\\infty}^{\\infty}e^{-\\lambda(x-a)^2} dx\\\\ &= A \\int_{-\\infty}^{\\infty}e^{-\\lambda u^2} du, \\quad u = x-a,\\; du = 1 \\\\ &= A\\sqrt{\\frac{\\pi}{\\lambda}} \\\\ \\therefore A &= \\sqrt{\\frac{\\lambda}{\\pi}}\\end{align}\\]\nBelow, we find \\(\\big&lt;x\\big&gt;\\), \\(\\big&lt;x^2\\big&gt;\\) and \\(\\sigma\\)\n\\[\\begin{align} \\big&lt;x\\big&gt; &= \\sqrt{\\frac{\\lambda}{\\pi}}\\int_{-\\infty}^{\\infty}xe^{-\\lambda(x-a)^2} dx \\\\ &= \\sqrt{\\frac{\\lambda}{\\pi}}\\int_{-\\infty}^{\\infty}(u+a)e^{-\\lambda u^2} du \\\\ &= \\sqrt{\\frac{\\lambda}{\\pi}}\\left(\\int_{-\\infty}^{\\infty}ue^{-\\lambda u^2} du + a\\int_{-\\infty}^{\\infty}e^{-\\lambda u^2} du \\right) \\\\ &= \\sqrt{\\frac{\\lambda}{\\pi}} \\left(0 + a\\sqrt{\\frac{\\pi}{\\lambda}}\\right) \\\\ &= a \\end{align}\\]\n\\[\\begin{align} \\big&lt;x^2\\big&gt; &= \\sqrt{\\frac{\\lambda}{\\pi}}\\int_{-\\infty}^{\\infty}x^2e^{-\\lambda(x-a)^2} dx \\\\ &= \\sqrt{\\frac{\\lambda}{\\pi}}\\int_{-\\infty}^{\\infty}(u+a)^2e^{-\\lambda u^2} du \\\\ &= \\sqrt{\\frac{\\lambda}{\\pi}} \\left(\\int_{-\\infty}^{\\infty} u^2 e^{-\\lambda u^2}du + 2a\\int_{-\\infty}^{\\infty} ue^{-\\lambda u^2}du + a^2\\int_{-\\infty}^{\\infty} e^{-\\lambda u^2}du\\right) \\\\ &= \\sqrt{\\frac{\\lambda}{\\pi}} \\left(\\frac{1}{2\\lambda}\\sqrt{\\frac{\\pi}{\\lambda}} + a^2 \\sqrt{\\frac{\\pi}{\\lambda}}\\right) \\\\ &= \\frac{1}{2\\lambda} + a^2\\end{align}\\]\n\\[\\begin{align} \\sigma &= \\sqrt{\\big&lt;x^2\\big&gt; - \\big&lt;x\\big&gt;^2} \\\\ &= \\sqrt{\\frac{1}{2\\lambda} + a^2 - a^2} \\\\ &= \\sqrt{\\frac{1}{2\\lambda}}\\end{align}\\]\nBelow is the plot of \\(\\rho(x)\\) for various \\(\\lambda\\) and \\(a\\) values.\n\n\n\n\n\n\n\n\n\n\n\n\nProblem 6\nAt time \\(t=0\\) a particle is represented by the wave function\n\\[\\Psi(x,0) = \\cases{A\\frac{x}{a} \\hspace{1.8cm} 0\\leq x\\leq a \\\\ A\\frac{(b-x)}{(b-a)} \\hspace{1cm} a\\leq x\\leq b \\\\ 0 \\hspace{2.3cm} otherwise}\\]\nwhere \\(A\\), \\(a\\), and \\(b\\) are constants.\n\nNormalize \\(\\Psi\\) (i.e. Find \\(A\\) in terms of \\(a\\) and \\(b\\))\nSketch \\(\\Psi(x,0)\\) as a function of \\(x\\). [Use Mathematica/Python to plot]\nWhere is the particle most likely to be found at \\(t=0\\)?\nWhat is the probability of finding the particle to the left of \\(a\\)? Check your result in the limiting cases \\(b=a\\) and \\(b=2a\\).\nwhat is the expectation value of \\(x\\)?\n\n\nResponse: We can normalize the wavefunction by solving \\(\\int_{-\\infty}^{\\infty}|\\Psi|^2 dx = 1\\), as such below.\n\\[\\begin{align} 1 &= \\frac{A^2}{a^2}\\int_0^a x^dx + \\frac{A^2}{(b-a^2)}\\int_a^b (b-x)^2 dx \\\\ &= \\frac{A^2 a + A^2(b-a)}{3} \\\\ &= \\frac{A^2b}{3} \\\\ \\text{So, } A &= \\sqrt{\\frac{3}{b}}\\end{align}\\]\nBelow is a plot of \\(\\Psi\\) for \\(a=1\\) and \\(b=2a\\).\n\n\n\nText(0, 0.5, '$\\\\Psi(x,0)$')\n\n\n\n\n\n\n\n\n\n\nAs we can see from the plot above, the particle is most likely to be found at the mode of \\(a\\).\nThe probability of finding the particle to the left of \\(a\\) can be calculated as:\n\\[\\begin{align} P(x\\leq a) &= \\int_0^a |\\Psi|^2 dx \\\\ &= \\frac{3}{ba^2}\\int_0^a x^2 dx \\\\ &= \\frac{a}{b}\\end{align}\\]\nChecking our solutions with our limiting cases, we can see our answer matches our intuition. Namely if \\(b=a\\), the wavefunction is only on one point and we get a probability of one. Similarly, if \\(b=2a\\) we get a probability of one half, which matches our intuition given the symmetry of the wavefunction.\nThe expectation value of \\(x\\) is calculated below.\n\\[\\begin{align} \\big&lt;x\\big&gt; &= \\int_{-\\infty}^{\\infty}x|\\Psi|^2dx \\\\ &= \\frac{3}{ba^2}\\int_0^a x^3 dx + \\frac{3}{b(b-a)^2} \\int_a^b x(b-x)^2 dx \\\\ &= \\frac{3a^2}{4b} + \\frac{(b-a)(3a + b)}{4b} \\\\ &= \\frac{b+2a}{4}\\end{align}\\]"
  },
  {
    "objectID": "courses/topology/index.html",
    "href": "courses/topology/index.html",
    "title": "Topology",
    "section": "",
    "text": "Final Project\n\nFinal Paper\nFinal Presentation"
  },
  {
    "objectID": "courses/topology/presentation.html#section-1",
    "href": "courses/topology/presentation.html#section-1",
    "title": "Persistent Images of Gravitionally Lensed Galaxies",
    "section": "",
    "text": "What?\n\nPredicted by GR\nLight from distant galaxies gets warped by foreground galaxies + dark matter\n\n\nWhy?\n\nNatural way to see futher\nMap\n\nthe universe\ndark matter\ndark energy\n\n\n\n\n[explain dark matter and dark energy]\n\nFor this presentation, they are\n\npredicted by GR\nresult of when light from distant…\n\nWhy do we care about them?\n\nallow us to see more distant galaxies\nmap further into universe\nmore datapoints of places with dark matter –&gt; better understand dark matter\nfurther we see, the better we can estimate expansion, better we can constrain/characterize dark energy\n…[pause]\nNow we know what and why…"
  },
  {
    "objectID": "courses/topology/presentation.html#video",
    "href": "courses/topology/presentation.html#video",
    "title": "Persistent Images of Gravitionally Lensed Galaxies",
    "section": "Video",
    "text": "Video\n\n\nIn March of 2025, Euclid released its first data products [gesture to the screen]\n…[pause]\nWhat you you watching is 1% of the area Euclid plans on covering in its mission\n…[pause]\nWhat are Euclid’s goals?"
  },
  {
    "objectID": "courses/topology/presentation.html#euclid-goals",
    "href": "courses/topology/presentation.html#euclid-goals",
    "title": "Persistent Images of Gravitionally Lensed Galaxies",
    "section": "Euclid Goals",
    "text": "Euclid Goals\n\n3D map of \\(\\frac{1}{3}\\) of the sky\ndark matter and dark energy\n10 billion light years\n\n\n(Collaboration et al. 2025)\n\n3D map of \\(\\frac{1}{3}\\) of the sky (why 1/3? turns out lots of dust in galaxy; we want to avoid)\ndark matter and dark energy (get better understanding)\n10 billion light years (very big)\n\nOverall Goals of Euclid\n\nWhat is the structure and history of the cosmic web?\nWhat is the nature of dark matter?\nHow has the expansion of the Universe changed over time?\nWhat is the nature of dark energy?\nIs our understanding of gravity complete?\nThat brings us to my project"
  },
  {
    "objectID": "courses/topology/presentation.html#data",
    "href": "courses/topology/presentation.html#data",
    "title": "Persistent Images of Gravitionally Lensed Galaxies",
    "section": "Data",
    "text": "Data\n\n\n \n\n\nCoordinates from Walmsley et al. (2025)\nOnly select top candidates\nGet image cutouts of each target\nPreprocess the images (resize, clip, scale, and invert)\n\n\n\n\nThe Euclid Collaboration published a list of lensed galaxies they identified through ML algorithm\nTake coordinates from that\nFilter by only grade “A” candidates ~250\nGet image cutouts of each target (242 now)\nPreprocess the images (resize, clip, scale, and invert (h0))\n…[pause]\nthose images look like this [next slide]"
  },
  {
    "objectID": "courses/topology/presentation.html#section-2",
    "href": "courses/topology/presentation.html#section-2",
    "title": "Persistent Images of Gravitionally Lensed Galaxies",
    "section": "",
    "text": "on left, we have the “natural” images of the cluster\nNote that most of the image is a value of 0\ncan pose a problem for sublevel persistence algorithms\n\nstart from the lowest pixel value and end at the highest\nlike the Cubical persistence algorithm in the python library giotto, which I used for this project\nNote cubical persistence can be thought of as the cubical analog to simplicial persistence\n\nNote that the sublevel persistence issue is more of a problem when identifying connected components\n\nIf everything is zero, then everything will be connected (we can see that in the diagram too)\n\nWe can get around this by inverting the image for our h0 calculations\n\nWe can see the rest of the process is straightforward: compute persistence diagram, then convert the persistence diagram into a persistence image using the Persim python library\n\nUnlike the h0 superlevel persistence, the h1 calculations can be done with sublevel persistence\nOnce we have our images, we can vectorize and concatenate h0 and h1\nConvert vector to dataframe\nPerform Hierarchical Clustering"
  },
  {
    "objectID": "courses/topology/presentation.html#section-3",
    "href": "courses/topology/presentation.html#section-3",
    "title": "Persistent Images of Gravitionally Lensed Galaxies",
    "section": "",
    "text": "Here are the results for the “natural” images\nThese were just the raw images from the previous slide\nIf you haven’t seen a dendrogram before, essentially: [explain dendrogram]\nThe key thing to take note of for this is the hight at which we start seeing connections\nThat is: when do clusters start being similar?\nKeep that hight in mind as we switch to the results from the persistence image clustering"
  },
  {
    "objectID": "courses/topology/presentation.html#section-4",
    "href": "courses/topology/presentation.html#section-4",
    "title": "Persistent Images of Gravitionally Lensed Galaxies",
    "section": "",
    "text": "what do you notice?\nDid anyone catch the scale? (0-600) now (0-100)\nThe persistence image appears to have found similar clusters more easily than the natural images"
  },
  {
    "objectID": "courses/topology/presentation.html#references",
    "href": "courses/topology/presentation.html#references",
    "title": "Persistent Images of Gravitionally Lensed Galaxies",
    "section": "References",
    "text": "References\n\n\n\n\nChen, Yen-Chi, Shirley Ho, Peter E. Freeman, Christopher R. Genovese, and Larry Wasserman. 2015. “Cosmic Web Reconstruction Through Density Ridges: Method and Algorithm.” Monthly Notices of the Royal Astronomical Society 454 (1): 1140–56. https://doi.org/10.1093/mnras/stv1996.\n\n\nCollaboration, Euclid, M. Walmsley, P. Holloway, N. E. P. Lines, K. Rojas, T. E. Collett, A. Verma, et al. 2025. “Euclid Quick Data Release (Q1): The Strong Lensing Discovery Engine a – System Overview and Lens Catalogue.” https://arxiv.org/abs/2503.15324.\n\n\nSousbie, T., C. Pichon, and H. Kawahara. 2011. “The Persistent Cosmic Web and Its Filamentary Structure – II. Illustrations.” Monthly Notices of the Royal Astronomical Society 414 (1): 384–403. https://doi.org/10.1111/j.1365-2966.2011.18395.x.\n\n\nWalmsley, Mike, Philip Holloway, Natalie Lines, Karina Rojas, Thomas Collett, Aprajita Verma, Tian Li, James Nightingale, Giulia Despali, and Stefan Schuldt. 2025. “Euclid Quick Data Release (Q1): The Strong Lensing Discovery Engine.” Zenodo. https://doi.org/10.5281/zenodo.15025832."
  },
  {
    "objectID": "research/CONs/index.html",
    "href": "research/CONs/index.html",
    "title": "Notes on Compact Obscured Nuclei. I. The Evolutionary Context",
    "section": "",
    "text": "Luminosity sources in compact obscured nuclei (CONs) and the role of CONs in galaxy evolution are yet to be fully understood. We address these through an examination of luminosity and mass distributions of a sample of CONs in luminous infrared galaxies (CON-LIRGs), Compton thick active galactic nuclei (CT-AGN), and luminous infrared galaxies (LIRGs). We find LIRGs, CON-LIRGs, and CT-AGN occur in disk galaxies with similar stellar masses. CON-LIRGs and LIRGs, while potentially powered in different ways, have similar bolometric luminosities. Since these three unusual evolutionary phases occur in the same general population of galaxies, transitions between these evolutionary phases are possible within the same galaxy. However, CONs typically are substantially more luminous than CT-AGN, so evolution from a CT-AGN to a CON-LIRG would require additional luminosity from a compact starburst or increased central black hole accretion power. CON-LIRGs are not simply more deeply embedded versions of CT-AGNs.\nHere is the link to our first AAS research note (1/2).\nHere is the link to our second AAS research note (2/2)"
  },
  {
    "objectID": "research/H2OMegamasers/index.html",
    "href": "research/H2OMegamasers/index.html",
    "title": "Analysis of the Optical Spectra of H2O Megamaser Emissions",
    "section": "",
    "text": "At Massanutten Regional Governor’s School (MRGS), students take a research course where they are given the opportunnity to work with outside mentors. I was fortunate enough to be able to work with Dr. Anca Constantin, a professor in JMU’s Physics and Astronomy Department, during my two years at MRGS.\nThe goal of our project was to learn more about water megamasers by examining the optical spectra of a large dataset of confirmed megamasers. This was done by crossmatching a catalog of known megamasers, provided by the Megamaser Cosmology Project (MCP), with the Sloan Digital Sky Survey (SDSS). Python was used for data manipulation and visualization. SQL was used for data filtering in the SDSS. If interested, you can download a copy of my end-of-year/ISEF presentation.\n\nAt the end of my senior year, this project was entered into the Shenandoah Valley Regional Science Fair, where it won first place in its division, allowing it to be entered into the Virginia State Science and Engineering Fair. It was also a finalist project in the 2022 International Science and Engineering Fair. Here is the link to the ISEF abstract.\nThis was my first hands-on experience working with Python, SQL, and astronomy research in general. I am incredibly thankful for Dr. Constantin for the opportunnity to work with her, and for Mr. Russel Kohrs and Ms. Jennifer Moyers of MRGS for advising me on this two-year project."
  },
  {
    "objectID": "research/LeoP/index.html",
    "href": "research/LeoP/index.html",
    "title": "Fundamental Astrophysics at Low Metallicity: Leo P",
    "section": "",
    "text": "Leo P is a nearby and metal-poor (3% of the Solar abundance) star-forming dwarf galaxy that anchors fundamental galaxy scaling relationships. Originally identified by its neutral hydrogen content in the ALFALFA survey, Leo P has undergone intense observational scrutiny which has revealed it to be the lowest mass galaxy that still lies on the mass-metallicity relationship. In order to further explore the neutral interstellar medium of this cosmologically important system, we present deep neutral hydrogen HI 21cm imaging acquired with the National Radio Astronomy Observatory’s Karl G. Jansky Very Large Array (VLA) in the extended B configuration. The high spectral and spatial resolution of these data (2.5 km/s per channel in a synthesized beam size of 10” = 79 pc at the adopted distance of 1.62 Mpc) allow us to undertake the most comprehensive study of the HI in Leo P to date. The relationship between HI and nebular Hα emission is complex. At full angular resolution we measure a peak HI column density \\(N_{HI} = 9\\cdot10^{20} \\text{cm}^{-2}\\) (corresponding to a peak HI mass surface density of ~7 Solar masses per square parsec) that is slightly offset from the Hα maximum (where the extremely low oxygen abundance is measured). We compare the HI morphology and kinematics on various angular scales to the faint Hα loops seen in deep MUSE IFU observations.\nHere is the link to the AAS244 IPoster presentation."
  },
  {
    "objectID": "research/SwimSense/index.html",
    "href": "research/SwimSense/index.html",
    "title": "SwimSense: Open-Source Data Science for Swimmers",
    "section": "",
    "text": "The goal of this project is to provide swimmers and coaches with all the resources they need to analyze their swimming technique through data. This project arose out of a desire of mine to improve my own technique, and was inspired by the work of Ken Ono and others with the University of Vrigina’s Swim Team.\nHere is the link to the GitHub repo where most of this project is documented, for the time being."
  },
  {
    "objectID": "research/TURBO/index.html",
    "href": "research/TURBO/index.html",
    "title": "TURBO: Total-coverage Ultra-fast Response to Binary Mergers Observatory",
    "section": "",
    "text": "Python programming"
  },
  {
    "objectID": "courses/quantum/index.html",
    "href": "courses/quantum/index.html",
    "title": "Quantum",
    "section": "",
    "text": "Problem Sets\n\nPS1\nPS2"
  }
]